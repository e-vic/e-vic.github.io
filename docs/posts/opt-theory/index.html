<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Bahman Gharesifard">
<meta name="author" content="Emma Hansen">
<meta name="dcterms.date" content="2024-07-01">

<title>Notes - Optimisation Theory and Applications</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Contents</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Optimisation Theory and Applications</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">notes</div>
                <div class="quarto-category">optimisation</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-heading">Affiliations</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Bahman Gharesifard </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              Queen’s University
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Emma Hansen </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              N/A
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 1, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Abstract</div>
      <p>This is a write up of notes from the Optimisation Theory and Applicaitons course taught by Bahman Gharesifard in 2016 at Queen’s University.</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#examples-of-optimisation-problems" id="toc-examples-of-optimisation-problems" class="nav-link" data-scroll-target="#examples-of-optimisation-problems"><span class="header-section-number">1.1</span> Examples of Optimisation Problems</a></li>
  <li><a href="#notations" id="toc-notations" class="nav-link" data-scroll-target="#notations"><span class="header-section-number">1.2</span> Notations</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">1.3</span> References</a></li>
  </ul></li>
  <li><a href="#introduction-to-optimisation" id="toc-introduction-to-optimisation" class="nav-link" data-scroll-target="#introduction-to-optimisation"><span class="header-section-number">2</span> Introduction to Optimisation</a>
  <ul class="collapse">
  <li><a href="#unconstrained-optimisation" id="toc-unconstrained-optimisation" class="nav-link" data-scroll-target="#unconstrained-optimisation"><span class="header-section-number">2.1</span> Unconstrained Optimisation</a>
  <ul class="collapse">
  <li><a href="#necessary-conditions-of-optimality" id="toc-necessary-conditions-of-optimality" class="nav-link" data-scroll-target="#necessary-conditions-of-optimality"><span class="header-section-number">2.1.1</span> Necessary Conditions of Optimality</a></li>
  <li><a href="#sufficient-conditions-of-optimality" id="toc-sufficient-conditions-of-optimality" class="nav-link" data-scroll-target="#sufficient-conditions-of-optimality"><span class="header-section-number">2.1.2</span> Sufficient Conditions of Optimality</a></li>
  </ul></li>
  <li><a href="#constrained-optimisation" id="toc-constrained-optimisation" class="nav-link" data-scroll-target="#constrained-optimisation"><span class="header-section-number">2.2</span> Constrained Optimisation</a></li>
  </ul></li>
  <li><a href="#convex-analysis" id="toc-convex-analysis" class="nav-link" data-scroll-target="#convex-analysis"><span class="header-section-number">3</span> Convex Analysis</a>
  <ul class="collapse">
  <li><a href="#convex-sets" id="toc-convex-sets" class="nav-link" data-scroll-target="#convex-sets"><span class="header-section-number">3.1</span> Convex Sets</a>
  <ul class="collapse">
  <li><a href="#operations-that-preserve-convexity" id="toc-operations-that-preserve-convexity" class="nav-link" data-scroll-target="#operations-that-preserve-convexity"><span class="header-section-number">3.1.1</span> Operations that Preserve Convexity</a></li>
  </ul></li>
  <li><a href="#convex-functions" id="toc-convex-functions" class="nav-link" data-scroll-target="#convex-functions"><span class="header-section-number">3.2</span> Convex Functions</a>
  <ul class="collapse">
  <li><a href="#conditions-of-convexity" id="toc-conditions-of-convexity" class="nav-link" data-scroll-target="#conditions-of-convexity"><span class="header-section-number">3.2.1</span> Conditions of Convexity</a></li>
  </ul></li>
  <li><a href="#separation-of-convex-sets" id="toc-separation-of-convex-sets" class="nav-link" data-scroll-target="#separation-of-convex-sets"><span class="header-section-number">3.3</span> Separation of Convex Sets</a></li>
  </ul></li>
  <li><a href="#convex-optimisation" id="toc-convex-optimisation" class="nav-link" data-scroll-target="#convex-optimisation"><span class="header-section-number">4</span> Convex Optimisation</a>
  <ul class="collapse">
  <li><a href="#conjugate-functions" id="toc-conjugate-functions" class="nav-link" data-scroll-target="#conjugate-functions"><span class="header-section-number">4.1</span> Conjugate Functions</a></li>
  <li><a href="#duality-theory" id="toc-duality-theory" class="nav-link" data-scroll-target="#duality-theory"><span class="header-section-number">4.2</span> Duality Theory</a></li>
  <li><a href="#nonlinear-programming" id="toc-nonlinear-programming" class="nav-link" data-scroll-target="#nonlinear-programming"><span class="header-section-number">4.3</span> Nonlinear Programming</a>
  <ul class="collapse">
  <li><a href="#nlp-and-duality" id="toc-nlp-and-duality" class="nav-link" data-scroll-target="#nlp-and-duality"><span class="header-section-number">4.3.1</span> NLP and Duality</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#algorithms-for-optimisation" id="toc-algorithms-for-optimisation" class="nav-link" data-scroll-target="#algorithms-for-optimisation"><span class="header-section-number">5</span> Algorithms for Optimisation</a>
  <ul class="collapse">
  <li><a href="#gradient-methods---first-order" id="toc-gradient-methods---first-order" class="nav-link" data-scroll-target="#gradient-methods---first-order"><span class="header-section-number">5.1</span> Gradient Methods - First Order</a></li>
  <li><a href="#gradient-methods---second-order" id="toc-gradient-methods---second-order" class="nav-link" data-scroll-target="#gradient-methods---second-order"><span class="header-section-number">5.2</span> Gradient Methods - Second Order</a></li>
  </ul></li>
  <li><a href="#extras" id="toc-extras" class="nav-link" data-scroll-target="#extras"><span class="header-section-number">6</span> Extras</a>
  <ul class="collapse">
  <li><a href="#minimising-polynomials" id="toc-minimising-polynomials" class="nav-link" data-scroll-target="#minimising-polynomials"><span class="header-section-number">6.1</span> Minimising Polynomials</a></li>
  </ul></li>
  <li><a href="#references-1" id="toc-references-1" class="nav-link" data-scroll-target="#references-1"><span class="header-section-number">7</span> References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<div class="hidden">

</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>This document is a write-up of the <em>Optimisation Theory and Applications</em> course (MTHE434) taught by <a href="https://gharesifard.github.io/">Bahman Gharesifard</a> in 2016 at <a href="https://www.queensu.ca/mathstat/undergraduate/prospective-undergraduate/mthe">Queen’s University</a>. The vast majority of the content is from Bahman’s notes, with some comments from me (Emma Hansen).</p>
<section id="examples-of-optimisation-problems" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="examples-of-optimisation-problems"><span class="header-section-number">1.1</span> Examples of Optimisation Problems</h2>
<div class="example">
<div class="example-header">
<p>Example: 150BC – Alexandria’s Problem</p>
</div>
<div class="example-container">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture1.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>Find a point <span class="math inline">\(D\in\mathbb{R}^2\)</span> such that <span class="math inline">\(||AD||+||DB||\)</span> is minimised.</p>
<p>Consider the figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture2.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>Claim: this <span class="math inline">\(D\)</span> is the solution.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose otherwise, i.e.&nbsp;let some point <span class="math inline">\(D'\)</span> be the minimiser:</p>
<p><span class="math display">\[\begin{aligned}
||AD'|| + ||D'B|| &amp;\geq ||AB'|| \textrm{ (from the claim)} \\
&amp;= ||AD|| + ||DB'|| \\
&amp;= ||AD|| + ||DB||
\end{aligned}\]</span></p>
<p>Why is this true? Triangle inequality! Thus, <span class="math inline">\(D\)</span> is the minimiser. ■</p>
</div>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example: 850BC – Dido’s Problem</p>
</div>
<div class="example-container">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture4.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Consider a curve of fixed length <span class="math inline">\(\ell\)</span>. What is the maximum area of land you can enclose using this curve?</p>
<p>To start, think about what kind of shape you should use.</p>
<p>Formally, the problem is: <span class="math inline">\(\max_y \ \ J(y) = \int_a^b y(x) dx, \ y\in\mathcal{C}([a,b])\)</span>.</p>
<p>It turns out the optimal shape is a semicircle, but we will not be able to prove this even at the end of the course.</p>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example: 1556 – Tartaglia’s Problem</p>
</div>
<div class="example-container">
<p>This problem is also known as the Cardano-Tartaglia problem.</p>
<p>Pick two numbers <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> such that:</p>
<ul>
<li>they sum to 8: <span class="math inline">\(x+y = 8 \Rightarrow y = (8-x)\)</span></li>
<li>their product, <span class="math inline">\(xy\)</span>, and difference, <span class="math inline">\(x-y\)</span>, is maximised</li>
</ul>
<p>Today this problem is easy to solve (because of calculus): we need to maximise <span class="math inline">\(f(x) = (xy)(x-y) = x(8-x)(x-(8-x)) = -2x^3 + 24x^2 - 64x\)</span>.</p>
<p>Solution: <span class="math inline">\(x = 4(1-\frac{1}{\sqrt{3}})\)</span>.</p>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example: 1954 – Max-flow</p>
</div>
<div class="example-container">
<p>Consider a directed graph <span class="math inline">\(\mathcal{G}(V,E)\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture6.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Let each edge <span class="math inline">\(ij\)</span> have a capacity <span class="math inline">\(c_{ij}\)</span>.</p>
<p>We are looking for a map <span class="math inline">\(f:E\rightarrow \mathbb{R}_{\geq 0}\)</span> such that</p>
<ul>
<li><span class="math inline">\(f(ij)\leq c_{ij}\)</span></li>
<li>Flow is conserved at each vertex: incoming = outgoing</li>
</ul>
<p>With these conditions, we want to maximise flow across the graph: <span class="math inline">\(\max \ |f|\)</span> over all routes through the graph.</p>
</div>
</div>
</section>
<section id="notations" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="notations"><span class="header-section-number">1.2</span> Notations</h2>
<ul>
<li>For parts of this course, we work with functionals on normed vector spaces. We usually consider functions on subsets of <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
</ul>
<p><span class="math display">\[\begin{aligned}
&amp;f: D\subset\mathbb{R}^n \rightarrow \mathbb{R},\\
&amp;f: E \rightarrow \mathbb{R}, \ E \textnormal{ a normed vector space}.
\end{aligned}\]</span></p>
<ul>
<li>We usually assume <span class="math inline">\(f\)</span> is differentiable, specifically Fréchet or Gâteaux differentiable. Nevertheless, we denote gradient (or derivative) of <span class="math inline">\(f\)</span> by <span class="math inline">\(\nabla f\)</span>:</li>
</ul>
<p><span class="math display">\[\nabla f = (\frac{\partial f}{\partial x_1},...,\frac{\partial f}{\partial x_n})^T\]</span></p>
</section>
<section id="references" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="references"><span class="header-section-number">1.3</span> References</h2>
<ul>
<li>Convex Optimisation - <span class="citation" data-cites="Boyd_Vandenberghe_2004">Boyd and Vandenberghe (<a href="#ref-Boyd_Vandenberghe_2004" role="doc-biblioref">2004</a>)</span></li>
<li>Foundations of Optimisation - <span class="citation" data-cites="Güler_2010">Güler (<a href="#ref-Güler_2010" role="doc-biblioref">2010</a>)</span></li>
<li>Numerical Optimisation - <span class="citation" data-cites="Nocedal_Wright_2006">Nocedal and Wright (<a href="#ref-Nocedal_Wright_2006" role="doc-biblioref">2006</a>)</span></li>
</ul>
</section>
</section>
<section id="introduction-to-optimisation" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Introduction to Optimisation</h1>
<div id="def-localglobaloptimisers" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Local and Global Minimisers/Maximisers)</strong></span> Let <span class="math inline">\(f:\mathcal{U}\rightarrow\mathbb{R}\)</span>, where <span class="math inline">\(\mathcal{U}\subset\mathbb{R}^n\)</span>. Then <span class="math inline">\(x^*\in\mathcal{U}\)</span> is a:</p>
<ul>
<li><strong>local minimiser</strong> if there exists a neighbourhood of <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(f(x^*)\leq f(x)\)</span> for all <span class="math inline">\(x\)</span> in this neighbourhood. If this is true for all <span class="math inline">\(x\in\mathcal{U}\)</span>, <span class="math inline">\(x^*\)</span> is a <strong>global minimiser</strong>.</li>
<li><strong>local/global maximiser</strong> defined similarly.</li>
</ul>
</div>
<div id="thm-weierstrass" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Weierstrass, 1858)</strong></span> Let <span class="math inline">\(K\)</span> be a compact set in a topological space. Let <span class="math inline">\(f:K\rightarrow\mathbb{R}\)</span> be a continuous function. Then, there exists <span class="math inline">\(x^*\in K\)</span> such that <span class="math inline">\(f(x^*)\leq f(x)\)</span> for all <span class="math inline">\(x\in K\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that <span class="math inline">\(K\)</span> is compact if every open cover of <span class="math inline">\(K\)</span> has a finite subcover.</p>
<p>We prove <a href="#thm-weierstrass" class="quarto-xref">Theorem&nbsp;1</a> in two steps.</p>
<p>Step 1: We show that <span class="math inline">\(f\)</span> is bounded from below. Let <span class="math inline">\(K_n = \{ x\in K | f(x) &gt; n\}\)</span> (open sets). Then,</p>
<p><span class="math display">\[ K = \cup_{n=-\infty}^\infty K_n.\]</span></p>
<p>Since <span class="math inline">\(K\)</span> is compact, there exists a finite subcover <span class="math inline">\(\{K_{n_i}\}_{i=1}^k\)</span></p>
<p><span class="math display">\[ K = \cup_{i=1}^k K_{n_i}. \]</span></p>
<p>Let <span class="math inline">\(\tilde{n} = \min\{n_i \mid i=1,...,k\}\)</span>. Then, <span class="math inline">\(K_{\tilde{n}} = K\)</span>.</p>
<p>By definition of <span class="math inline">\(K_n\)</span>, we now let</p>
<p><span class="math display">\[f^* = \inf\{ f(x) | x\in K\} = \inf\{f(x) | f(x) &gt;\tilde{n}\} &gt; -\infty. \]</span></p>
<p>Therefore <span class="math inline">\(f\)</span> is bounded from below.</p>
<p>Step 2: We show that <span class="math inline">\(f\)</span> has a gloal minimizer. Suppose otherwise and let:</p>
<ul>
<li><span class="math inline">\(Y_n = \{x\in K | f(x) &gt; f^* + \frac{1}{n}\}\)</span> (open)</li>
<li><span class="math inline">\(K = \cup_{n=1}^\infty Y_n\)</span> again. Using compactness and a similar argument as before, there exists <span class="math inline">\(\tilde{n}&gt;1\)</span> such that <span class="math inline">\(K = Y_{\tilde{n}}\)</span>.</li>
</ul>
<p>Thus we have that <span class="math inline">\(f(x)&gt;f^* + \frac{1}{\tilde{n}}\)</span> for all <span class="math inline">\(x\in K\)</span>. This, however, contradicts the definition of <span class="math inline">\(f^*\)</span> as the infimum. ■</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="figures/Picture5.jpg" class="img-fluid" style="width:80.0%"></p>
</div></div><p>Remark: Compactness is a good assumption in finite dimensional spaces, but often not in infinite dimensional spaces. The reason is that this assumption is too strong. For example, the unit ball in <span class="math inline">\(C^0([0,1])\)</span> is not compact. (In fact, this statement can be proved using Weierstrass theorem! You can show there exists a minimiser which is not continuous.) Weierstrass theorem doesn’t work well for infinite dimensions.</p>
<p>We now go about <strong>relaxing compactness</strong>.</p>
<div id="thm-globalmin" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> Suppose <span class="math inline">\(F: E\rightarrow\mathbb{R}\)</span> is continuous. If <span class="math inline">\(f\)</span> has a non-empty compact sublevel set, i.e.&nbsp;<span class="math inline">\(\{x\in E | f(x) \leq \alpha\}\)</span> for some <span class="math inline">\(\alpha\in\mathbb{R}\)</span>, then <span class="math inline">\(f\)</span> achieves a <em>global minimum</em>.</p>
</div>
<p>This means that if the set of values of a function that are less than <span class="math inline">\(\alpha\)</span> is finite, a minimum exists.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="figures/Picture7.jpg" class="img-fluid" style="width:80.0%"></p>
</div></div><div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Pretty straightforward. For an idea, see the figure.</p>
</div>
<div id="def-CoerciveFunction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Coercive function)</strong></span> A function <span class="math inline">\(f:E\rightarrow\mathbb{R}\)</span> is call coercive if</p>
<p><span class="math display">\[ ||x||\rightarrow\infty \ \ \Rightarrow \ \ f(x) \rightarrow\infty .\]</span></p>
<p>Note: here, <span class="math inline">\(E\)</span> is any normed vector space.</p>
</div>
<div id="prp-globalmin" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1</strong></span> If <span class="math inline">\(f:D\rightarrow\mathbb{R}\)</span>, where <span class="math inline">\(D\)</span> is closed and <span class="math inline">\(f\)</span> is continuous, then <span class="math inline">\(f\)</span> achieves a <em>global minimum</em> on <span class="math inline">\(D\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since <span class="math inline">\(F\)</span> is continuous and <span class="math inline">\(D\)</span> is closed, we have that <span class="math inline">\(\{x\in D | f(x) \leq \alpha\}\)</span>, <span class="math inline">\(\alpha\in\mathbb{R}\)</span>, is closed.</p>
<p>We claim that this set is also bounded: Suppose otherwise, then there is a sequence <span class="math inline">\(||x_n||\rightarrow\infty\)</span>. Then, using coercivity <span class="math inline">\(|f(x)|\rightarrow\infty\)</span> on this sublevel set, which is a contradiction! Thus, the sublevel set is also bounded.</p>
<p>By the previous result, since the sublevel set is compact, we have a global minimum. ■</p>
</div>
<section id="unconstrained-optimisation" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="unconstrained-optimisation"><span class="header-section-number">2.1</span> Unconstrained Optimisation</h2>
<p>Let <span class="math inline">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>,</p>
<p><span class="math display">\[ \min_{x\in\mathbb{R}^n} \ \ f(x) \]</span></p>
<p>is an unconstrained optimisation problem.</p>
<section id="necessary-conditions-of-optimality" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="necessary-conditions-of-optimality"><span class="header-section-number">2.1.1</span> Necessary Conditions of Optimality</h3>
<p>Consider a <span class="math inline">\(C^1\)</span>-function <span class="math inline">\(f\)</span> on <span class="math inline">\(\mathbb{R}^n\)</span> and suppose tht <span class="math inline">\(x^*\)</span> is a local minimum of <span class="math inline">\(f\)</span> in some open set <span class="math inline">\(D\subset \mathbb{R}^n\)</span>. Note that <span class="math inline">\(x^*\)</span> is in the interior of <span class="math inline">\(D\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture8.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>Let <span class="math inline">\(d\)</span> be some vector in <span class="math inline">\(\mathbb{R}^n\)</span>, and <span class="math inline">\(\alpha\in\mathbb{R}\)</span>. Consider:</p>
<p><span class="math display">\[g(\alpha) = f(x^* + \alpha d).\]</span></p>
<p>Note that: <span class="math inline">\(\alpha = 0\)</span> is the local minimum of <span class="math inline">\(g\)</span>. Using a first order approximation:</p>
<p><span class="math display">\[ g(\alpha) = g(0) + g'(0)(\alpha - 0) + \mathcal{O}(\alpha),\]</span></p>
<p>where <span class="math inline">\(\lim_{\alpha\rightarrow } \frac{\mathcal{O}(\alpha)}{\alpha} = 0\)</span>.</p>
<div id="lem-lemma" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> We have that <span class="math inline">\(g'(0) = 0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose otherwise. Then for <span class="math inline">\(\varepsilon &gt;0\)</span> small enough and <span class="math inline">\(\alpha \neq 0\)</span> with <span class="math inline">\(|\alpha|&lt;\varepsilon\)</span>, we have</p>
<p><span class="math display">\[\begin{aligned}
|g'(0) \alpha| &gt; |\mathcal{O}(\alpha)|\\
\Rightarrow g(\alpha) - g(0) &lt; g'(0) \alpha + |g'(0)\alpha|
\end{aligned}\]</span></p>
<p>If we let <span class="math inline">\(\alpha\)</span> have the opposite sign (since <span class="math inline">\(\alpha\)</span> is any number in <span class="math inline">\(\mathbb{R}\)</span>) of <span class="math inline">\(g'(0)\)</span>, then <span class="math inline">\(g(\alpha) &lt; g(0)\)</span>. Which is a contradiction! Thus <span class="math inline">\(g'(0)=0\)</span>. ■</p>
</div>
<p>Now, with <span class="math inline">\(g'(0) = 0\)</span>, we consider what this means for <span class="math inline">\(f\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
g'(\alpha) &amp;= \nabla f (x^* + \alpha d) \cdot \vec{d} \\
&amp;= \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^* + \alpha d) \cdot d_i
\end{aligned}\]</span></p>
<p>Then, <span class="math inline">\(g'(0) = \nabla f(x^*) \cdot d = 0\)</span> for all <span class="math inline">\(d\in\mathbb{R}^n\)</span>. Thus, <span class="math inline">\(\nabla f(x^*) = 0\)</span>. This is the <em>first necessary condition</em> of optimality.</p>
</section>
<section id="sufficient-conditions-of-optimality" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="sufficient-conditions-of-optimality"><span class="header-section-number">2.1.2</span> Sufficient Conditions of Optimality</h3>
<p>Consider now the function <span class="math inline">\(f\)</span>, this time in <span class="math inline">\(C^2\)</span>.</p>
<p><span class="math display">\[g(\alpha) = g(0) + g'(0) \alpha + \frac{1}{2}g''(0)\alpha^2 + \mathcal{O}(\alpha^2),\]</span></p>
<p>with <span class="math inline">\(\lim_{\alpha\rightarrow 0} \frac{\mathcal{O}(\alpha^2)}{\alpha^2} = 0\)</span>.</p>
<div id="lem-secondorder" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2</strong></span> If <span class="math inline">\(x^*\)</span> is a local minimum, then <span class="math inline">\(g''(0) \geq 0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose otherwise. I.e. <span class="math inline">\(g''(0) &lt; 0\)</span>. Then, or <span class="math inline">\(\alpha\)</span> small enough, we have</p>
<p><span class="math display">\[\frac{1}{2} |g''(0)|\alpha^2 &gt; \mathcal{O}(\alpha^2). \]</span></p>
<p>Since we know that <span class="math inline">\(g'(0) = 0 \Rightarrow g(\alpha) - g(0) &lt; \frac{1}{2} g''(0)\alpha^2 + \frac{1}{2}|g''(0)|\alpha^2\)</span> and <span class="math inline">\(\frac{1}{2}g''(0)\alpha^2 &lt;0\)</span>, then $g()- g(0) &lt;0 $, which is contradicts <span class="math inline">\(g(0)\)</span> being the minimiser! ■</p>
</div>
<p>Let us compute <span class="math inline">\(g''(0)\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
g''(\alpha) &amp;= \frac{d}{dx} g'(\alpha) \\
&amp;= \frac{d}{d\alpha} \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x^* + \alpha d) d_i \\
&amp;= \sum_{i=1}^n \sum_{j=1}^n \frac{\partial^2 f}{\partial x_i \partial x_j} (x^* + \alpha d) d_i d_j
\end{aligned}\]</span></p>
<p>Then, <span class="math inline">\(g''(0) = \sum_{i,j = 1}^n \frac{\partial^2 f}{\partial x_i \partial x_j} (x^*)d_i d_j = d^T \nabla^2 f(x^*) d\)</span>. And since <span class="math inline">\(g''(0)\geq 0\)</span>, then <span class="math inline">\(d^T \nabla^2 f(x^*) d \geq 0\)</span> for all <span class="math inline">\(d\)</span>, which implies <span class="math inline">\(\nabla^2 f(x^*)\)</span> is symmetric and positive semidefinite.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Symmetric matrices
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider an <span class="math inline">\(n\times n\)</span> real symmetric matrix <span class="math inline">\(A\in\mathbb{R}^{n\times n}\)</span>. It is well known and easy to show that <span class="math inline">\(A\)</span> has real eigenvalues, denote these by <span class="math inline">\(\lambda_1,...,\lambda_n\in\mathbb{R}\)</span>, with corresponding eigenvectors <span class="math inline">\(\vec{u}_1,...,\vec{u}_n\)</span> such that <span class="math inline">\(\langle \vec{u}_i, \vec{u}_j\rangle = 0\)</span> when <span class="math inline">\(i\neq j\)</span>, and <span class="math inline">\(||\vec{u}_i|| = 1\)</span>. Let now</p>
<p><span class="math display">\[ \Lambda = \left[\begin{array}{ccc}
\lambda_1 &amp; ... &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; ... &amp; \lambda_n
\end{array}\right] = \texttt{diag}(\lambda_1,...,\lambda_n), \ \ U = \left[\vec{u}_1,...,\vec{u}_n \right].\]</span></p>
<p>Note that <span class="math inline">\(AU = \left[A\vec{u}_1,...,A\vec{u}_n \right] = \left[\lambda_1\vec{u}_1,...,\lambda_n\vec{u}_n \right] = U\Lambda\)</span>.</p>
<p>Since <span class="math inline">\(U^T U = I_n\)</span> (identity), then <span class="math inline">\(A = A(U U^T) = (AU)U^T = U\Lambda U^T\)</span>. (note: <span class="math inline">\(U\)</span> is non-singular).</p>
<div id="thm-spectraldecomp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (Spectral Decomposition of Symmetric Matrices)</strong></span> Let <span class="math inline">\(A\)</span> be a real symmetric matrix. Then, <span class="math inline">\(A\)</span> can be written as</p>
<p><span class="math display">\[ A = U \Lambda U^T.\]</span></p>
</div>
<div id="cor-PSD" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1</strong></span> <span class="math inline">\(A\)</span> is positive semidefinite if and only if all eigenvalues of <span class="math inline">\(A\)</span> are non-negative. Similarly, positive definite if and only if all eigenvalues are strictly positive.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By the previous result,</p>
<p><span class="math display">\[\begin{aligned}
d^T A d &amp;= d^T U\Lambda U^T d \\
&amp;= (U^Td)^T \Lambda (U^T d)
\end{aligned}.\]</span></p>
<p>Since <span class="math inline">\(U\)</span> is non-singular, we have that <span class="math inline">\(d^T A d\geq 0\)</span> for all <span class="math inline">\(d\)</span> if and only if <span class="math inline">\(d^T \Lambda d\geq 0\)</span> for all <span class="math inline">\(d\)</span>.</p>
<p><span class="math display">\[ \begin{aligned}
d^T \Lambda d &amp;= \sum_{i=1}^n \lambda_a d_i^2 \geq 0 \ \ \forall d_i \\
&amp;\Rightarrow \lambda_i \geq 0 \ \forall i.
\end{aligned}\]</span></p>
<p>The proof of the second part is similar. ■</p>
</div>
</div>
</div>
<p>It is easy to see that the necessary conditions we have are not sufficient. For example, take <span class="math inline">\(f: x\mapsto x^3\)</span>. However, if <span class="math inline">\(f\)</span> is in <span class="math inline">\(C^2\)</span> and <span class="math inline">\(\nabla f (X^*) = 0\)</span>, <span class="math inline">\(\nabla^2f(x^*) \prec 0\)</span>, then <span class="math inline">\(x^*\)</span> is a minimiser.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To show this, note that</p>
<p><span class="math display">\[f(x^* + \alpha d) = f(x^*) + \frac{1}{2}\alpha^2 d^T\nabla^2 f(x^*) d + ... \ .\]</span></p>
<p>Again, we can choose $small enough to have</p>
<p><span class="math display">\[ \frac{\alpha^2}{2} d^T \nabla^2 f(x^*) d &gt; |\mathcal{O}(\alpha^2)|.\]</span></p>
<p>Since <span class="math inline">\(\nabla^2 f(x^*)\)</span> is positive definite, then <span class="math inline">\(f(x^* + \alpha d) &gt; f(x^*)\)</span> for all <span class="math inline">\(d\)</span>. Thus, <span class="math inline">\(x^*\)</span> is a local minimum! ■</p>
</div>
<p>Note that: <span class="math inline">\(\alpha\)</span> depends on <span class="math inline">\(d\)</span>. If <span class="math inline">\(|\alpha| &lt; \varepsilon^*(d)\)</span>, different <span class="math inline">\(d\)</span> will result in different <span class="math inline">\(\varepsilon\)</span>, thus, choose the smallest <span class="math inline">\(\varepsilon\)</span>. Without loss of generality assume <span class="math inline">\(||d|| = 1\)</span>, <span class="math inline">\(\varepsilon^* = \min(\varepsilon(d))\)</span> which results from Weierstrass theorem.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture9.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<div class="example">
<div class="example-header">
<p>Example: Arithmetic-Geometric mean inequality</p>
</div>
<div class="example-container">
<p>We show that: for <span class="math inline">\(x_i&gt;0 \in\mathbb{R} \ \forall i\)</span></p>
<p><span class="math display">\[ (x_1 \cdot ... \cdot x_n)^{1/n} \leq \frac{1}{n} \sum_{i=1}^n x_i .\]</span></p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(y_i = \ln(x_i)\)</span> and let</p>
<p><span class="math display">\[ f(y_1,...,y_n) \triangleq \frac{1}{n} \sum_{i=1}^n e^{y_i} - e^{\frac{y_1 + ... + y_n}{n}} \]</span></p>
<p>(this came from replacing <span class="math inline">\(x_i\)</span> with <span class="math inline">\(e^{y_i}\)</span> in the statement.)</p>
<p>Note that we want tot show <span class="math inline">\(f(y_1,..,y_n) \geq 0\)</span>. This is the same problem as above. It is enough to show that <span class="math inline">\(f\)</span> acheives its minimum value at zero.</p>
<p>To do this, use the first order condition of optimality:</p>
<p><span class="math display">\[ \frac{\partial f}{\partial y_i} = \frac{1}{n} e^{y_i} - \frac{1}{n} e^{\frac{y_1 + ... + y_n}{n}} \ \ \forall i \]</span></p>
<p>which are zero if and only if <span class="math inline">\(e^{y_i} = e^{\frac{y_1 + ... + y_n}{n}} \ \Rightarrow y_i = \frac{y_1 + ... + y_n}{n}\)</span> for all <span class="math inline">\(i\)</span>. (Check this for yourselves).</p>
<p>This gives a system of linear equations with solution <span class="math inline">\(y_i = \frac{Y}{n}\)</span> for all <span class="math inline">\(i\)</span>. i.e.&nbsp;picked all <span class="math inline">\(y_i\)</span> to be the same for any value of <span class="math inline">\(Y\in\mathbb{R}\)</span>.</p>
<p>Note that <span class="math inline">\(f\left(\frac{Y}{n},...,\frac{Y}{n}\right) = 0\)</span>.</p>
<p>We now need to justify that we have a global minimiser. (And, ideally a unique one.)</p>
<p>We reduce the problem over the set</p>
<p><span class="math display">\[\{(y_1,...,y_n) | \sum_{i=1}^n y_i = Y\}\]</span></p>
<p>for some <span class="math inline">\(Y\in\mathbb{R}\)</span>. Using this, and by eliminating <span class="math inline">\(y_n\)</span> (by saying <span class="math inline">\(y_n = Y - y_1 - ... - y_{n-1}\)</span>), define</p>
<p><span class="math display">\[g(y_1,...,y_{n-1}) = \frac{1}{n}\left( e^{y_1} + ... + e^{y_{n-1}} + e^{Y - y_1 - ... - y_{n-1}}\right) - e^\frac{Y}{n}.\]</span></p>
<p>It is easy to check now that <span class="math inline">\(g\)</span> has a unique critical point</p>
<p><span class="math display">\[y_i = \frac{Y}{n}, \ \ i\in\{1,...,n-1\}.\]</span></p>
<p>The function <span class="math inline">\(g\)</span> is coercive and hence has a global minimum, which has to be</p>
<p><span class="math display">\[y_i = \frac{Y}{n}, \ \ i\in\{1,...,n-1\}.\]</span> ■</p>
</div>
</div>
</div>
</section>
</section>
<section id="constrained-optimisation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="constrained-optimisation"><span class="header-section-number">2.2</span> Constrained Optimisation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture10.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<ul>
<li>we assume for now that <span class="math inline">\(D\)</span> is not of lower dimension, <span class="math inline">\(D\subseteq \mathbb{R}^n\)</span></li>
<li><span class="math inline">\(D\)</span> is assumed to be closed and bounded</li>
</ul>
<p>We wish to study the minimisers of</p>
<p><span class="math display">\[f: D\rightarrow \mathbb{R}\]</span></p>
<p>We can even assume that <span class="math inline">\(f\)</span> has a minimiser in <span class="math inline">\(D\)</span>. This minimiser could be on the boundary.</p>
<p>Note that we cannot conclude that if</p>
<p><span class="math display">\[ g(\alpha) = f(x^* + \alpha d),\]</span></p>
<p>where <span class="math inline">\(d\)</span> is a feasible direction, we have <span class="math inline">\(g'(0) = 0\)</span> (since <span class="math inline">\(\alpha\)</span> is dependent on <span class="math inline">\(d\)</span>). We still have that, for <span class="math inline">\(\alpha&gt;0\)</span> small enough,</p>
<p><span id="eq-constrained"><span class="math display">\[\begin{aligned}
&amp; g(\alpha) - g(0) &lt; g'(0)\alpha + |g'(0)|\alpha \\
\Rightarrow &amp; g(\alpha) - g(0) &lt; \alpha (g'(0) + |g'(0)|).
\end{aligned} \tag{1}\]</span></span></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Claim
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(g'(0)\geq 0\)</span>.</p>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose otherwise (ie. <span class="math inline">\(g'(0)&lt;0\)</span>). By choosing <span class="math inline">\(\alpha&gt;0\)</span> small enough and using <a href="#eq-constrained" class="quarto-xref">Equation&nbsp;1</a> we have</p>
<p><span class="math display">\[\begin{aligned}
&amp; g(\alpha) &lt; g(0) \\
\Rightarrow &amp; g'(0) \geq 0
\end{aligned}\]</span>■</p>
</div>
<p>Recall $g’(0) = f(x^*)d $. (from first order optimality).</p>
<p>One can also conclude a weaker version of the second order condition of optimality:</p>
<p><span class="math display">\[d^T \nabla^2 f(x^*) d \geq 0\]</span></p>
<p>when <span class="math inline">\(\nabla f(x^*)\cdot d = 0\)</span>, which we call the second order condition of optimality.</p>
<p><img src="figures/Picture11.jpg" class="img-fluid" style="width:100.0%"></p>
<p>We cannot use <span class="math inline">\(x^* + \alpha d\)</span>! Doing so would take us out of the surface.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Objective
</div>
</div>
<div class="callout-body-container callout-body">
<p>Minimise <span class="math inline">\(f(x)\)</span> subject to <span class="math inline">\(D\)</span>.</p>
</div>
</div>
<p>This optmisation problem can be written as</p>
<p><span class="math display">\[\left\{ \begin{array}{ll}
\min f(x) &amp; \\
h_i(x) = 0 &amp; i=1,...,m
\end{array} \right. ,\]</span></p>
<p>where <span class="math inline">\(D\)</span> is described usin functions <span class="math inline">\(h_i\)</span>.</p>
<p>What if instead we use some curves, <span class="math inline">\(\gamma(\cdot)\)</span>, passing through <span class="math inline">\(x^*\)</span>, ie. we take <span class="math inline">\(\gamma(0) = x^*\)</span>. Let us take these curves to be at least <span class="math inline">\(C^1\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture12.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>Let us define <span class="math inline">\(g(\alpha) = f(\gamma(\alpha))\)</span>. We can now carry the same argument as before, using <span class="math inline">\(g(\alpha) = g(0) + \alpha g'(0) + \mathcal{O}(\alpha)\)</span>, and argue that</p>
<p><span class="math display">\[\nabla f(\gamma(0))\cdot \gamma'(0) = 0.\]</span></p>
<p>We need to study this <span class="math inline">\(\gamma'(0)\)</span> more carefully.</p>
<p>Note that <span class="math inline">\(\gamma'(0)\)</span> lives in the so called tangent space at <span class="math inline">\(x^*\)</span> which we denote by <span class="math inline">\(T_{x^*}D\)</span> (this is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>). We have not used <span class="math inline">\(h_i(\gamma(\alpha)) = 0 \ \forall i\)</span>, differentiating to get <span class="math inline">\(\nabla h_i(\gamma(\alpha)) \cdot \gamma'(\alpha) = 0\)</span>. Choosing <span class="math inline">\(\alpha = 0\)</span>, we have</p>
<p><span class="math display">\[\nabla h_i(x^*) \cdot \gamma'(0) = 0\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>We assume that <span class="math inline">\(x^*\)</span> is <em>regular</em>. This means that the gradients <span class="math inline">\(\nabla h_i(x^*)\)</span> for <span class="math inline">\(i\in\{1,...,m\}\)</span> are linearly independent.</p>
</div>
</div>
<p>Note that since <span class="math inline">\(x^*\)</span> is regular, it’s also true that any vector <span class="math inline">\(d\in\mathbb{R}^n\)</span> for which <span class="math inline">\(\nabla h_i(x^*) \cdot d = 0\)</span> for all <span class="math inline">\(i\)</span> has to lie in the tangent space <span class="math inline">\(T_{x^*}D\)</span>. (check this on your own)</p>
<p>In summary, we showed that</p>
<p><span id="eq-constrainedmin"><span class="math display">\[\nabla f(x^*)\cdot d = 0 \tag{2}\]</span></span></p>
<p>for all <span class="math inline">\(d\)</span> such that <span class="math inline">\(\nabla h_i(x^*)\cdot d = 0\)</span> for all <span class="math inline">\(i\)</span>.</p>
<div id="prp-lagrange" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2</strong></span> If <a href="#eq-constrainedmin" class="quarto-xref">Equation&nbsp;2</a> holds, and <span class="math inline">\(x^*\)</span> is regular, then we have that</p>
<p><span class="math display">\[\nabla f(x^*) \in \texttt{span}\{\nabla h_i(x^*), \ i\in\{1,...,m\}\}.\]</span></p>
<p>ie. there exists a <span class="math inline">\(\lambda_i^*\in\mathbb{R}\)</span> such that</p>
<p><span class="math display">\[\nabla f(x^*) + \lambda_1^* \nabla h_1(x^*) + ... + \lambda_m^* \nabla h_m(x^*) = 0.\]</span></p>
<p>These <span class="math inline">\(\lambda_i^*\)</span> are called <em>Langrange multipliers</em>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Use contradiction, do on your own.</p>
</div>
</section>
</section>
<section id="convex-analysis" class="level1 page-columns page-full" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Convex Analysis</h1>
<p>We start by discussing affine sets and maps.</p>
<div id="def-affine_set" class="theorem definition page-columns page-full">
<p><span class="theorem-title"><strong>Definition 3 (Affine Set - informal)</strong></span> A set <span class="math inline">\(A\subseteq\mathbb{R}^n\)</span> is <em>affine</em> if for all <span class="math inline">\(x,y\in A\)</span>, the full line connecting <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is in <span class="math inline">\(A\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="figures/Picture14.jpg" class="img-fluid" style="width:80.0%"></p>
</div></div></div>
<div id="def-affine_set_forma" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Affine Set - formal)</strong></span> A set <span class="math inline">\(A\subseteq \mathbb{R}^n\)</span> is <em>affine</em> if and only if for all <span class="math inline">\(x,y\in\ A\)</span> and <span class="math inline">\(t\in\mathbb{R}\)</span>,</p>
<p><span class="math display">\[ (1-t) x + t y \in A.\]</span></p>
</div>
<div id="thm-linearsubspace" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4</strong></span> <span class="math inline">\(V\subseteq\mathbb{R}^n\)</span> is a linear subspace if and only if <span class="math inline">\(V\)</span> is affine and <span class="math inline">\(0\in V\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(\Rightarrow\)</span> immediate</p>
<p><span class="math inline">\(\Leftarrow\)</span> Suppose <span class="math inline">\(V\)</span> is affine and <span class="math inline">\(0\in V\)</span>. Need to check scalar multiplication and vector addition.</p>
<p>Scalar multiplication: let <span class="math inline">\(\alpha \in\mathbb{R}\)</span>, <span class="math inline">\(u\in V\)</span>, then <span class="math inline">\(\alpha u = \alpha u + (1-\alpha)0 \in V\)</span></p>
<p>Addition: let <span class="math inline">\(u,w\in V\)</span>. Then, <span class="math inline">\(u + w = 2 (\frac{1}{2}u + \frac{1}{2}w)\)</span>, which is in the form <span class="math inline">\(\alpha u + (1-\alpha)w\)</span>, and is therefore in <span class="math inline">\(V\)</span> since <span class="math inline">\(V\)</span> is affine. Since <span class="math inline">\(V\)</span> is also closed under scalar multiplication, we conclude that <span class="math inline">\(u+w\in V\)</span>. ■</p>
</div>
<div id="thm">
<p>A set <span class="math inline">\(A\in\mathbb{R}^n\)</span> is <em>affine</em> if and only if <span class="math inline">\(A-a\)</span> is a linear subspace for all <span class="math inline">\(a\in\mathbb{R}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>In text <span class="citation" data-cites="Güler_2010">(<a href="#ref-Güler_2010" role="doc-biblioref">Güler 2010</a>)</span>.</p>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture15.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>One of the most important examples of an affine set in <span class="math inline">\(\mathbb{R}^n\)</span> is an <span class="math inline">\((n-1)\)</span> dimensional hyperplane. A hyperplane is an <span class="math inline">\((n-1)\)</span> dimensional set.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Analytical description of a Hyperplane
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any hyperplane <span class="math inline">\(H\subseteq\mathbb{R}^n\)</span>, we have</p>
<p><span class="math display">\[H = L + a,\]</span></p>
<p>for some <span class="math inline">\(a\in \mathbb{R}^n\)</span>, where <span class="math inline">\(L\)</span> is an <span class="math inline">\((n-1)\)</span> dimensional linear subspace. So,</p>
<p><span class="math display">\[\begin{aligned}
H &amp;= \{x\in\mathbb{R}^n \mid \langle x,b\rangle = 0\} + a \\
&amp;= \{x+a\in\mathbb{R}^n \mid \langle x,b\rangle = 0\} \\
&amp;= \{y\in\mathbb{R}^n \mid \langle y-a,b\rangle = 0\} \\
&amp;= \{y\in\mathbb{R}^n \mid \langle y,b\rangle = \alpha\},
\end{aligned}\]</span></p>
<p>for some <span class="math inline">\(\alpha\in\mathbb{R}\)</span>.</p>
<p>Thus, any <span class="math inline">\((n-1)\)</span> dimensional affine set in <span class="math inline">\(\mathbb{R}^n\)</span> can be written as <span class="math inline">\(\{y\in\mathbb{R}^n \mid \langle y,b\rangle = \alpha \}\)</span> for some <span class="math inline">\(b\in\mathbb{R}^n, \ \alpha\in\mathbb{R}\)</span>.</p>
</div>
</div>
<div id="def-affinemap" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Affine map)</strong></span> A mapping <span class="math inline">\(F:\mathbb{R}^n\rightarrow \mathbb{R}^n\)</span> is <em>affine</em> if for all <span class="math inline">\(x,y\in\mathbb{R}^n\)</span> and <span class="math inline">\(t\in\mathbb{R}\)</span>,</p>
<p><span class="math display">\[F((1-t)x + ty) = (1-t) F(x) + t F(y) .\]</span></p>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture31.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>If <span class="math inline">\(A\in M_{n\times m}(\mathbb{R})\)</span> and <span class="math inline">\(b\in\mathbb{R}^n\)</span>, <span class="math inline">\(F(x) = Ax + b\)</span> is affine.</p>
</div>
</div>
<section id="convex-sets" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="convex-sets"><span class="header-section-number">3.1</span> Convex Sets</h2>
<div id="def-convexset" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 (Convex set - informal)</strong></span> A set <span class="math inline">\(S\subseteq\mathbb{R}^n\)</span> is <em>convex</em> is for all <span class="math inline">\(x,y\in S\)</span>, the line segment connecting <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is in <span class="math inline">\(S\)</span>.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture16.jpg" class="img-fluid figure-img" style="width:42.0%"></p>
<figcaption>Convex.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture17.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Not convex.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<div id="def-convexsetformal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Convex set - formal)</strong></span> <span class="math inline">\(S\subseteq\mathbb{R}^n\)</span> is <em>convex</em> if for all <span class="math inline">\(x,y\in S\)</span> and <span class="math inline">\(\lambda\in[0,1]\)</span>,</p>
<p><span class="math display">\[(1-\lambda) x + \lambda y\in S.\]</span></p>
</div>
<div class="example">
<div class="example-header">
<p>Examples</p>
</div>
<div class="example-container">
<ol type="1">
<li><p><span class="math inline">\(S_1 = \{(x,y)\in\mathbb{R}^2 \mid x^2 + y^2 \leq 1\}\)</span> unit disk is convex. <img src="figures/Picture18.jpg" class="img-fluid" style="width:20.0%"></p></li>
<li><p><span class="math inline">\(S_2 = \{(x,0)\in\mathbb{R}^2 \mid x\in \mathbb{R}\}\)</span> (x-axis) is affine <span class="math inline">\(\Rightarrow\)</span> convex.</p></li>
<li><p><span class="math inline">\(S_3 = \{(x,y)\in\mathbb{R}^2 \mid y\geq e^x\}\)</span> is convex. <img src="figures/Picture19.jpg" class="img-fluid" style="width:20.0%"></p></li>
<li><p><span class="math inline">\(S_4 = \{(x,y)\in\mathbb{R}^2 \mid y \geq\sin(x)\}\)</span> is <strong>not</strong> convex. <img src="figures/Picture20.jpg" class="img-fluid" style="width:20.0%"></p></li>
</ol>
</div>
</div>
<p>If we are given <span class="math inline">\(m\)</span> real numbers <span class="math inline">\(\lambda_1,...,\lambda_m\)</span> such that <span class="math inline">\(\lambda_i\geq 0\)</span> and <span class="math inline">\(\sum_{i=1}^m \lambda_i = 1\)</span>, then these are called <em>convex coefficients</em>, and a sum</p>
<p><span class="math display">\[\sum_{i=1}^m \lambda_i x_i\]</span></p>
<p>is called a <em>convex combination</em> of vectors <span class="math inline">\(x_i\in\mathbb{R}^n\)</span>, <span class="math inline">\(i=1,...,m\)</span>. When:</p>
<ul>
<li><span class="math inline">\(m=2\)</span>: line <img src="figures/Picture32.jpg" class="img-fluid" style="width:20.0%"></li>
<li><span class="math inline">\(m=3\)</span>: surface <img src="figures/Picture21.jpg" class="img-fluid" style="width:20.0%"></li>
</ul>
<div id="thm-convexset2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5</strong></span> The set <span class="math inline">\(S\subseteq \mathbb{R}^n\)</span> is <em>convex</em> if and only if <span class="math inline">\(S\)</span> contains all convex combinations of its elements.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(\Leftarrow\)</span> Set <span class="math inline">\(m=2\)</span>, then it follows from the definition.</p>
<p><span class="math inline">\(\Rightarrow\)</span> By induction on <span class="math inline">\(m\)</span>.</p>
<p><strong>Base case:</strong> <span class="math inline">\(m=2\)</span>. By definition of convexity, <span class="math inline">\(\lambda x + (1-\lambda)y \in S\)</span>. Let <span class="math inline">\(\lambda_1 = \lambda\)</span>, <span class="math inline">\(\lambda_2 = 1-\lambda\)</span>.</p>
<p><strong>Inductive step:</strong> Assume true for <span class="math inline">\(2,...,m\)</span> and prove for <span class="math inline">\(m+1\)</span>. Show that a convex combination of <span class="math inline">\(m+1\)</span> elements of <span class="math inline">\(S\)</span> lies in <span class="math inline">\(S\)</span>. Let <span class="math inline">\(x_1,...,x_{m+1}\in S\)</span>, <span class="math inline">\(\lambda_1,...,\lambda_{m+1}\in\mathbb{R}\)</span>, <span class="math inline">\(\lambda_i\geq 0\)</span> and <span class="math inline">\(\sum_{i=1}^{m+1}\lambda_i =1\)</span>.</p>
<p>Assume <span class="math inline">\(\lambda_{m+1}\neq 0\)</span>, then</p>
<p><span class="math display">\[ \begin{aligned}
\sum_{k=1}^{m+1} \lambda_k x_k &amp;= \lambda_{m+1} x_{m+1} + \sum_{k=1}^{m+1} \lambda_k x_k \\
&amp;= \lambda_{m+1} x_{m+1} + (1-\lambda) \sum_{k=1}^{m} \frac{\lambda_k}{1-\lambda_{m+1}} x_k.
\end{aligned}\]</span></p>
<p>Need to show <span class="math inline">\(\sum_{k=1}^{m} \frac{\lambda_k}{1-\lambda_{m+1}} x_k\)</span> is in <span class="math inline">\(S\)</span>.</p>
<p>Claim: <span class="math inline">\(\frac{\lambda_k}{1-\lambda_{m+1}}\)</span> are convex coefficients for all <span class="math inline">\(k=1,...,m\)</span>.</p>
<p><span class="math display">\[\sum_{k=1}^m \frac{\lambda_k}{1-\lambda_{m+1}} = \frac{1}{1-\lambda_{m+1}} \sum_{k=1}^m \lambda_k = \frac{1-\lambda_{m+1}}{1-\lambda_{m+1}} = 1\]</span></p>
<p>Thus, <span class="math inline">\(\sum_{k=1}^m \frac{\lambda_k}{1-\lambda_{m+1}} x_k\)</span> is a convex combination of <span class="math inline">\(m\)</span> points in <span class="math inline">\(S\)</span>, and is therefore in <span class="math inline">\(S\)</span>.</p>
<p>Thus, <span class="math inline">\(\lambda_{m+1} x_{m+1} + (1-\lambda) \sum_{k=1}^{m} \frac{\lambda_k}{1-\lambda_{m+1}} x_k \in S\)</span>. ■</p>
</div>
<p>We now introduce an object called the <em>convex hull</em>. Given a set <span class="math inline">\(X\subseteq\mathbb{R}^n\)</span>, we want to construct a convex set which contains <span class="math inline">\(X\)</span> and is as small as possible. Define</p>
<p><span class="math display">\[ \texttt{co} \ (X) = \left\lbrace \sum_{k=1}^m \lambda_k x_k \mid x_k\in X, \lambda_k \geq 0, \sum_{k=1}^m \lambda_k = 1, \textnormal{ for all } m \right\rbrace .\]</span></p>
<p>The convex hull is the set of all convex combinations of elements of <span class="math inline">\(X\)</span>. Imagine fitting an elastic band over the set.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture33.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Claims
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><span class="math inline">\(\texttt{co} \ (X)\)</span> is a convex set.</p></li>
<li><p><span class="math inline">\(\texttt{co} \ (X)\)</span> is minimal in the sense that if <span class="math inline">\(S\subseteq\mathbb{R}^n\)</span> is convex and <span class="math inline">\(X\subseteq S\)</span>, then <span class="math inline">\(\texttt{co} \ (X) \subseteq S\)</span>.</p></li>
</ol>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em> (of 1). </span>Let <span class="math inline">\(x,y\in\texttt{co} \ (X)\)</span>. Then we can write</p>
<p><span class="math display">\[ x = \sum_{k+1}^m \lambda_k x_k, \ \ y = \sum_{\ell=1}^p \mu_\ell y_\ell, \]</span></p>
<p>for some <span class="math inline">\(m,p, \{x_k\}, \{y_k\}\in X\)</span>, convex coefficients <span class="math inline">\(\lambda_1,...,\lambda_m\)</span>, <span class="math inline">\(\mu_1,...,\mu_p\)</span>. Now let <span class="math inline">\(\gamma\in[0,1]\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
(1-\gamma)x + \gamma y &amp;= \sum_{k=1}^m (1-\gamma)\lambda_k x_k  + \sum_{\ell=1}^p \gamma\mu_ell y_\ell \\
&amp;= \sum_{k=1}^{m+p} \alpha_k z_k
\end{aligned}\]</span></p>
<p>where</p>
<p><span class="math display">\[ \begin{aligned}
&amp;\alpha_k = \left\lbrace\begin{array}{ll}
(1-\gamma)\lambda_k, &amp; 1\leq k\leq m\\
\gamma \mu_{k-m}, &amp; m+1 \leq k\leq m+p
\end{array} \right.  \\
&amp;z_k = \left\lbrace\begin{array}{ll}
x_k, &amp; 1\leq k\leq m \\
y_{k-m}, &amp; m+1\leq k \leq m+p
\end{array} \right.
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(\sum_{k=1}^{m+p} \alpha_k = 1\)</span> (prove this on your own). Thus, we have shown <span class="math inline">\((1-\gamma)x + \gamma y \in \texttt{co} \ (X)\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\texttt{co} \ (X)\)</span> is a convex set. ■</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em> (of 2). </span>If <span class="math inline">\(x\in\texttt{co} \ (X)\)</span> then <span class="math inline">\(x = \sum_{k=1}^m \lambda_k x_k\)</span>, <span class="math inline">\(x_k\in X\)</span> for all <span class="math inline">\(k=1,...,m\)</span>.</p>
<p><span class="math inline">\(X\subseteq S \Rightarrow\)</span> if <span class="math inline">\(x_k\in X\)</span> then <span class="math inline">\(x_k\in S\)</span>. Then, <span class="math inline">\(\texttt{co} \ (X)\subseteq S\)</span>. ■</p>
</div>
<div id="thm-convexhull" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6</strong></span> If <span class="math inline">\(X\subseteq\mathbb{R}^n\)</span> and <span class="math inline">\(x\in\texttt{co} \ (X)\)</span>, then</p>
<p><span class="math display">\[ x = \sum_{k=1}^{n+1} \lambda_k x_k \]</span></p>
<p>for the convex coefficients <span class="math inline">\(\lambda_k\)</span> and points <span class="math inline">\(x_k\in X\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>In text <span class="citation" data-cites="Güler_2010">(<a href="#ref-Güler_2010" role="doc-biblioref">Güler 2010</a>)</span>.</p>
</div>
<section id="operations-that-preserve-convexity" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="operations-that-preserve-convexity"><span class="header-section-number">3.1.1</span> Operations that Preserve Convexity</h3>
<ol type="1">
<li>If <span class="math inline">\(F:\mathbb{R}^n\rightarrow\mathbb{R}^m\)</span> is affine and <span class="math inline">\(S\subseteq\mathbb{R}^n\)</span> is convex, <span class="math inline">\(F(S)\subseteq\mathbb{R}^m\)</span> is convex.</li>
</ol>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(y_1, y_2\in F(S)\)</span>. Then, <span class="math inline">\(y_1 = F(x_1)\)</span> and <span class="math inline">\(y_2 = F(x_2)\)</span> for <span class="math inline">\(x_1, x_2\in S\)</span>. Then, for any <span class="math inline">\(\lambda\in[0,1]\)</span></p>
<p><span class="math display">\[\begin{aligned}
(1-\lambda) y_1 + \lambda y_2 &amp;= (1-\lambda)F(x_1) + \lambda F(x_2) \\
&amp;= F((1-\lambda)x_1 + \lambda x_2) \in F(S)
\end{aligned}\]</span> ■</p>
</div>
<ol start="2" type="1">
<li><p><span class="math inline">\(\cap_{i\in I} S_i\)</span> is convex if <span class="math inline">\(S_i\)</span> are convex for all <span class="math inline">\(i\in I\)</span>. This is not true for unions.</p></li>
<li><p>Minkowski sum. <span class="math inline">\(S_1 + S_2 + ... + S_m = \{x_1 + x_2 + ... + x_m \mid x_i\in S_i\}\)</span>, the Minkowski sum preserves convexity.</p></li>
</ol>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p><span class="math inline">\(S_1 = D\)</span> (unit disk) in <span class="math inline">\(\mathbb{R}^2\)</span>, <span class="math inline">\(S_2\)</span> is the <span class="math inline">\(x\)</span>-axis.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture22.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Of Minkowski sum preserving convexity.</p>
<p>Let <span class="math inline">\(x, y\in S_1 + ... + S_m\)</span> and <span class="math inline">\(S_i\)</span> convex for all <span class="math inline">\(i=1,...,m\)</span>. Then, <span class="math inline">\(x = x_1 + ... + x_m\)</span>, <span class="math inline">\(y = y_1 + ... + y_m\)</span>, <span class="math inline">\(x_i\in s_i\)</span>, <span class="math inline">\(y_i\in S_i\)</span>. For any $</p>
<p><span class="math display">\[(1-\lambda)x + \lambda y = \underbrace{(1-\lambda) x_1 + \lambda y_1}_{\in S_1} + ... \underbrace{(1-\lambda) x_m + \lambda y_m}_{\in S_m}\]</span></p>
<p>By convexity, <span class="math inline">\((1-\lambda)x_i + \lambda y_i \in S_i\)</span>. Thus, <span class="math inline">\((1-\lambda)x + \lambda y \in S_1 + ... S_m\)</span>. ■</p>
</div>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>Minkowski sums don’t necessarily preserve set properties.</p>
<p><span class="math display">\[\begin{aligned}
&amp; S_1 = \{(x,y)\in\mathbb{R}^ \mid y\geq e^x\} \textnormal{ closed} \\
&amp; S_2 = x-\textnormal{axis}
\end{aligned}\]</span></p>
<p><span class="math inline">\(S_1+S_2 = \{(x,y)\in\mathbb{R}^2 \mid y&gt;0 \}\)</span> is open, but still convex.</p>
</div>
</div>
<div id="def-convexcone" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8</strong></span> Let <span class="math inline">\(C\)</span> be a set in a vector space <span class="math inline">\(V\)</span>. Then <span class="math inline">\(C\)</span> is called a <em>cone</em> if <span class="math inline">\(t\cdot x\in C\)</span> for all <span class="math inline">\(x\in C\)</span>, <span class="math inline">\(t\geq 0\)</span>. We call <span class="math inline">\(C\)</span> a <em>convex cone</em> is <span class="math inline">\(C\)</span> is additionally convex.</p>
</div>
<div id="lem-convexcone" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3</strong></span> A set <span class="math inline">\(C\)</span> is a convex cone if and only if for all <span class="math inline">\(x, y\in C\)</span> and <span class="math inline">\(t&gt;0\)</span> we have that <span class="math inline">\(tx\in C\)</span> and <span class="math inline">\(x+y\in C\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(C\)</span> is a convex cone, then</p>
<p><span class="math display">\[ x + y = 2\underbrace{\left(\frac{x}{2} + \frac{y}{2}\right)}_{\in C \textnormal{ by convexity}}  \in C\]</span></p>
<p>Conversely, if <span class="math inline">\(tx\in C\)</span> and <span class="math inline">\(x+y\in C\)</span> for all <span class="math inline">\(x,y\in C\)</span>, then for any <span class="math inline">\(\lambda\in(0,1)\)</span></p>
<p><span class="math display">\[\lambda x + (1-\lambda)y = \lambda \left(x + \frac{1-\lambda}{\lambda} y \right) \in C.\]</span> ■</p>
</div>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>The cone of symmetric positive definite matrices, denoted <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>Note that for <span class="math inline">\(A,B\in \mathbf{S}\)</span>:</p>
<ul>
<li><span class="math inline">\(A+B\in \mathbf{S}\)</span>,</li>
<li><span class="math inline">\(t A\in \mathbf{S}\)</span>, <span class="math inline">\(t&gt;0.\)</span></li>
</ul>
<p>Thus, <span class="math inline">\(\mathbf{S}\)</span> is a convex cone.</p>
</div>
</div>
</section>
</section>
<section id="convex-functions" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="convex-functions"><span class="header-section-number">3.2</span> Convex Functions</h2>
<div id="def-convexfunction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9</strong></span> For <span class="math inline">\(f:V\rightarrow\mathbb{R}\)</span>, <span class="math inline">\(V\)</span> a vector space, we say that <span class="math inline">\(f\)</span> is a <em>convex function</em> if</p>
<p><span class="math display">\[f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y),\]</span></p>
<p>where <span class="math inline">\(0\leq\lambda \leq 1\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="figures/Picture23.jpg" class="img-fluid" style="width:90.0%"></p>
</div></div><p>What convexity tells us is that the values of the function are below the line between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<div id="def-epigraph" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10</strong></span> For a function <span class="math inline">\(f:V\rightarrow\mathbb{R}\)</span> the <em>epigraph</em> of <span class="math inline">\(f\)</span> is defined to be</p>
<p><span class="math display">\[ \texttt{epi}(f) = \{(x,\alpha) \mid x\in V, \alpha \in \mathbb{R}, f(x) \leq \alpha\}.\]</span></p>
</div>
<div id="prp-epigraph" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3</strong></span> The function <span class="math inline">\(f\)</span> is convex if and only if <span class="math inline">\(\texttt{epi}(f)\)</span> is convex.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose that <span class="math inline">\(f\)</span> is convex. Let <span class="math inline">\((x_1,\alpha_1), (x_2,\alpha_2)\in\texttt{epi}(f)\)</span>. For <span class="math inline">\(\lambda\in[0,1]\)</span> we have:</p>
<p><span class="math display">\[\begin{aligned}
f(\lambda x_1 + (1-\lambda)x_2) &amp;\leq \lambda f(x_1) + (1-\lambda) f(x_2) \\
&amp;\leq \underbrace{\lambda \alpha_1 + (1-\lambda)\alpha_2}_{\alpha} \\
&amp;\leq \alpha
\end{aligned}\]</span></p>
<p>Then, <span class="math inline">\(\lambda(x_1,\alpha_1) + (1-\lambda)(x_2,\alpha_2) = (\lambda x_1 + (1-\lambda)x_2, \alpha) \in \texttt{epi}(f)\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\texttt{epi}(f)\)</span> is convex.</p>
<p>For the other direction, suppose that <span class="math inline">\(\texttt{epi}(f)\)</span> is convex. Then, for all <span class="math inline">\(x_1, x_2\in V\)</span>, <span class="math inline">\(\lambda\in[0,1]\)</span> we have</p>
<p><span class="math display">\[\lambda (x_1, f(x_1)) + (1-\lambda)(x_2, f(x_2)) \in \texttt{epi}(f). \]</span></p>
<p>Hence, by definition of epigraph,</p>
<p><span class="math display">\[f(\lambda x_1 + (1-\lambda)x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2),\]</span></p>
<p>which means that <span class="math inline">\(f\)</span> is convex. ■</p>
</div>
<div class="example">
<div class="example-header">
<p>Examples</p>
</div>
<div class="example-container">
<ol type="1">
<li><span class="math inline">\(f: x\mapsto e^x\)</span></li>
<li><span class="math inline">\(f: x\mapsto -\log(x)\)</span>, <span class="math inline">\(x\in\mathbb{R}\)</span> positive</li>
<li><span class="math inline">\(f: x\mapsto \langle a, Ax\rangle + \langle b,x\rangle\)</span> where <span class="math inline">\(A\in\mathcal{M}^n\)</span> is symmetric positive definite and <span class="math inline">\(b\)</span> is a vector in <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
</ol>
<p>are convex functions.</p>
</div>
</div>
<div id="thm-jensens" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 (Jensen’s inequality)</strong></span> Let <span class="math inline">\(f:C\rightarrow\mathbb{R}\)</span> be a convex function and <span class="math inline">\(\lambda_i \in[0,1]\)</span>, <span class="math inline">\(\sum_{i=1}^n \lambda_i = 1\)</span>. Then,</p>
<p><span class="math display">\[ f(\sum_{i=1}^n \lambda_i x_i) \leq \sum_{i=1}^n \lambda_i f(x_i), \ x_i\in C.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Note that <span class="math inline">\((x_i,f(x_i))\in\texttt{epi}(f)\)</span>. Since <span class="math inline">\(f\)</span> is convex, <span class="math inline">\(\texttt{epi}(f)\)</span> is a convex set. Thus</p>
<p><span class="math display">\[\begin{aligned}
&amp; \sum_{i=1}^n \lambda_i (x_i,f(x_i)) \in\texttt{epi}(f) \\
\Rightarrow &amp; \left(\sum_{i=1}^n \lambda_i x_i, \sum_{i=1}^n \lambda_i f(x_i) \right) \in\texttt{epi}(f)\\
\Rightarrow &amp; f\left(\sum_{i=1}^n \lambda_i x_i\right) \leq \sum_{i=1}^n \lambda_i f(x_i)
\end{aligned}\]</span> ■</p>
</div>
<p>Note that we can easily conclude the arithmetic geometric mean inequality using this: take <span class="math inline">\(\ln(\cdot)\)</span>, which is a <em>concave function</em> (ie. the negative of a convex function),</p>
<p><span class="math display">\[\begin{aligned}
&amp; \ln\left(\frac{1}{2} x + \frac{1}{2}y\right) \geq \frac{1}{2}\ln(x) + \frac{1}{2}\ln(y), \ x,y&gt;0 \\
\Rightarrow &amp; \ln\left(\frac{1}{2} x + \frac{1}{2}y\right) \geq \ln((xy)^{1/2}) \\
\Rightarrow &amp; \frac{1}{2} (x + y) \geq \sqrt(xy)
\end{aligned}\]</span></p>
<section id="conditions-of-convexity" class="level3 page-columns page-full" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="conditions-of-convexity"><span class="header-section-number">3.2.1</span> Conditions of Convexity</h3>
<div id="thm-firstorderconv" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 (First order condition of convexity)</strong></span> Given <span class="math inline">\(C\subseteq\mathbb{R}^n\)</span> convex, <span class="math inline">\(f:C\rightarrow\mathbb{R}\)</span> continuously differentiable, we have that <span class="math inline">\(f\)</span> is convex if and only if</p>
<p><span class="math display">\[ \langle \nabla f(x), y-x\rangle \leq f(y) - f(x), \ \ \forall x,y\in C.\]</span></p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="figures/Picture24.jpg" class="img-fluid" style="width:100.0%"></p>
</div></div><div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(\Rightarrow\)</span>. Assume that <span class="math inline">\(f\)</span> is convex. Then for <span class="math inline">\(\alpha\in(0,1)\)</span></p>
<p><span class="math display">\[\begin{aligned}
f(x+ \alpha(y-x)) &amp;= f((1-\alpha)x + \alpha y) \\
&amp;\leq (1-\alpha) f(x) + \alpha f(x).
\end{aligned}\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[ \frac{f(x + \alpha(y-x)) - f(x)}{\alpha} \leq f(y) - f(x).\]</span></p>
<p>Taking the limit <span class="math inline">\(\alpha\rightarrow 0\)</span> results in</p>
<p><span class="math display">\[\langle \nabla f(x), y-x\rangle \leq f(y) - f(x), \]</span></p>
<p>which is the expression given in the theorem. Note, <span class="math inline">\(\langle \nabla f(x), y-x\rangle\)</span> is the directional derivative of <span class="math inline">\(f\)</span> in the direction <span class="math inline">\(y-x\)</span>, as a result of the limit.</p>
<p><span class="math inline">\(\Leftarrow\)</span>. Now suppose <span class="math inline">\(\langle \nabla f(x), y-x\rangle \leq f(y) - f(x)\)</span>, for all <span class="math inline">\(x,y\in C\)</span>. Let <span class="math inline">\(x_\alpha = (1-\alpha)x + \alpha y\)</span> where <span class="math inline">\(\alpha\in(0,1)\)</span>. Consider</p>
<p><span class="math display">\[\begin{aligned}
&amp; (1-\alpha) &amp; f(x) \geq f(x_\alpha) + \langle \nabla f(x_\alpha), x- x_\alpha\rangle \\
+ &amp; &amp; \\
&amp; \alpha &amp; f(y)\geq f(x_\alpha) + \langle \nabla f(x_\alpha),y-x_\alpha\rangle .
\end{aligned}\]</span></p>
<p>Expanding and simplifying results in</p>
<p><span class="math display">\[ \begin{aligned}
&amp;(1-\alpha) f(x) + \alpha f(y) \geq f(x_\alpha) + \langle \nabla f(x_\alpha), \underbrace{(1-\alpha)x + \alpha y - x_\alpha}_{=0}\rangle \\
\Rightarrow &amp;(1-\alpha) f(x) + \alpha f(y) \geq f(x_\alpha) \\
\Rightarrow &amp; (1-\alpha) f(x) + \alpha f(y) \geq f((1-\alpha)x + \alpha y),
\end{aligned}\]</span></p>
<p>which holds for all <span class="math inline">\(x,y\in C\)</span>, <span class="math inline">\(\alpha\in\mathbb{R}\)</span>. Thus, <span class="math inline">\(f\)</span> is convex. ■</p>
</div>
<p>Note, a function can also be <em>strictly convex</em>. Strict convexity is when <a href="#def-convexfunction" class="quarto-xref">Definition&nbsp;9</a> holds for <span class="math inline">\(x\neq y\)</span>, ie.</p>
<p><span class="math display">\[ f((1-\alpha)x + \alpha y) &lt; (1-\alpha) f(x) + \alpha f(y).\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture26.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<div id="thm-secondorderconv" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (Second order condition of convexity)</strong></span> Let <span class="math inline">\(C\subseteq\mathbb{R}^n\)</span>, and suppose <span class="math inline">\(f:C\rightarrow\mathbb{R}\)</span> is twice continuously differentiable. Then, <span class="math inline">\(f\)</span> is convex if and only if the Hessian of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span> is positive semidefinite for all <span class="math inline">\(x\in C\)</span>, ie.</p>
<p><span class="math display">\[\nabla^2 f(x) \succeq 0 \ \forall x\in C.\]</span></p>
<p>Moreover, if <span class="math inline">\(\nabla^2 f(x)\)</span> is strictly positive definite at every <span class="math inline">\(x\in C\)</span>, then <span class="math inline">\(f\)</span> is strictly convex. For concavity, the opposite holds.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(\Rightarrow\)</span>. Suppose <span class="math inline">\(f\)</span> is convex, by <a href="#thm-firstorderconv" class="quarto-xref">Theorem&nbsp;8</a></p>
<p><span class="math display">\[ f(x) + \langle \nabla f(x) ,\alpha d\rangle \leq f(x + \alpha d), \]</span></p>
<p>for all <span class="math inline">\(d\in\mathbb{R}^n\)</span>, <span class="math inline">\(\alpha &gt;0\)</span>. Fix <span class="math inline">\(x\)</span> and consider the Taylor expansion of <span class="math inline">\(f(x+ \alpha d)\)</span> where <span class="math inline">\(d\)</span> is the variable</p>
<p><span class="math display">\[\begin{aligned}
&amp; f(x) + \langle \nabla f(x) ,\alpha d\rangle \leq f(x) \langle \nabla f(x) + \alpha d\rangle + \frac{\alpha^2}{2} d^T \nabla^2 f(x) d + \mathcal{O}(\alpha^2) \\
\Rightarrow &amp; \frac{1}{2} d^T \nabla^2 f(x) d + \frac{\mathcal{O}(\alpha^2)}{\alpha^2}  \geq 0.
\end{aligned}\]</span></p>
<p>Taking <span class="math inline">\(\alpha\rightarrow 0\)</span>, we conclude that</p>
<p><span class="math display">\[\nabla^2 f(x) \succeq 0.\]</span></p>
<p><span class="math inline">\(\Leftarrow\)</span>. Now, suppose <span class="math inline">\(\nabla^2 f(x) \succeq 0\)</span>. Then, by the mean value theorem, we know that there exists <span class="math inline">\(z = x+\lambda y\)</span> where <span class="math inline">\(0\leq \lambda\leq 1\)</span> such that</p>
<p><span class="math display">\[ f(y) = f(x) + \langle \nabla f(x), y-x\rangle + \frac{1}{2}\langle \nabla^2 f(z)(y-x), y-x\rangle .\]</span></p>
<p>Since <span class="math inline">\(\nabla^2 f(z) \succeq 0\)</span>, we have</p>
<p><span class="math display">\[ f(y) \geq f(x) + \langle \nabla f(x), y-x\rangle, \]</span></p>
<p>which is the first order condition of convexity. Therefore, <span class="math inline">\(f\)</span> is convex. ■</p>
</div>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>A function that is strictly convex, but has <span class="math inline">\(\nabla^2 f(x) \nsucc 0\)</span>, is <span class="math inline">\(f(x) = x^4\)</span>. <span class="math inline">\(\nabla^2 f(x) = 12x^2 = 0\)</span> at <span class="math inline">\(x=0\)</span>, it is not strictly positive definite. Thus, the converse does not hold in the second part of <a href="#thm-secondorderconv" class="quarto-xref">Theorem&nbsp;9</a>.</p>
</div>
</div>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>Let <span class="math inline">\(f(x) =  \frac{1}{2} x^T A x + \langle b,x\rangle\)</span>, <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span> symmetric and <span class="math inline">\(b\in\mathbb{R}^n\)</span>. Then, <span class="math inline">\(f\)</span> is convex if and only if <span class="math inline">\(\nabla^2 f(x) \succeq 0\)</span>, that is</p>
<p><span class="math display">\[\nabla^2 f(x) = A \succeq 0,\]</span></p>
<p>ie. <span class="math inline">\(f\)</span> is convex if and only if <span class="math inline">\(A\)</span> is positive semidefinite.</p>
</div>
</div>
<div id="prp-convexlinearcomp" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4</strong></span> A function <span class="math inline">\(f:\mathbb{R}^n \rightarrow \mathbb{R}\)</span> is convex if and only if <span class="math inline">\(g:\mathbb{R}\rightarrow\mathbb{R}\)</span> given by</p>
<p><span class="math display">\[ g(t) = f(x + yt)\]</span></p>
<p>is convex for all <span class="math inline">\(x,y\in\mathbb{R}^n\)</span>.</p>
</div>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>Let <span class="math inline">\(f:S^n_{&gt;0} \rightarrow\mathbb{R}\)</span>, <span class="math inline">\(f(X) = \log\det X\)</span>.</p>
<p>Claim: <span class="math inline">\(f\)</span> is concave.</p>
<p>Let <span class="math inline">\(g(t) = \log\det(Z + tY)\)</span>, <span class="math inline">\(Z,Y\in S^n_{&gt;0}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
g(t) &amp;= \log\det(Z + tY) \\
&amp;= \log\det(Z^{1/2}(I + tZ^{-1/2}YZ^{-1/2})Z^{1/2}) \\
&amp;= \log\det(Z) + \log\det(I + tZ^{-1/2}YZ^{-1/2}).
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\lambda_1,...,\lambda_n\)</span> be the eigenvalues of <span class="math inline">\(Z^{-1/2}Y Z^{-1/2}\)</span>. Then</p>
<p><span class="math display">\[\begin{aligned}
&amp; g(t) = \log\det(Z) + \sum_{i=1}^n \log(1 + t\lambda_i)\\
&amp; g'(t) = \sum_{i=1}^n \frac{\lambda_i}{1 + t\lambda_i} \\
&amp; g''(t) = \sum_{i=1}^n \frac{-\lambda_{i}^2}{(1+t\lambda_i)^2} \leq 0
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(g\)</span> is concave.</p>
</div>
</div>
<p>We now note that convex functions have convex sublevel sets. This is, given <span class="math inline">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span> convex,</p>
<p><span class="math display">\[S_\alpha = \{x \mid f(x) \leq \alpha \}\]</span></p>
<p>is convex.</p>
<p>But what about the converse? Consider</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture25.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>We can see that the sublevel sets <span class="math inline">\(S_\alpha\)</span> are convex, but clearly <span class="math inline">\(f\)</span> is not.</p>
<div id="def-quasiconv" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 11</strong></span> Any function that has convex sublevel sets is called <em>quasi-convex</em>. Similarly, <em>quasi-concave</em> functions have convex superlevel sets.</p>
</div>
<p>Note that all convex functions are also quasi-convex.</p>
<div id="prp-convexglobalmin" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5</strong></span> Let <span class="math inline">\(f:C\rightarrow\mathbb{R}\)</span> be a convex function, where <span class="math inline">\(C\subseteq\mathbb{R}^n\)</span> is convex. Then any local minimum of <span class="math inline">\(f\)</span> is a global minimum.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(x^*\)</span> be a local minimimiser of <span class="math inline">\(f\)</span>, and let</p>
<p><span class="math display">\[ x_\lambda = (1-\lambda)x^* + \lambda x,\]</span></p>
<p>where <span class="math inline">\(x\in C\)</span>. Then <span class="math inline">\(f(x^*)\leq f(x_\lambda)\)</span> for <span class="math inline">\(\lambda\)</span> small enough, so</p>
<p><span class="math display">\[ f(x^*) \leq f((1-\lambda)x^* + \lambda x) \leq (1-\lambda)f(x^*) + \lambda f(x)\]</span></p>
<p>Thus, <span class="math inline">\(f(x^*)\leq f(x)\)</span> independantly of <span class="math inline">\(\lambda\)</span> and for all <span class="math inline">\(x\)</span>. Which confirms <span class="math inline">\(x^*\)</span> is a global minimiser.</p>
<p>If additionally we assume strict convexity of <span class="math inline">\(f\)</span>, then the global minimiser, if it exists, is unique.</p>
<p>Suppose otherwise: suppose <span class="math inline">\(x_1^*\)</span> and <span class="math inline">\(x_2^*\)</span> are both global minimisers, <span class="math inline">\(x_1^*\neq x_2^*\)</span>. Then</p>
<p><span class="math display">\[f(\lambda x_1^* + (1-\lambda)x_2^*) &lt; \lambda f(x_1^*) + (1-\lambda)f(x_2^*). \]</span></p>
<p>Since <span class="math inline">\(x_1^*\)</span> and <span class="math inline">\(x_2^*\)</span> are both global minimisers, <span class="math inline">\(f(x_1^*) = f(x_2^*) \triangleq F^*\)</span>. Then</p>
<p><span class="math display">\[f(\lambda x_1^* + (1-\lambda)x_2^*) &lt; F^*, \]</span></p>
<p>which is a contradiction. ■</p>
</div>
<div id="thm-variational" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10 (Variational Inequality)</strong></span> Let <span class="math inline">\(f:C\rightarrow\mathbb{R}\)</span>, <span class="math inline">\(C\subseteq\mathbb{R}^n\)</span> is convex, and <span class="math inline">\(f\)</span> is continuously differentiable. Suppose <span class="math inline">\(x^*\in C\)</span> is a minimiser of <span class="math inline">\(f\)</span>. Then</p>
<p><span id="eq-variational"><span class="math display">\[ \langle \nabla f(x^*) , x-x^*\rangle \geq 0  \tag{3}\]</span></span></p>
<p>holds for all <span class="math inline">\(x\in C\)</span>. Moreover, if <span class="math inline">\(f\)</span> is convex, and <span class="math inline">\(x^*\)</span> satisfies <a href="#eq-variational" class="quarto-xref">Equation&nbsp;3</a>, then <span class="math inline">\(x^*\)</span> is a global minimiser of <span class="math inline">\(f\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We have already seen the first part. For the second statement, suppose the variational inequality . Using the first order conditions of convexity</p>
<p><span class="math display">\[ f(x) \geq f(x^*)+ \underbrace{\langle \nabla f(x^*),x-x^*\rangle}_{\geq 0}\]</span></p>
<p>for all <span class="math inline">\(x\in C\)</span>, then <span class="math inline">\(f(x) \geq f(x^*)\)</span>, and <span class="math inline">\(x^*\)</span> is a global minimiser. ■</p>
</div>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>Consider</p>
<p><span class="math display">\[\begin{aligned}
\min &amp; f(x) \\
\texttt{st} &amp; Ax = b,
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(f\)</span> is convex and differentiable, <span class="math inline">\(A\in\mathcal{M}_n(\mathbb{R})\)</span>, <span class="math inline">\(b\in\mathbb{R}^n\)</span></p>
<p>Let <span class="math inline">\(C = \{x\in\mathbb{R}^n \mid Ax = b\}\)</span>. This is a convex set. By the variational inequality</p>
<p><span class="math display">\[ \langle \nabla f(x^*), x - x^* \rangle \geq 0, \ \ \forall x\in C\]</span></p>
<p>for <span class="math inline">\(x^*\)</span> a solution to the optimisation problem. Since <span class="math inline">\(Ax^* = b\)</span>, we can rewrite this as</p>
<p><span class="math display">\[ \langle \nabla f(x^*), z\rangle \geq 0\]</span></p>
<p>for all <span class="math inline">\(z\in\ker(A)\)</span>. Hence,</p>
<p><span class="math display">\[\langle \nabla f(x^*),z \rangle = 0, \ \ z\in\ker(A).\]</span></p>
<p>Recall that <span class="math inline">\(\ker(A)^\perp = \textnormal{Im}(A^T)\)</span>, thus <span class="math inline">\(\nabla f(x^*)\in \textnormal{Im}(A^T)\)</span> OR <span class="math inline">\(P_{\ker(A)} \nabla f(x^*) = 0\)</span> (project <span class="math inline">\(f\)</span> into the kernel).</p>
</div>
</div>
<p>Recall the projection mapping. Consider a set <span class="math inline">\(C\in\mathbb{R}^n\)</span> non-empty, closed, and convex. The projection map <span class="math inline">\(P_C\)</span> takes a point <span class="math inline">\(x\)</span> and finds a point in <span class="math inline">\(C\)</span> that is closest (in Euclidean distance) to <span class="math inline">\(x\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture27.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:30.0%"></p>
</figure>
</div>
<div id="thm-projection" class="theorem page-columns page-full">
<p><span class="theorem-title"><strong>Theorem 11</strong></span> Take <span class="math inline">\(C\)</span> as above. We have the following, where <span class="math inline">\(z\in C\)</span> and <span class="math inline">\(x\in\mathbb{R}^n\)</span>,</p>
<p><span class="math display">\[ \langle x- P_C(x), z - P_c(x)\rangle \leq 0\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="figures/Picture28.jpg" class="img-fluid" style="width:90.0%"></p>
</div></div></div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Note that <span class="math inline">\(x^* = P_C(x)\)</span> is a solution to</p>
<p><span class="math display">\[\min_{z\in C} \frac{1}{2} ||z-x||^2 .\]</span></p>
<p>This function is coercive <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x^*\)</span> exists. We can use <a href="#thm-variational" class="quarto-xref">Theorem&nbsp;10</a>. For all <span class="math inline">\(z\in C\)</span></p>
<p><span class="math display">\[\begin{aligned}
&amp;\langle x^*-x,z-x^*\rangle\geq 0 \\
\Rightarrow &amp; \langle x-x^*,z-x^*\rangle \leq 0 \\
\Rightarrow &amp; \langle x-P_C(x), z-P_C(x)\rangle \leq 0.
\end{aligned}\]</span> ■</p>
</div>
<p>This result holds true for Hilbert spaces. To show the existence of a minimiser we take a minimising sequence <span class="math inline">\(\{x_n\}_{n=1}^\infty\)</span>,</p>
<p><span class="math display">\[||x-x_n|| \rightarrow d\in\inf\{||x-z||, z\in C\}.\]</span></p>
<p>Note that Hilbert spaces are complete, and hence if we show that this sequence is Cauchy then it is convergent.</p>
<div class="page-columns page-full"><p><span class="math display">\[\begin{aligned}
||x_n-x_m||^2 &amp;= ||(x-x_n) - (x-x_m)||^2 \\
&amp;= 2(||x-x_n||^2 + ||x-x_m||^2) - ||(x-x_n) + (x-x_m)||^2  \\
&amp;= 2(||x-x_n||^2 + ||x-x_m||^2) - 4||x - \frac{x_n+x_m}{2}||^2 \\
&amp;\leq 2(||x-x_n|^2 + ||x-x_m||^2) - 4d^2.
\end{aligned}\]</span> </p><div class="no-row-height column-margin column-container"><span class="margin-aside">By the parallelogram law.</span></div></div>
<p>For all <span class="math inline">\(\varepsilon &gt;0\)</span>, there exists <span class="math inline">\(N\)</span> large enough such that <span class="math inline">\(n,m&gt;N\)</span> then</p>
<p><span class="math display">\[2(\underbrace{||x-x_n||^2}_{\rightarrow d^2} + \underbrace{||x-x_M||^2}_{\rightarrow d^2}) - 4d^2 \rightarrow 0.\]</span></p>
<p>So, <span class="math inline">\(||x_n - x_m||^2 \leq 0\)</span>. Therefore this is a Cauchy sequence, and the rest follows.</p>
<div id="prp-projectionnonexpansive" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 6</strong></span> Let <span class="math inline">\(C\subset\mathbb{R}\)</span> be non-empty, closed, and convex. Then the projection map <span class="math inline">\(P_C:\mathbb{R}^n\rightarrow C\)</span> is <em>non-expansive</em>.</p>
<p>i.e.&nbsp;<span class="math inline">\(||P_C(y) - P_C(x)|| \leq ||y-x||\)</span> for all <span class="math inline">\(x,y\in\mathbb{R}^n\)</span>. Consequently, <span class="math inline">\(P_C\)</span> is continuous.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that</p>
<p><span class="math display">\[\begin{aligned}
&amp; \langle x-P_C(x), P_C(y) - P_C(x)\rangle \leq 0\\
-&amp;\underline{\langle y-P_c(y),P_C(x) - P_C(y)\rangle \leq 0} \\
&amp; \langle x-y + P_C(y)-P_C(x), P_C(y) - P_C(x)\rangle \leq 0 \\
\Rightarrow &amp; \langle x-y,P_C(y)-P_C(x)\rangle + ||P_C(y) - P_C(x)||^2 \leq 0 \\
\Rightarrow &amp; ||P_C(y) - P_C(x)||^2 \leq \langle y-x,P_C(y)-P_C(x)\rangle \\
\Rightarrow &amp; ||P_C(y) - P_C(x)||^2 \leq ||y-x|| \ ||P_C(y) - P_C(x)|| .
\end{aligned}\]</span></p>
<p>Where the last line follows from Cauchy-Schwarz. ■</p>
</div>
</section>
</section>
<section id="separation-of-convex-sets" class="level2 page-columns page-full" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="separation-of-convex-sets"><span class="header-section-number">3.3</span> Separation of Convex Sets</h2>
<p>Recall that we denote by <span class="math inline">\(H_{(a,\alpha)}\)</span> the hyperplane (<span class="math inline">\(a\in\mathbb{R}n\)</span>, $)</p>
<p><span class="math display">\[ H_{(a,\alpha)} = \{x\in\mathbb{R}^n \mid \langle a,x\rangle = \alpha \} .\]</span></p>
<p>We also use</p>
<p><span class="math display">\[ H_{(a,\alpha)}^+ = \{x\in\mathbb{R}^n \mid \langle a,x\rangle &gt; \alpha \}, \]</span></p>
<p>and</p>
<p><span class="math display">\[H_{(a,\alpha)}^- = \{x\in\mathbb{R}^n \mid \langle a,x\rangle &lt; \alpha \}.\]</span></p>
<div id="def-separatinghyperplanes" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 12</strong></span> Let <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> be two non-empty sets in <span class="math inline">\(\mathbb{R}^n\)</span>$. Then a hyperplane <span class="math inline">\(H_{(a,\alpha)}\)</span> is called:</p>
<ul>
<li><em>Separating hyperplane</em> for <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> if <span class="math inline">\(C_1\)</span> is contained in <span class="math inline">\(\bar{H}^+\)</span> (<span class="math inline">\(H^+\)</span> closure) and <span class="math inline">\(C_2\)</span> in <span class="math inline">\(\bar{H}^-\)</span>.</li>
<li><em>Strictly separating</em> hyperplane for <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> is <span class="math inline">\(C_1\)</span> is contained in <span class="math inline">\(H^+\)</span> and <span class="math inline">\(C_2\)</span> in <span class="math inline">\(H^-\)</span>.</li>
<li><em>Strongly separating</em> for <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> if there exists <span class="math inline">\(\alpha_1,\alpha_2\in\mathbb{R}\)</span> with <span class="math inline">\(\alpha_1 &gt; \alpha &gt; \alpha_2\)</span> such that <span class="math inline">\(C_1 \subset \bar{H}^+_{(a,\alpha_1)}\)</span> and <span class="math inline">\(C_2\subset \bar{H}^-_{(a,\alpha_2)}\)</span>.</li>
<li><em>Properly separating</em> for <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> if <span class="math inline">\(H\)</span> is a separating hyperplane and not both <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> are contained in <span class="math inline">\(H\)</span>.</li>
<li><em>Supporting hyperplane</em> of <span class="math inline">\(C_1\)</span> at point <span class="math inline">\(\bar{x}\in\bar{C}_1\)</span> if <span class="math inline">\(\bar{x} \in H\)</span> and <span class="math inline">\(H\)</span> separates <span class="math inline">\(\{\bar{x}\}\)</span> and <span class="math inline">\(C_1\)</span>.</li>
</ul>
</div>
<p>Recall that for a set <span class="math inline">\(S\subseteq\mathbb{R}^n\)</span>, the relative interior of <span class="math inline">\(S\)</span> is</p>
<p><span class="math display">\[\textnormal{ri}(S) = \{x\in S\mid \exists \varepsilon &gt; 0, B_\varepsilon(x) \cap \textnormal{aff}(S) \subset S\},\]</span></p>
<p>where <span class="math inline">\(\textnormal{aff}(S)\)</span> is the affine hull of <span class="math inline">\(S\)</span>.</p>
<div id="thm-supportinghyperplane" class="theorem page-columns page-full">
<p><span class="theorem-title"><strong>Theorem 12</strong></span> Let <span class="math inline">\(C\in\mathbb{R}^n\)</span> be a non-empty, convex set, and let <span class="math inline">\(\bar{x}\in\bar{C}\backslash\textnormal{ri}(C)\)</span>. Then, there exists a supporting hyperplane to <span class="math inline">\(C\)</span> at <span class="math inline">\(\bar{x}\)</span>, i.e.&nbsp;there exists a vector <span class="math inline">\(a\in\mathbb{R}^n\)</span> such that</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/Picture29.jpg" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:80.0%"></p>
<figcaption>The supporting hyperplanes are in green.</figcaption>
</figure>
</div>
</div></div><p><span class="math display">\[\langle a,x\rangle \geq \langle a,\bar{x}\rangle\]</span></p>
<p>for all <span class="math inline">\(x\in C\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Consider a sequence <span class="math inline">\(\{x_k\}_{k=1}^\infty\)</span> where <span class="math inline">\(x_k\in C\)</span> approaching <span class="math inline">\(\bar{x}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture30.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>This sequence gives <span class="math inline">\(\{\hat{x}_k\}_{k=1}^\infty\)</span> where <span class="math inline">\(\hat{x}_k = P_{\bar{C}}(x_k)\)</span>. Note that</p>
<p><span id="eq-supphypseries"><span class="math display">\[ \langle \hat{x}_k - x_k, x - \hat{x}_k\rangle \geq 0 \ \ \forall x\in\bar{C}. \tag{4}\]</span></span></p>
<p>We have that</p>
<p><span class="math display">\[ \begin{aligned}
\langle \hat{x}_k - x_k, x\rangle &amp; \geq \langle \hat{x}_k - x_k, \hat{x}_k\rangle \\
&amp; = \langle \hat{x}_k - x_k, \hat{x}_k - x_k\rangle + \langle \hat{x}_k - x_k, x_k\rangle \\
&amp; \geq \langle \hat{x}_k - x_k, x_k \rangle .
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\alpha_k = \frac{\hat{x}_k - x_k}{|| \hat{x}_k - x_k||}\)</span>, then <span class="math inline">\(\langle \alpha_k,x\rangle \geq \langle \alpha_k, x_k\rangle\)</span>. The sequence <span class="math inline">\(\{\alpha_k\}_{k=1}^\infty\)</span> is bounded and has a convergent subsequence. Then, taking the limit,</p>
<p><span class="math display">\[\langle \alpha, x\rangle \geq \langle \alpha, \bar{x}\rangle, \]</span></p>
<p>which proves the result. ■</p>
</div>
<p>Note that the sequence <span class="math inline">\(\{P_{\bar{C}(x_k)\}_{k=1}^\infty\)</span> is convergent to <span class="math inline">\(P_{\bar{C}(\bar{x})} = \bar{x}\)</span> since <span class="math inline">\(P_{\bar{C}}\)</span> is continuous. We could have taken the limit directly from <a href="#eq-supphypseries" class="quarto-xref">Equation&nbsp;4</a>.</p>
<div id="thm-separatinghyperplane" class="theorem">
<p><span class="theorem-title"><strong>Theorem 13</strong></span> Let <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> be two disjoint, non-empty, convex subsets of <span class="math inline">\(\mathbb{R}^n\)</span>. Then, there exists a hyperplane that separates <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>. I.e. there exists a vector <span class="math inline">\(a\in\mathbb{R}^n\)</span> such that</p>
<p><span class="math display">\[ \langle a,x_2\rangle \geq \langle a,x_1\rangle \ \ \forall x_1\in C_1, \ x_2\in C_2.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Consider the convex set</p>
<p><span class="math display">\[ C = C_2 - C_1 = \{x = x_2 - x_1 \mid x_1\in C_1, \ x_2\in C_2\}. \]</span></p>
<p>This set is convex. Since <span class="math inline">\(C_1 \cap C_2 = \emptyset\)</span>, we conclude that <span class="math inline">\(0\notin C\)</span>. Now, by an extension of <a href="#thm-supportinghyperplane" class="quarto-xref">Theorem&nbsp;12</a>, we conclude that there exists a vector <span class="math inline">\(a\neq 0\)</span> passing through 0 such that</p>
<p><span class="math display">\[\langle a,x\rangle \geq \langle a,0\rangle = 0, \ \forall x\in C\]</span></p>
<p>i.e.&nbsp;<span class="math inline">\(\langle a,x_2\rangle \geq \langle a,x_1\rangle\)</span> for all <span class="math inline">\(x_1\in C_1\)</span> and <span class="math inline">\(x_2\in C_2\)</span>. ■</p>
</div>
<div id="thm-strongseparation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 14</strong></span> Let <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> be two non-empty, disjoint, convex sets in <span class="math inline">\(\mathbb{R}^n\)</span>. Suppose that the set <span class="math inline">\(C_2-C_1\)</span> is closed. Then, there exists a hyperplane that separates <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> strongly.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> are disjoint, 0 is not in <span class="math inline">\(C_2-C_1\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture34.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>Let then <span class="math inline">\(\bar{x}_2-\bar{x}_1\)</span> be the vector of minimum norm over <span class="math inline">\(C\)</span> which is well defined.</p>
<p>Let <span class="math inline">\(a=\frac{\bar{x}_2-\bar{x}_1}{2}\)</span> and <span class="math inline">\(\bar{x} = \frac{\bar{x}_2+\bar{x}_1}{2}\)</span>. Note that <span class="math inline">\(a\neq 0\)</span>.</p>
<p>We claim that the hyperplane <span class="math inline">\(H_{(a,\alpha)}\)</span> given by</p>
<p><span class="math display">\[ H_{(a,\alpha)} = \{x\in\mathbb{R}^n \mid \langle a,x\rangle = \alpha \triangleq \langle a, \bar{x}\rangle \}\]</span></p>
<p>strongly separates <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>. Finish the proof on your own, show <span class="math inline">\(\langle a,x_1\rangle &lt; \beta &lt; \langle a, x_2\rangle\)</span> (similar to the previous proof). ■</p>
</div>
<div id="thm-properseparation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 15</strong></span> Two non-empty convex sets <span class="math inline">\(C_1\)</span>, <span class="math inline">\(C_2\)</span> can be properly separated if <span class="math inline">\(\textnormal{ri}(C_1)\)</span> and <span class="math inline">\(\textnormal{ri}(C_2)\)</span> are disjoint.</p>
</div>
<div id="prp-separatinghyperplane" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7</strong></span> Let <span class="math inline">\(C\subset\mathbb{R}^n\)</span> be non-empty and convex, and let <span class="math inline">\(A\)</span> be an affine set such that <span class="math inline">\(\textnormal{ri}(C) \cap A = \emptyset\)</span>. Then, there exists a hyperplane <span class="math inline">\(H\)</span> separating <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span> such that <span class="math inline">\(A\subseteq H\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Existence of a hyperplane <span class="math inline">\(H\)</span> is clear. If <span class="math inline">\(H\)</span> includes <span class="math inline">\(A\)</span>, we are done. Otherwise, there exists <span class="math inline">\(x\in A\backslash H\)</span>.</p>
<p>The affine set <span class="math inline">\(A\)</span> and <span class="math inline">\(H\)</span> must be disjoint, otherwise <span class="math inline">\(y\in A\cap H\)</span> and the line between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> intersects both <span class="math inline">\(\bar{H}^+\)</span> and <span class="math inline">\(\bar{H}^-\)</span>, which is not possible since, since <span class="math inline">\(H\)</span> is separating. Thus, <span class="math inline">\(A\)</span> has to be “parallel” to <span class="math inline">\(H\)</span>. Hence, we can shift the hyperplane <span class="math inline">\(H\)</span> to a new one <span class="math inline">\(\tilde{H}\)</span> that includes <span class="math inline">\(A\)</span>. ■</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture35.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div></div></section>
</section>
<section id="convex-optimisation" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Convex Optimisation</h1>
<div id="thm-systemlinearinequalities" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16</strong></span> Consider a system of linear inequalities</p>
<p><span id="eq-inequalitysystem"><span class="math display">\[ \ell_i(x) &lt; \alpha_i  \tag{5}\]</span></span></p>
<p>where <span class="math inline">\(\ell_i:\mathbb{R}^n \rightarrow\mathbb{R}\)</span> is linear for all <span class="math inline">\(i\in\{1,...,m\}\)</span>. Then, there exists <em>no solution</em> to <a href="#eq-inequalitysystem" class="quarto-xref">Equation&nbsp;5</a> if and only if there exists a set of non-negative scalars <span class="math inline">\(\{\lambda_i\}_{i=1}^m\)</span> not all zero such that</p>
<p><span class="math display">\[ \begin{aligned}
&amp; \sum_{i=1}^m \lambda_i \ell_i(x) = 0 \ \forall x, \\
&amp; \sum_{i=1}^m \lambda_i \alpha_i \leq 0.
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first show that if there is a solution to <a href="#eq-inequalitysystem" class="quarto-xref">Equation&nbsp;5</a> then such scalars cannot exist (contrapositive of <span class="math inline">\(\Leftarrow\)</span>). Suppose otherwise, then <span class="math inline">\(\sum_{i=1}^m \lambda_i\ell_i(x) = 0\)</span> and <span class="math inline">\(\sum_{i=1}^m \lambda_i\alpha_i \leq 0\)</span>. But since there is a solution to <a href="#eq-inequalitysystem" class="quarto-xref">Equation&nbsp;5</a>, we also have</p>
<p><span class="math display">\[\sum_{i=1}^m \lambda_i (\alpha_i - \ell_i(x)) &gt; 0.\]</span></p>
<p>Which contradicts our assumption. Thus, no such scalars <span class="math inline">\(\lambda_i\)</span> exist.</p>
<p>Now for the other side (direct proof of <span class="math inline">\(\Rightarrow\)</span>). Define</p>
<p><span class="math display">\[\begin{aligned}
&amp; A = \{y\in\mathbb{R}^n \mid \exists x\in\mathbb{R}^n, y_i = \ell_i(x) - \alpha_i, i=1,...,m\}, \\
&amp; K = \{y\in\mathbb{R}^n \mid y_i&lt;0, i=1,...,m\}.
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(K\)</span> is an open convex set. Moreover, <span class="math inline">\(K\cap A = \emptyset\)</span>, since vectors in <span class="math inline">\(A\)</span> have at least one entry for which <span class="math inline">\(\ell_i(x) - \alpha_i \geq 0\)</span> since <a href="#eq-inequalitysystem" class="quarto-xref">Equation&nbsp;5</a> is inconsistent. Since <span class="math inline">\(K\cap A=\emptyset\)</span>, by <a href="#prp-separatinghyperplane" class="quarto-xref">Proposition&nbsp;7</a> there exists a hyperplane <span class="math inline">\(H_{(\lambda,\gamma)}\)</span> that separates <span class="math inline">\(K\)</span> and <span class="math inline">\(A\)</span> such that <span class="math inline">\(A\subseteq H\)</span>. In particular,</p>
<p><span class="math display">\[K \subseteq H^- \textnormal{ meaning }\langle \lambda, y\rangle &lt; \gamma \ \forall \ y\in K.\]</span></p>
<p>This implies that <span class="math inline">\(\lambda \geq 0\)</span> and <span class="math inline">\(\gamma \geq 0\)</span>. Also, since <span class="math inline">\(A\subseteq H\)</span>, we have that</p>
<p><span class="math display">\[\begin{aligned}
&amp; \langle \lambda, y\rangle = \gamma \ \forall y\in A, \\
\Rightarrow &amp; \gamma = \sum_{i=1}^m \lambda_i (\ell_i(x) - \alpha_i) = \sum_{i=1}^m \lambda_i \ell_i(x) - \sum_{i=1}^m \lambda_i \alpha_i \ \forall x\in\mathbb{R}^n
\end{aligned}\]</span></p>
<p>Thus <span class="math inline">\(\sum_{i=1}^m \lambda_i \ell_i(x) = 0\)</span> (since <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\lambda\)</span> are fixed), and <span class="math inline">\(- \sum_{i=1}^m \lambda_i \alpha_i = \gamma \Rightarrow \sum_{i=1}^m \lambda_i \alpha_i \leq 0\)</span>. ■</p>
</div>
<section id="conjugate-functions" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="conjugate-functions"><span class="header-section-number">4.1</span> Conjugate Functions</h2>
<div id="def-conjugates" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 13</strong></span> Let <span class="math inline">\(C\)</span> be a convex non-empty set in <span class="math inline">\(\mathbb{R}^n\)</span> and let <span class="math inline">\(f\)</span> be a function on <span class="math inline">\(C\)</span>. The <em>conjugate set</em> <span class="math inline">\(C^*\)</span>of <span class="math inline">\(C\)</span> on <span class="math inline">\(\mathbb{R}^n\)</span> is defined as</p>
<p><span class="math display">\[ C^* = \{x^*\in\mathbb{R}^n \mid \sup_{x\in C} (\langle x, x^*\rangle - f(x)) &lt; \infty\} \]</span></p>
<p>The <em>conjugate function</em> <span class="math inline">\(f^*\)</span> of <span class="math inline">\(f\)</span> on <span class="math inline">\(C^*\)</span> is defined as</p>
<p><span class="math display">\[ f^*(x^*) = \sup_{x\in C} (\langle x, x^*\rangle - f(x)).\]</span></p>
</div>
<p>Note that when <span class="math inline">\(f\)</span> is differentiable the critical point of <span class="math inline">\(\langle x, x^*\rangle - f(x)\)</span> happens at <span class="math inline">\(x^* = f'(x)\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture36.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%"></p>
</figure>
</div>
<p>For example, in <span class="math inline">\(\mathbb{R}^2\)</span> this is: <span class="math inline">\(x^*x - f(x)\)</span>, now fix <span class="math inline">\(x^*\)</span></p>
<p><span class="math display">\[\sup_{x\in\mathbb{R}^2} (x^*x - f(x)) \Rightarrow x^* = f'(x).\]</span></p>
<p>The slope of the upper purple line is parallel to the slope of <span class="math inline">\(x^*x\)</span>. I.e. <span class="math inline">\(x\)</span> is such that the slope of the tangent to the curve is exactly <span class="math inline">\(x^*\)</span>.</p>
<p>We also note that the set conjugate set <span class="math inline">\(C^*\)</span> and the conjugate function <span class="math inline">\(f^*\)</span> are both convex. Moreover, <span class="math inline">\(\texttt{epi}(f^*)\)</span> is a closed convex set.</p>
<div class="example">
<div class="example-header">
<p>Examples</p>
</div>
<div class="example-container">
<ol type="1">
<li>Consider <span class="math inline">\(f:\mathbb{R}\rightarrow\mathbb{R}\)</span> such that <span class="math inline">\(x\mapsto f(x) = ax + b\)</span>, <span class="math inline">\(a,b\in\mathbb{R}\)</span>.</li>
</ol>
<p><span class="math display">\[ f^*(x^*) = \sup_{x\in\mathbb{R}} (x^*x - (ax + b)) = \sup_{x\in\mathbb{R}} (x(x^*-a) - b) \]</span></p>
<p>If <span class="math inline">\(x^*\neq a\)</span>, then the supremum is unbounded, and <span class="math inline">\(x^*\notin C^*\)</span>. When <span class="math inline">\(x^* = a\)</span>, then <span class="math inline">\(f^*(x) = -b\)</span>.</p>
<ol start="2" type="1">
<li>Consider <span class="math inline">\(f:\mathbb{R}\rightarrow\mathbb{R}\)</span> such that <span class="math inline">\(x\mapsto (e^x)\)</span>. Then</li>
</ol>
<p><span class="math display">\[ f^*(x^*) = \sup_{x\in\mathbb{R}} (x^*x-e^x)\]</span></p>
<p>When <span class="math inline">\(x^* &lt;0\)</span>, this is unbounded. When <span class="math inline">\(x^*&gt;0\)</span>, the maximum is reached at <span class="math inline">\(x = \log(x^*)\)</span> and hence</p>
<p><span class="math display">\[f^*(x^*) = x^*\log(x^*) - x^*.\]</span></p>
<p>When <span class="math inline">\(x^*=0\)</span>, <span class="math inline">\(f^*(x^*) = 0\)</span>.</p>
</div>
</div>
<p>The notion of conjugate functions in the context of optimsation is due to Fenchal (1953), however, this is a generalisation of Legendre transforms (1787)</p>
<p><span class="math display">\[f^*(p) = \sup_{x\in S} (p^Tx - f(x)).\]</span></p>
<!-- not doing this example since it's not relevant to the backgrounds of the students I'm teaching -->
<!-- ::: {.example}
:::: {.example-header}
Example: Hamiltonian
::::
:::: {.example-container}
Consider a (simple) mechanical system defined on a configuration $Q = \mathbb{R}^n$. We use the coordinates $q = (q_1,...,q_n)$ for $Q$. The kinetic energy of this system is 

$$ K(q,\dot{q}) = \frac{1}{2} \dot{q}^T M(q) \dot{q}, $$

and the potential energy is $V(q)$.

**figure**

Recall, for a spring-mass system, 

$$ K(x,\dot{x}) = \frac{1}{2} m\dot{x}^2. $$

We define the Lagrangian to be

$$\mathcal{L}(q,\dot{q}) = K(q,\dot{q}) - V(q).$$

And recall the Newton equation of motion is $\sum F = m a$, in terms of the Lagrangian this is

$$\frac{d}{dt} \frac{\partial \mathcal{L}}{\partial \dot{q}} - \frac{\partial \mathcal{L}}{\partial q} = 0,$$

this defines the Euler-Lagrange equations. 

::::
::: -->
<p>Note that <span class="math inline">\(f(x) + f^*(y) \geq x^T y\)</span>, this is called the Fenchel-Young inequality.</p>
<div class="example">
<div class="example-header">
<p>Example</p>
</div>
<div class="example-container">
<p>Let <span class="math inline">\(f:\mathbb{R}\rightarrow\mathbb{R}\)</span> such that <span class="math inline">\(a\mapsto \frac{1}{2} a^2\)</span>, then <span class="math inline">\(f^*(b) = \sup (ab - \frac{a^2}{2}) = \frac{b^2}{2}\)</span>. We know from Young’s inequality</p>
<p><span class="math display">\[ \frac{1}{2}(a^2 + b^2) \geq ab,\]</span></p>
<p>but this is the Fenchel-Young inequality using the funciton <span class="math inline">\(f\)</span> defined above!</p>
</div>
</div>
</section>
<section id="duality-theory" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="duality-theory"><span class="header-section-number">4.2</span> Duality Theory</h2>
<div id="thm-fenchelduality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 17 (Fenchel Duality)</strong></span> Let <span class="math inline">\(f\)</span> be a convex function over <span class="math inline">\(C\)</span>, <span class="math inline">\(g\)</span> be a concave function of <span class="math inline">\(D\)</span>. Both <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span> are convex sets in <span class="math inline">\(X = \mathbb{R}^n\)</span>. Suppose <span class="math inline">\(C\cap D\)</span> has a non-empty relative interior and <span class="math inline">\(\texttt{epi}_f(C)\)</span> and <span class="math inline">\(\texttt{epi}_g(D)\)</span> have non-empty interiors. Consider</p>
<p><span class="math display">\[ \inf_{x\in C\cap D} \ (f(x) - g(x))\]</span></p>
<p>and suppose that this problem has a finite solution <span class="math inline">\(\mu\in\mathbb{R}\)</span>, its value achieved at some <span class="math inline">\(x_0\in C\cap D\)</span>. Then,</p>
<p><span class="math display">\[ \mu = \inf_{x\in C\cap D}  \ (f(x) - g(x)) = \sup_{x^*\in C^*\cap D^*} \ (g^*(x^*) - f^*(y^*))\]</span></p>
<p>and the supremum on the right hand side is achieved at some <span class="math inline">\(x_0^*\in C^*\cap D^*\)</span>. Moreover</p>
<p><span class="math display">\[\max_{x\in C} (\langle x, x_0^*\rangle - f(x)) = \langle x_0, x_0^*\rangle - f(x_0)\]</span></p>
<p>and</p>
<p><span class="math display">\[\min_{x\in D} (\langle x, x_0^*\rangle - g(x)) = \langle x_0,x_0^*\rangle - g(x_0).\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first show that</p>
<p><span id="eq-dualityproof1"><span class="math display">\[\mu = \inf_{x\in C\cap D} (f(x) - g(x)) \geq \sup_{x^*\in C^*\cap D^*}(g^*(x^*)-f^*(x^*)). \tag{6}\]</span></span></p>
<p>By the Fenchel-Young inequality</p>
<p><span class="math display">\[f^*(x^*) \geq \langle x,x^*\rangle - f(x),\]</span></p>
<p>and</p>
<p><span class="math display">\[f^*(x^*) \geq \langle x,x^*\rangle g(x),\]</span></p>
<p>for all <span class="math inline">\(x,x^*\)</span>. Thus, <span class="math inline">\(f(x) - g(x) \geq g^*(x^*) - f^*(x^*)\)</span>. This proves the claim.</p>
<p>It is enough to show that there exists some point <span class="math inline">\(x_0^* \in C^*\cap D^*\)</span> for with the inequality in <a href="#eq-dualityproof1" class="quarto-xref">Equation&nbsp;6</a> is achieved.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture37.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>We want to separate the epigrpahs. The natural candidates for separation are</p>
<p><span class="math display">\[\begin{aligned}
\texttt{epi}_{f-\mu}(C) = \{(x,\alpha) \mid x\in C, f(x) - \mu\leq \alpha\}, \\
\texttt{epi}_{g}(D) = \{(x,\alpha) \mid x\in D, g(x) \geq \alpha\},
\end{aligned}\]</span></p>
<p>noting that for concave functions, the epigraph is written as the infimum. After verifying tha tthe separation theorem can be applied, we get that there exists a hyperplane that separates <span class="math inline">\(\texttt{epi}_g(D)\)</span> and <span class="math inline">\(\texttt{epi}_{f-\mu}(C)\)</span>, denoted by</p>
<p><span class="math display">\[ H= \{(r,x) \in\mathbb{R}\times X \mid \langle x, x_0^*\rangle -r = c\},\]</span></p>
<p>where <span class="math inline">\(x_0^*\in\mathbb{R}^n\)</span> and <span class="math inline">\(c\in\mathbb{R}\)</span>, without loss of generality. Note that we are saying that this hyperplane is not “vertical”, since</p>
<p><span class="math display">\[\langle x, x_0^*\rangle - r = (r,x)\cdot(-1,x_0^*).\]</span></p>
<p>The <span class="math inline">\(-1\)</span> can’t be zero, since if it was it would separate <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span> and we assumed they can’t be separated (since their intesection has non-empty relative interior).</p>
<p>Now that we have this hyperplane, we note that <span class="math inline">\(H\)</span> is above <span class="math inline">\(\texttt{epi}_g(D)\)</span> and arbitrarily close to it. I.e.</p>
<p><span id="eq-dualityproof2"><span class="math display">\[c = \inf_{x\in D} (\langle x,x_0^*\rangle - g(x)) = g^*(x^*), \tag{7}\]</span></span></p>
<p>similarly, <span class="math inline">\(H\)</span> is below <span class="math inline">\(\texttt{epi}_{f-\mu}(C)\)</span> and arbitrarily close to it</p>
<p><span id="eq-dualityproof3"><span class="math display">\[c = \sup_{x\in C} (\langle x,x_0^*\rangle - f(x) + \mu) = f^*(x^*) + \mu. \tag{8}\]</span></span></p>
<p>Thus, with <a href="#eq-dualityproof2" class="quarto-xref">Equation&nbsp;7</a> and <a href="#eq-dualityproof3" class="quarto-xref">Equation&nbsp;8</a> we have <span class="math inline">\(g^*(x_0^*) = f^*(x_0^*) + \mu\)</span>, i.e.&nbsp;the inequality <a href="#eq-dualityproof1" class="quarto-xref">Equation&nbsp;6</a> is achieved! ■</p>
</div>
<div class="example">
<div class="example-header">
<p>Example: Resource allocation</p>
</div>
<div class="example-container">
<p>Suppose a total of <span class="math inline">\(R\in\mathbb{R}_{&gt;0}\)</span> units of resources are supposed to be allocated to <span class="math inline">\(n\)</span> locations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/Picture38.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:20.0%"></p>
</figure>
</div>
<p>The payoff of allocating <span class="math inline">\(x_i\)</span> units of resources to location <span class="math inline">\(i\)</span> is <span class="math inline">\(g_i(x_i)\)</span>, <span class="math inline">\(g_i:\mathbb{R}\rightarrow\mathbb{R}\)</span>, and suppose that <span class="math inline">\(g_i\)</span> is increasing and concave.</p>
<p>We want to maximise <span class="math inline">\(\sum_{i=1}^n g_i(x)\)</span> subject to <span class="math inline">\(\sum_{i=1}^n x_i = R\)</span> (max number of resources) and <span class="math inline">\(x_i\geq 0\)</span>.</p>
<p>Let us take: - <span class="math inline">\(D\)</span> to be the positive orthant, <span class="math inline">\(D = \{x \mid x_i\geq 0, i\in\{1,...,n\}\}\)</span> - <span class="math inline">\(C = \{x \mid \sum_{i=1}^n x_i = R\}\)</span> - <span class="math inline">\(f\)</span> to be zero (since we are going to be using Fenchel Duality and need a convex and a concave function)</p>
<p>Note that <span class="math inline">\(f^*(y) = \sup_{x\in C} y^T x\)</span>. When is <span class="math inline">\(f^*\)</span> well defined? Suppose <span class="math inline">\(y = \alpha\mathbb{1}_n\)</span> (where <span class="math inline">\(\mathbb{1}_n\)</span> is the vector of ones), then</p>
<p><span class="math display">\[ f^*(y) = \sup_{x\in C} \ \alpha \mathbb{1}_n^T x = \alpha R.\]</span></p>
<p>If <span class="math inline">\(y\neq \alpha\mathbb{1}_n\)</span> for any <span class="math inline">\(\alpha\in\mathbb{R}\)</span>, then <span class="math inline">\(f^*\)</span> is not well defined (i.e.&nbsp;infinity, check this on your own). Then</p>
<p><span class="math display">\[C^* = \{y \mid y = \alpha\mathbb{1}_n, \alpha\in\mathbb{R}\}.\]</span></p>
<p>Let <span class="math inline">\(g_i^*(y_i) = \inf_{x_i\geq 0} (x_i y_i - g_i(x_i))\)</span>. Then the dual problem is given by</p>
<p><span class="math display">\[\min_{\alpha\in\mathbb{R}} (\alpha R - \sum_{i=1}^n g_i^*(\alpha))\]</span></p>
</div>
</div>
<p>Consider two convex sets <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span>. Suppose that there are two players <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span> choosing <span class="math inline">\(x\in X\)</span> and <span class="math inline">\(B\)</span> choosing <span class="math inline">\(y\in Y\)</span>, with the goal of, respectively, minimising <span class="math inline">\(\langle x,y\rangle\)</span> and maximising <span class="math inline">\(\langle x,y\rangle\)</span> over <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>Let now the strategy of one player be <span class="math inline">\(\min_{x\in X} \max_{y\in Y} \langle x,y\rangle\)</span>, and the strategy of the other player to be <span class="math inline">\(\max_{y\in Y}\min_{x\in X} \langle x,y\rangle\)</span>. Do the players end up with the same result?</p>
<div id="thm-minimax" class="theorem">
<p><span class="theorem-title"><strong>Theorem 18 (Minimax)</strong></span> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two compact convex sets in <span class="math inline">\(\mathbb{R}^n\)</span>. Then,</p>
<p><span class="math display">\[\min_{x\in X} \max_{y\in Y} \langle x,y\rangle = \max_{y\in Y}\min_{x\in X} \langle x,y\rangle.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using Fenchel Duality (<a href="#thm-fenchelduality" class="quarto-xref">Theorem&nbsp;17</a>) let <span class="math inline">\(C = \mathbb{R}^n\)</span>. Define <span class="math inline">\(f(x) = \max_{y\in Y} \langle x,y\rangle\)</span>, <span class="math inline">\(x\in C\)</span>. By compactness of <span class="math inline">\(Y\)</span>, <span class="math inline">\(f\)</span> is continuous and clearly convex on <span class="math inline">\(C\)</span>. Consider now the problem</p>
<p><span class="math display">\[\min_{x\in X} \ f(x)\]</span></p>
<p>which has a solution by the Weierstrass theorem (<a href="#thm-weierstrass" class="quarto-xref">Theorem&nbsp;1</a>). Let us now take <span class="math inline">\(g\)</span> to be the zero function on <span class="math inline">\(D=X\)</span>. Note that</p>
<p><span class="math display">\[g^*(x^*) = \min_{x\in X} \langle x,x^*\rangle.\]</span></p>
<p>It is well-defined and hence <span class="math inline">\(D^* = \mathbb{R}^n\)</span> (since <span class="math inline">\(X\)</span> is compact). It is left to compute <span class="math inline">\(f^*\)</span> on <span class="math inline">\(C^*\)</span></p>
<p><span class="math display">\[\begin{aligned}
f^*(x^*) &amp;= \sup_{x\in C} (\langle x,x^*\rangle - f(x)) \\
&amp;= \sup_{x\in C} (\langle x,x^*\rangle - \max_{y\in Y}\langle x,y\rangle), \ \ x^*\in Y
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(X\subset C\)</span> if <span class="math inline">\(x^*\in Y\)</span>, then <span class="math inline">\(f^*(x^*) = 0\)</span> (by picking <span class="math inline">\(x\)</span> such that <span class="math inline">\(\max_{y\in Y} \langle x,y\rangle\)</span> is at <span class="math inline">\(x^*\)</span>). Also, we can show that, if <span class="math inline">\(x^*\notin Y\)</span>, then</p>
<p><span class="math display">\[\sup_{x\in C} (\langle x,x^*\rangle - \max_{y\in Y}\langle x,y\rangle) = +\infty.\]</span></p>
<p>As a result, <span class="math inline">\(C^* = Y\)</span>.</p>
<p>Now, using Fenchel duality</p>
<p><span class="math display">\[\begin{aligned}
&amp; \min_{x\in C\cap D} (f(x) - 0) = \max_{y\in C^*\cap D^*} (g^*(y) - 0) \\
\Rightarrow &amp; \min_{x\in X} f(x) = \max_{y\in Y} g^*(y) \\
\Rightarrow &amp; \min_{x\in X} \max_{y\in Y} \langle x,y\rangle = \max_{y\in Y} \min_{x\in X} \langle x,y\rangle
\end{aligned}\]</span> ■</p>
</div>
<div id="def-saddlepoint" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 14 (Saddle points)</strong></span> A point <span class="math inline">\(x^*,z^*\in X\times Z\)</span>, where <span class="math inline">\(X\subseteq \mathbb{R}^n\)</span> and <span class="math inline">\(Z\subseteq\mathbb{R}^m\)</span>, is called a local min-max saddle point of a continuously differentiable function <span class="math inline">\(F\)</span> if there exists open neighbourhoods <span class="math inline">\(U_x^* \subseteq\mathbb{R}^n\)</span> and <span class="math inline">\(U_z^* \subseteq \mathbb{R}^m\)</span> such that</p>
<p><span class="math display">\[F^*(x^*,z) \leq F(x^*,z^*) \leq F(x,z^*).\]</span></p>
<p>This point is called a global min-max saddle point if <span class="math inline">\(U_x^* = X\)</span> and <span class="math inline">\(U_z^* = Z\)</span>.</p>
</div>
<div id="thm-weakduality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19 (Weak duality)</strong></span> Let <span class="math inline">\(F:X\times Z\rightarrow\mathbb{R}\)</span> be as before. Then,</p>
<p><span class="math display">\[\sup_{z\in Z}\inf_{x\in X} F(x,z) \leq \inf_{x\in X}\sup_{z\in Z} F(x,z).\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We know <span class="math inline">\(\inf_{x\in X} F(x,z) \leq F(\tilde{x},z)\)</span> for all <span class="math inline">\(z\in Z\)</span>, <span class="math inline">\(\tilde{x}\in X\)</span>. In particular, we have</p>
<p><span class="math display">\[\sup_{z\in Z} \inf_{x\in X} F(x,z) \leq \sup_{z\in Z} F(\tilde{x},z) \ \forall \tilde{x}\in X.\]</span></p>
<p>As a result,</p>
<p><span class="math display">\[\sup_{z\in Z}\inf_{x\in X} F(x,z) \leq \inf_{x\in X}\sup_{z\in Z} F(x,z).\]</span> ■</p>
</div>
<p>Now we ask the question, when is this inequality an equality? We now study this question.</p>
<p>Define</p>
<p><span class="math display">\[P_v \triangleq \inf_{x\in X} \sup_{z\in Z} F(x,z)\]</span></p>
<p>to be the <em>primal problem</em> and <span class="math inline">\(P_v\)</span> the primal value, and</p>
<p><span class="math display">\[D_v\triangleq \sup_{z\in Z}\inf_{x\in X} F(x,z)\]</span></p>
<p>to be the <em>dual problem</em> with <span class="math inline">\(D_v\)</span> the dual value. So far we know that <span class="math inline">\(P_v \geq D_v\)</span>. Let us define the <em>duality gap</em> to be the difference between the primal and dual, <span class="math inline">\(P_v - D_v\)</span>.</p>
<div id="thm-primaldual" class="theorem">
<p><span class="theorem-title"><strong>Theorem 20</strong></span> Let <span class="math inline">\(F:X\times Z\rightarrow\mathbb{R}\)</span> be as before. Then the following two conditions are equivalent.</p>
<ol type="1">
<li><p><span class="math inline">\((x^*,z^*)\in X\times Z\)</span> is a saddle point of <span class="math inline">\(F\)</span>. (Note: if it isnot specified as local, then saddle points are assumed to be global.)</p></li>
<li><p><span class="math inline">\(x^*\)</span> solves the primal problem, <span class="math inline">\(z^*\)</span> solves the dual problem, and the duality gap is zero, i.e.</p></li>
</ol>
<p><span class="math display">\[\min_{x\in X}\max_{z\in Z} F(x,z) = \max_{z\in Z}\min_{x\in X} F(x,z).\]</span></p>
<p>And, whenever any of these two conditions are satisfied, <span class="math inline">\(P_v = D_v = F(x^*,z^*)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose that 1. holds. Then, by definition we have</p>
<p><span class="math display">\[F(x^*,z) \leq F(x^*,z^*) \leq F(x,z^*).\]</span></p>
<p>Hence, <span class="math inline">\(\sup_{z\in Z} F(x^*,z) = \max_{z\in Z} F(x^*,z) = F(x^*,z^*)\)</span>, and <span class="math inline">\(\inf_{x\in X} F(x,z^*) = \min_{x,z^*} F(x,z^*) = F(x^*,z^*)\)</span>. And as a result,</p>
<p><span class="math display">\[\begin{aligned}
\inf_{x\in X}\sup_{z\in Z} F(x,z) \leq \sup_{z\in Z} F(x^*,z) &amp;= F(x^*,z^*) \\
&amp;= \inf_{x\in X} F(x,z^*) \\
&amp;\leq \sup_{z\in Z} \inf_{x\in X} F(x,z).
\end{aligned}\]</span></p>
<p>Then, using Weak duality (<a href="#thm-weakduality" class="quarto-xref">Theorem&nbsp;19</a>), we have <span class="math inline">\(\min_{x\in X}\sup_{z\in Z} F(x,z) = \max_{z\in Z}\inf_{x\in X} F(x,z) = F(x^*,z^*)\)</span> and the duality gap is 0.</p>
<p>To prove the other side, suppose that 2. holds. Then, we have that the primal and dual problems have the same solution</p>
<p><span class="math display">\[\inf_{x\in X} F(x,z^*) = \sup_{z\in Z} F(x^*,z).\]</span></p>
<p>This implies that <span class="math inline">\(F(x^*,z^*) = \sup_{\tilde{z}\in Z} F(x^*,\tilde{z}) \geq F(x^*,z)\)</span> for all <span class="math inline">\(z\in Z\)</span>. Similarly, <span class="math inline">\(F(x^*,z^*) = \inf_{\tilde{x}\in X} F(\tilde{x},z^*) \leq F(x,z^*)\)</span> for all <span class="math inline">\(x\in X\)</span>. Using these, we have</p>
<p><span class="math display">\[F(x^*,z) \leq F(x^*,z^*) \leq F(x,z^*),\]</span></p>
<p>which implies that <span class="math inline">\((x^*,z^*)\)</span> is a saddle point. ■</p>
</div>
</section>
<section id="nonlinear-programming" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="nonlinear-programming"><span class="header-section-number">4.3</span> Nonlinear Programming</h2>
<p>Consider the problem of minimising a function subject to both equality and inequality constraints</p>
<p><span id="eq-NLP"><span class="math display">\[\begin{aligned}
\min_x &amp; \ f(x) \\
\textnormal{such that} &amp; \ h_i(x) = 0, i\in\{1,...,m\},\\
&amp; \ g_j(x) \leq 0, j\in\{1,...,r\}.
\end{aligned} \tag{9}\]</span></span></p>
<p>Note, the functions <span class="math inline">\(f\)</span>, <span class="math inline">\(h_i\)</span>, and <span class="math inline">\(g_j\)</span> are “nice”, map <span class="math inline">\(\mathbb{R^n}\rightarrow\mathbb{R}\)</span>, and continuously differentiable.</p>
<div id="def-regular" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15 (Regularity)</strong></span> A feasible point <span class="math inline">\(x\)</span> is said to be regular if the equality constraint gradients, <span class="math inline">\(\nabla h_i(x)\)</span>, <span class="math inline">\(i\in\{1,...,m\}\)</span> and the inequality constraint gradients, <span class="math inline">\(\nabla g_j(x)\)</span>, for <span class="math inline">\(j\)</span> in the active set</p>
<p><span class="math display">\[A(x) = \{j \mid g_j(x) = 0\},\]</span></p>
<p>are linearly independent.</p>
</div>
<div id="thm-KKT" class="theorem">
<p><span class="theorem-title"><strong>Theorem 21 (Karush-Kuhn-Tucker (KKT) necessary conditions of optimality)</strong></span> Suppose that <span class="math inline">\(x^*\)</span> is a solution to the nonlinear programming problem <a href="#eq-NLP" class="quarto-xref">Equation&nbsp;9</a>. Suppose that <span class="math inline">\(X^*\)</span> is regular. Then, there exists unique vectors <span class="math inline">\(\lambda^* = (\lambda_1^*,...,\lambda_m^*)\)</span> and <span class="math inline">\(\mu^* = (\mu_1^*,...,\mu_r^*)\)</span> such that for</p>
<p><span class="math display">\[L(x,\lambda,\mu) = f(x) + \sum_{i=1}^m \lambda_i h_i(x) + \sum_{j=1}^r \mu_j g_j(x),\]</span></p>
<p>we have</p>
<p><span class="math display">\[\begin{aligned}
&amp; \nabla L(x^*,\lambda^*,\mu^*) = 0 \\
&amp; \mu_j^* \geq 0 \textnormal{ for any } j\in\{1,...,r\} \\
&amp; \mu_j^* = 0, j\notin A(x^*).
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We have shown this result for the case of equality constraints. Hence, all we need to show here is that <span class="math inline">\(\mu_j^*\geq 0\)</span> for all <span class="math inline">\(j\in A(x^*)\)</span>.</p>
<p>Let <span class="math inline">\(g_j^+(x) = \max\{0,g_j(x)\}\)</span>, <span class="math inline">\(j\in\{1,...,r\}\)</span>. Note tht this function is continuously differentiable. Let us now define a sequence of functions <span class="math inline">\(\{\tilde{F}^k\}_{k=1}^\infty\)</span></p>
<p><span class="math display">\[ \tilde{F}^k(x) = f(x) + \frac{k}{2}||h(x)||^2 + \frac{k}{2}\sum_{j=1}^r (g_j^+(x))^{-2} +\frac{\alpha}{2}||x-x^*||^2,\]</span></p>
<p>where <span class="math inline">\(x\in\mathbb{S} = \{x\mid ||x-x^*||\leq \varepsilon\}\)</span>, <span class="math inline">\(\alpha&gt;0\)</span>, <span class="math inline">\(\varepsilon&gt;0\)</span> small enough such that <span class="math inline">\(f(x^*)\leq f(x) \ \forall x\in\mathbb{S}\)</span> that are feasible.</p>
<p>For simplicity, let us assume that the functions <span class="math inline">\(\tilde{F}^k\)</span> are represented by equality constraints only. The general proof is similar.</p>
<p><span class="math display">\[ F^k(x) = f(x) + \frac{k}{2} ||h(x)||^2 + \frac{\alpha}{2}||x-x^*||^2.\]</span></p>
<p>Suppose now that <span class="math inline">\(\{x^k\}\)</span> is a sequence of minimisers of <span class="math inline">\(\{F^k\}\)</span> over <span class="math inline">\(\mathbb{S}\)</span> (we can say this because of Weierstrass <a href="#thm-weierstrass" class="quarto-xref">Theorem&nbsp;1</a>).</p>
<p><span class="math display">\[ F^k(x^k) = f(x^k) + \frac{k}{2}||h(x^k)||^2 + \frac{\alpha}{2}||x^k-x^*||^2 \leq F^k(x^*) = f(x^*).\]</span></p>
<p>Now, since <span class="math inline">\(f(x^k)\)</span> is bounded over <span class="math inline">\(\mathbb{S}\)</span>, we must have that</p>
<p><span class="math display">\[\lim_{k\rightarrow 0} ||h(x^k)|| = 0.\]</span></p>
<p>Therefore, by continuity, the limit point <span class="math inline">\(\bar{x}\)</span> of <span class="math inline">\(\{x^k\}\)</span> satisfies <span class="math inline">\(h(\bar{x}) = 0\)</span>. Taking the limit</p>
<p><span class="math display">\[ f(\bar{x}) + \frac{\alpha}{2}||\bar{x}-x^*||^2 \leq f(x^*).\]</span></p>
<p>But since <span class="math inline">\(\bar{x}\in\mathbb{S}\)</span> and <span class="math inline">\(\bar{x}\)</span> is also feasible, we have that</p>
<p><span class="math display">\[f(x^*) \leq f(\bar{x}).\]</span></p>
<p>Thus, combining the last two inequalities, we must have <span class="math inline">\(\bar{x} = x^*\)</span>. We conclude that <span class="math inline">\(\{x^k\}\)</span> converges to <span class="math inline">\(x^*\)</span>.</p>
<p>Now, by the necessary condition of optimality for <span class="math inline">\(F^k\)</span>, we have</p>
<p><span class="math display">\[\nabla F^k(x^k) = \nabla f(x^k) + k\nabla h(x^k) h(x^k) + \alpha (x^k - x^*) = 0.\]</span></p>
<p>Recall that by regularity, for <span class="math inline">\(k\)</span> large enough, <span class="math inline">\(\nabla h(x^k)\)</span> has rank <span class="math inline">\(m\)</span>. Hence, <span class="math inline">\(H_h(x^k) \triangleq \nabla^T h(x^k) \nabla h(x^k)\)</span> is invertible. Thus,</p>
<p><span class="math display">\[ k h(x^k) = - H_h^{-1} (x_k) \nabla^T h(x^k) (\nabla f(^k) + \alpha (x^k - x^*)).\]</span></p>
<p>By taking the limit</p>
<p><span class="math display">\[ \begin{aligned}
&amp; \lim_{k\rightarrow\infty} k h(x^k) = -(\nabla^T h(x^*) \nabla h(x^*))^{-1} \nabla h(x^*) \nabla f(x^*), \\
\Rightarrow &amp; \nabla^T h(x^*) \left(\lim_{k\rightarrow\infty} k h(x^k)\nabla h(x^k) + \nabla f(x^k)\right) = 0.
\end{aligned}\]</span></p>
<p>Using that <span class="math inline">\(\nabla h(x^*)\)</span> is full rank,</p>
<p><span class="math display">\[\lim_{k\rightarrow\infty} (k h(x^k))\nabla h(x^*) + \nabla f(x^*) = 0.\]</span></p>
<p>By the necessary condition of optimality,</p>
<p><span class="math display">\[ \lim_{k\rightarrow\infty} k h(x^k) = \lambda^*.\]</span></p>
<p>A similar argument for <span class="math inline">\(\tilde{F}(x^k)\)</span> shows that</p>
<p><span class="math display">\[\lim_{k\rightarrow\infty} k g_j^+(x^k) = \mu_j^*.\]</span></p>
<p>And, note that <span class="math inline">\(g_j^+(x^*) \geq 0 \ \Rightarrow \mu_j^* \geq 0\)</span>. ■</p>
</div>
<section id="nlp-and-duality" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="nlp-and-duality"><span class="header-section-number">4.3.1</span> NLP and Duality</h3>
</section>
</section>
</section>
<section id="algorithms-for-optimisation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Algorithms for Optimisation</h1>
<section id="gradient-methods---first-order" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="gradient-methods---first-order"><span class="header-section-number">5.1</span> Gradient Methods - First Order</h2>
</section>
<section id="gradient-methods---second-order" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="gradient-methods---second-order"><span class="header-section-number">5.2</span> Gradient Methods - Second Order</h2>
</section>
</section>
<section id="extras" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Extras</h1>
<section id="minimising-polynomials" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="minimising-polynomials"><span class="header-section-number">6.1</span> Minimising Polynomials</h2>
</section>
</section>
<section id="references-1" class="level1" data-number="7">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">7 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Boyd_Vandenberghe_2004" class="csl-entry" role="listitem">
Boyd, Stephen P., and Lieven Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge, UK ; New York: Cambridge University Press.
</div>
<div id="ref-Güler_2010" class="csl-entry" role="listitem">
Güler, Osman. 2010. <em>Foundations of Optimization</em>. Vol. 258. Graduate Texts in Mathematics. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-0-387-68407-9">https://doi.org/10.1007/978-0-387-68407-9</a>.
</div>
<div id="ref-Nocedal_Wright_2006" class="csl-entry" role="listitem">
Nocedal, Jorge, and Stephen J. Wright. 2006. <em>Numerical Optimization</em>. Springer Series in Operations Research and Financial Engineering. Springer New York. <a href="https://doi.org/10.1007/978-0-387-40065-5">https://doi.org/10.1007/978-0-387-40065-5</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="e-vic/e-vic.github.io" data-repo-id="R_kgDOMOCiIQ" data-category="General" data-category-id="DIC_kwDOMOCiIc4CgYjG" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>
[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "Optimisation Theory and Applications\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/opt-theory/index.html#examples-of-optimisation-problems",
    "href": "posts/opt-theory/index.html#examples-of-optimisation-problems",
    "title": "Optimisation Theory and Applications",
    "section": "1.1 Examples of Optimisation Problems",
    "text": "1.1 Examples of Optimisation Problems\n\n\nExample: 150BC – Alexandria’s Problem\n\n\nfigure\nfind a point \\(D\\in\\mathbb{R}^2\\) such that \\(||AD||+||DB||\\) is minimised.\nConsider the figure below.\nanother figure\nClaim: this \\(D\\) is the solution.\n\nProof. Suppose otherwise, i.e. let some point \\(D'\\) be the minimiser:\n\\[\\begin{aligned}\n||AD'|| + ||D'B|| &\\geq ||AB'|| \\textrm{ (from the claim)} \\\\\n&= ||AD|| + ||DB'|| \\\\\n&= ||AD|| + ||DB||\n\\end{aligned}\\]\nWhy is this true? Triangle inequality! Thus, \\(D\\) is the minimiser. ■\n\n\n\n\n\nExample: 850BC – Dido’s Problem\n\n\nfigure\nConsider a curve of fixed length \\(\\ell\\). What is the maximum area of land you can enclose using this curve?\nTo start, think about what kind of shape you should use.\nFormally, the problem is: \\(\\max_y \\ \\ J(y) = \\int_a^b y(x) dx, \\ y\\in\\mathcal{C}([a,b])\\).\nIt turns out the optimal shape is a semicircle, but we will not be able to prove this even at the end of the course.\n\n\n\n\nExample: 1556 – Tartaglia’s Problem\n\n\nThis problem is also known as the Cardano-Tartaglia problem.\nPick two numbers \\(x\\) and \\(y\\) such that:\n\nthey sum to 8: \\(x+y = 8 \\Rightarrow y = (8-x)\\)\ntheir product, \\(xy\\), and difference, \\(x-y\\), is maximised\n\nToday this problem is easy to solve (because of calculus): we need to maximise \\(f(x) = (xy)(x-y) = x(8-x)(x-(8-x)) = -2x^3 + 24x^2 - 64x\\).\nSolution: \\(x = 4(1-\\frac{1}{\\sqrt{3}})\\).\n\n\n\n\nExample: 1954 – Max-flow\n\n\nConsider a directed graph \\(\\mathcal{G}(V,E)\\).\nfigure\nLet each edge \\(ij\\) have a capacity \\(c_{ij}\\).\nWe are looking for a map \\(f:E\\rightarrow \\mathbb{R}_{\\geq 0}\\) such that\n\n\\(f(ij)\\leq c_{ij}\\)\nFlow is conserved at each vertex: incoming = outgoing\n\nWith these conditions, we want to maximise flow across the graph: \\(\\max \\ |f|\\) over all routes through the graph."
  },
  {
    "objectID": "posts/opt-theory/index.html#notations",
    "href": "posts/opt-theory/index.html#notations",
    "title": "Optimisation Theory and Applications",
    "section": "1.2 Notations",
    "text": "1.2 Notations\n\nFor parts of this course, we work with functionals on normed vector spaces. We usually consider functions on subsets of \\(\\mathbb{R}^n\\).\n\n\\[\\begin{aligned}\n&f: D\\subset\\mathbb{R}^n \\rightarrow \\mathbb{R}\\\\\n&f: E \\rightarrow \\mathbb{R}, \\ E \\textrm{ a normed vector space}\n\\end{aligned}\\]\n\nWe usually assume \\(f\\) is differentiable, specifically Fréchet or Gâteaux differentiable. Nevertheless, we denote gradient (or derivative) of \\(f\\) by \\(\\nabla f\\):\n\n\\[\\nabla f = (\\frac{\\partial f}{\\partial x_1},...,\\frac{\\partial f}{\\partial x_n})^T\\]"
  },
  {
    "objectID": "posts/opt-theory/index.html#references",
    "href": "posts/opt-theory/index.html#references",
    "title": "Optimisation Theory and Applications",
    "section": "1.3 References",
    "text": "1.3 References\n\nConvex Optimisation - Boyd and Vandenberghe (2004)\nFoundations of Optimisation - Güler (2010)\nNumerical Optimisation - Nocedal and Wright (2006)"
  },
  {
    "objectID": "posts/opt-theory/index.html#unconstrained-optimisation",
    "href": "posts/opt-theory/index.html#unconstrained-optimisation",
    "title": "Optimisation Theory and Applications",
    "section": "2.1 Unconstrained Optimisation",
    "text": "2.1 Unconstrained Optimisation\n\n2.1.1 Existence and Uniqueness of Solutions\n\nTheorem 1 (Weierstrass, 1858) Let \\(K\\) be a compact set in a topological space. Let \\(f:K\\rightarrow\\mathbb{R}\\) be a continuous function. Then, there exists \\(x^*\\in K\\) such that \\(f(x^*)\\leq f(x)\\) for all \\(x\\in K\\).\n\n\nProof. Recall that \\(K\\) is compact if every open cover of \\(K\\) has a finite subcover.\nWe prove Theorem 1 in two steps.\nStep 1: We show that \\(f\\) is bounded from below. Let \\(K_n = \\{ x\\in K | f(x) &gt; n\\}\\) (open sets). Then,\n\\[ K = \\cup_{n=-\\infty}^\\infty K_n.\\]\nSince \\(K\\) is compact, there exists a finite subcover \\(\\{K_{n_i}\\}_{i=1}^k\\)\n\\[ K = \\cup_{i=1}^k K_{n_i}. \\]\nLet \\(\\tilde{n} = \\min\\{n_i \\mid i=1,...,k\\}\\). Then, \\(K_{\\tilde{n}} = K\\).\nBy definition of \\(K_n\\), we now let\n\\[f^* = \\inf\\{ f(x) | x\\in K\\} = \\inf\\{f(x) | f(x) &gt;\\tilde{n}\\} &gt; -\\infty. \\]\nTherefore \\(f\\) is bounded from below.\nStep 2: We show that \\(f\\) has a gloal minimizer. Suppose otherwise and let:\n\n\\(Y_n = \\{x\\in K | f(x) &gt; f^* + \\frac{1}{n}\\}\\) (open)\n\\(K = \\cup_{n=1}^\\infty Y_n\\) again. Using compactness and a similar argument as before, there exists \\(\\tilde{n}&gt;1\\) such that \\(K = Y_{\\tilde{n}}\\).\n\nThus we have that \\(f(x)&gt;f^* + \\frac{1}{\\tilde{n}}\\) for all \\(x\\in K\\). This, however, contradicts the definition of \\(f^*\\) as the infimum. ■\n\ninclude figure\nRemark: Compactness is a good assumption in finite dimensional spaces, but often not in infinite dimensional spaces. The reason is that this assumption is too strong. For example, the unit ball in \\(C^0([0,1])\\) is not compact. (In fact, this statement can be proved using Weierstrass theorem! You can show there eists a minimiser which is not continuous.) Weierstrass theorem doesn’t work well for infinite dimensions.\nWe now go about relaxing compactness.\n\nTheorem 2 Suppose \\(F: E\\rightarrow\\mathbb{R}\\) is continuous. If \\(f\\) has a non-empty compact sublevel set, i.e. \\(\\{x\\in E | f(x) \\leq \\alpha\\}\\) for some \\(\\alpha\\in\\mathbb{R}\\), then \\(f\\) achieves a global minimum.\n\nThis means that if the set of values of a function that are less than \\(\\alpha\\) is finite, a minimum exists.\nadd figure\n\nProof. Pretty straightforward. For an idea, see the figure.\n\n\nDefinition 2 (Coercive function) A function \\(f:E\\rightarrow\\mathbb{R}\\) is call coercive if\n\\[ ||x||\\rightarrow\\infty \\ \\ \\Rightarrow \\ \\ f(x) \\rightarrow\\infty .\\]\nNote: here, \\(E\\) is any normed vector space.\n\n\nProposition 1 If \\(f:D\\rightarrow\\mathbb{R}\\), where \\(D\\) is closed and \\(f\\) is continuous, then \\(f\\) achieves a global minimum on \\(D\\).\n\n\nProof. Since \\(F\\) is continuous and \\(d\\) is closed, we have that \\(\\{x\\in D | f(x) \\leq \\alpha\\}\\), \\(\\alpha\\in\\mathbb{R}\\), is closed.\nWe claim that this set is also bounded: Suppose otherwise, then there is a sequence \\(||x_n||\\rightarrow\\infty\\). Then, using coercivity \\(|f(x)|\\rightarrow\\infty\\) on this sublevel set, which is a contradiction! Thus, the sublevel set is also bounded.\nBy the previous result, since the sublevel set is compact, we have a global minimum. ■\n\n\n\n2.1.2 Necessary Conditions of Optimality\nConsider a \\(C^1\\)-function \\(f\\) on \\(\\mathbb{R}^n\\) and suppose tht \\(x^*\\) is a local minimum of \\(f\\) in some open set \\(D\\subset \\mathbb{R}^n\\). Note that \\(x^*\\) is in the interior of \\(d\\).\nfigure\nLet \\(d\\) be some vector in \\(\\mathbb{R}^n\\), and \\(\\alpha\\in\\mathbb{R}\\). Consider:\n\\[g(\\alpha) = f(x^* + \\alpha d).\\]\nNote that: \\(\\alpha = 0\\) is the local minimum of \\(g\\). Using a first order approximation:\n\\[ g(\\alpha) = g(0) + g'(0)(\\alpha - 0) + \\mathcal{O}(\\alpha),\\]\nwhere \\(\\lim_{\\alpha\\rightarrow } \\frac{\\mathcal{O}(\\alpha)}{\\alpha} = 0\\).\n\nLemma 1 We have that \\(g'(0) = 0\\).\n\n\nProof. Suppose otherwise. Then for \\(\\varepsilon &gt;0\\) small enough and \\(\\alpha \\neq 0\\) with \\(|\\alpha|&lt;\\varepsilon\\), we have\n\\[\\begin{aligned}\n|g'(0) \\alpha| &gt; |\\mathcal{O}(\\alpha)|\\\\\n\\Rightarrow g(\\alpha) - g(0) &lt; g'(0) \\alpha + |g'(0)\\alpha|\n\\end{aligned}\\]\nIf we let \\(\\alpha\\) have the opposite sign (since \\(\\alpha\\) is any number in \\(\\mathbb{R}\\)) of \\(g'(0)\\), then \\(g(\\alpha) &lt; g(0)\\). Which is a contradiction! Thus \\(g'(0)=0\\). ■\n\nNow, with \\(g'(0) = 0\\), we consider what this means for \\(f\\).\n\\[\\begin{aligned}\ng'(\\alpha) &= \\nabla f (x^* + \\alpha d) \\cdot \\vec{d} \\\\\n&= \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}(x^* + \\alpha d) \\cdot d_i\n\\end{aligned}\\]\nThen, \\(g'(0) = \\nabla f(x^*) \\cdot d = 0\\) for all \\(d\\in\\mathbb{R}^n\\). Thus, \\(\\nabla f(x^*) = 0\\). This is the first necessary condition of optimality.\n\n\n2.1.3 Sufficient Conditions of Optimality\nConsider now the function \\(f\\), this time in \\(C^2\\).\n\\[g(\\alpha) = g(0) + g'(0) \\alpha + \\frac{1}{2}g''(0)\\alpha^2 + \\mathcal{O}(\\alpha^2),\\]\nwith \\(\\lim_{\\alpha\\rightarrow 0} \\frac{\\mathcal{O}(\\alpha^2)}{\\alpha^2} = 0\\).\n\nLemma 2 If \\(x^*\\) is a local minimum, then \\(g''(0) \\geq 0\\).\n\n\nProof. Suppose otherwise. I.e. \\(g''(0) &lt; 0\\). Then, or \\(\\alpha\\) small enough, we have\n\\[\\frac{1}{2} |g''(0)|\\alpha^2 &gt; \\mathcal{O}(\\alpha^2). \\]\nSince we know that \\(g'(0) = 0 \\Rightarrow g(\\alpha) - g(0) &lt; \\frac{1}{2} g''(0)\\alpha^2 + \\frac{1}{2}|g''(0)|\\alpha^2\\) and \\(\\frac{1}{2}g''(0)\\alpha^2 &lt;0\\), then $g()- g(0) &lt;0 $, which is contradicts \\(g(0)\\) being the minimiser! ■\n\nLet us compute \\(g''(0)\\):\n\\[\\begin{aligned}\ng''(\\alpha) &= \\frac{d}{dx} g'(\\alpha) \\\\\n&= \\frac{d}{d\\alpha} \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}(x^* + \\alpha d) d_i \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (x^* + \\alpha d) d_i d_j\n\\end{aligned}\\]\nThen, \\(g''(0) = \\sum_{i,j = 1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (x^*)d_i d_j = d^T \\nabla^2 f(x^*) d\\). And since \\(g''(0)\\geq 0\\), then \\(d^T \\nabla^2 f(x^*) d \\geq 0\\) for all \\(d\\), which implies \\(\\nabla^2 f(x^*)\\) is symmetric and positive semidefinite.\n\n\n\n\n\n\nSymmetric matrices\n\n\n\nConsider an \\(n\\times n\\) real symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\). It is well known and easy to show that \\(A\\) has real eigenvalues, denote these by \\(\\lambda_1,...,\\lambda_n\\in\\mathbb{R}\\), with corresponding eigenvectors \\(\\vec{u}_1,...,\\vec{u}_n\\) such that \\(\\langle \\vec{u}_i, \\vec{u}_j\\rangle = 0\\) when \\(i\\neq j\\), and \\(||\\vec{u}_i|| = 1\\). Let now\n\\[ \\Lambda = \\left[\\begin{array}{ccc}\n\\lambda_1 & ... & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & ... & \\lambda_n\n\\end{array}\\right] = \\texttt{diag}(\\lambda_1,...,\\lambda_n), \\ \\ U = \\left[\\vec{u}_1,...,\\vec{u}_n \\right].\\]\nNote that \\(AU = \\left[A\\vec{u}_1,...,A\\vec{u}_n \\right] = \\left[\\lambda_1\\vec{u}_1,...,\\lambda_n\\vec{u}_n \\right] = U\\Lambda\\).\nSince \\(U^T U = I_n\\) (identity), then \\(A = A(U U^T) = (AU)U^T = U\\Lambda U^T\\). (note: \\(U\\) is non-singular).\n\nTheorem 3 (Spectral Decomposition of Symmetric Matrices) Let \\(A\\) be a real symmetric matrix. Then, \\(A\\) can be written as\n\\[ A = U \\Lambda U^T.\\]\n\n\nCorollary 1 \\(A\\) is positive semidefinite if and only if all eigenvalues of \\(A\\) are non-negative. Similarly, positive definite if and only if all eigenvalues are strictly positive.\n\n\nProof. By the previous result,\n\\[\\begin{aligned}\nd^T A d &= d^T U\\Lambda U^T d \\\\\n&= (U^Td)^T \\Lambda (U^T d)\n\\end{aligned}.\\]\nSince \\(U\\) is non-singular, we have that \\(d^T A d\\geq 0\\) for all \\(d\\) if and only if \\(d^T \\Lambda d\\geq 0\\) for all \\(d\\).\n\\[ \\begin{aligned}\nd^T \\Lambda d &= \\sum_{i=1}^n \\lambda_a d_i^2 \\geq 0 \\ \\ \\forall d_i \\\\\n&\\Rightarrow \\lambda_i \\geq 0 \\ \\forall i.\n\\end{aligned}\\]\nThe proof of the second part is similar. ■\n\n\n\nIt is easy to see that the necessary conditions we have are not sufficient. For example, take \\(f: x\\mapsto x^3\\). However, if \\(f\\) is in \\(C^2\\) and \\(\\nabla f (X^*) = 0\\), \\(\\nabla^2f(x^*) \\prec 0\\), then \\(x^*\\) is a minimiser.\n\nProof. To show this, note that\n\\[f(x^* + \\alpha d) = f(x^*) + \\frac{1}{2}\\alpha^2 d^T\\nabla^2 f(x^*) d + ... \\.\\]\nAgain, we can choose $small enough to have\n\\[ \\frac{\\alpha^2}{2} d^T \\nabla^2 f(x^*) d &gt; |\\mathcal{O}(\\alpha^2).\\]\nSince \\(\\nabla^2 f(x^*)\\) is positive definite, then \\(f(x^* + \\alpha d) &gt; f(x^*)\\) for all \\(d\\). Thus, \\(x^*\\) is a local minimum! ■\n\nNote that: \\(\\alpha\\) depends on \\(d\\). If \\(|\\alpha| &lt; \\varepsilon^*(d)\\), different \\(d\\) will result in different \\(\\varepsilon\\), thus, choose the smallest \\(\\varepsilon\\). Without loss of generality assume \\(||d|| = 1\\), \\(\\varepsilon^* = \\min(\\varepsilon(d))\\) which results from Weierstrass theorem.\nfigure\n\n\nExample: Arithmetic-Geometric mean inequality\n\n\nWe show that: for \\(x_i&gt;0 \\in\\mathbb{R} \\ \\forall i\\)\n\\[ (x_1 \\cdot ... \\cdot x_n)^{1/n} \\leq \\frac{1}{n} \\sum_{i=1}^n x_i .\\]\n\nProof. Let \\(y_i = \\ln(x_i)\\) and let\n\\[ f(y_1,...,y_n) \\triangleq \\frac{1}{n} \\sum_{i=1}^n e^{y_i} - e^{\\frac{y_1 + ... + y_n}{n}} \\]\n(this came from replacing \\(x_i\\) with \\(e^{y_i}\\) in the statement.)\nNote that we want tot show \\(f(y_1,..,y_n) \\geq 0\\). This is the same problem as above. It is enough to show that \\(f\\) acheives its minimum value at zero.\nTo do this, use the first order condition of optimality:\n\\[ \\frac{\\partial f}{\\partial y_i} = \\frac{1}{n} e^{y_i} - \\frac{1}{n} e^{\\frac{y_1 + ... + y_n}{n}} \\ \\ \\forall i \\]\nwhich are zero if and only if $ e^{y_i} = e^{}  y_i = $ for all \\(i\\). (Check this for yourselves).\nThis gives a system of linear equations with solution \\(y_i = \\frac{Y}{n}\\) for all \\(i\\). i.e. picked all \\(y_i\\) to be the same for any value of \\(Y\\in\\mathbb{R}\\).\nNote that \\(f\\left(\\frac{Y}{n},...,\\frac{Y}{n}\\right) = 0\\).\nWe now need to justify that we have a global minimiser. (And, ideally a unique one.)\nWe reduce the problem over the set\n\\[\\{(y_1,...,y_n) | \\sum_{i=1}^n y_i = Y\\}\\]\nfor some \\(Y\\in\\mathbb{R}\\). Using this, and by eliminating \\(y_n\\) (by saying \\(y_n = Y - y_1 - ... - y_{n-1}\\)), define\n\\[g(y_1,...,y_{n-1}) = \\frac{1}{n}\\left( e^{y_1} + ... + e^{y_{n-1}} + e^{Y - y_1 - ... - y_{n-1}}\\right) - e^\\frac{Y}{n}.\\]\nIt is easy to check now that \\(g\\) has a unique critical point\n\\[y_i = \\frac{Y}{n}, \\ \\ i\\in\\{1,...,n-1\\}.\\]\nThe function \\(g\\) is coercive and hence has a global minimum, which has to be\n\\[y_i = \\frac{Y}{n}, \\ \\ i\\in\\{1,...,n-1\\}.\\] ■"
  },
  {
    "objectID": "posts/opt-theory/index.html#constrained-optimisation",
    "href": "posts/opt-theory/index.html#constrained-optimisation",
    "title": "Optimisation Theory and Applications",
    "section": "2.2 Constrained Optimisation",
    "text": "2.2 Constrained Optimisation\nfigure\n\nwe assume for now that \\(D\\) is not of lower dimension, \\(D\\subseteq \\mathbb{R}^n\\)\n\\(D\\) is assumed to be closed and bounded\n\nWe wish to study the minimisers of\n\\[f: D\\rightarrow \\mathbb{R}\\]\nWe can even assume that \\(f\\) has a minimiser in \\(D\\). This minimiser could be on the boundary.\nNote that we cannot conclude that if\n\\[ g(\\alpha) = f(x^* + \\alpha d),\\]\nwhere \\(d\\) is a feasible direction, we have \\(g'(0) = 0\\) (since \\(\\alpha\\) is dependent on \\(d\\)). We still have that, for \\(\\alpha&gt;0\\) small enough,\n\\[\\begin{aligned}\n& g(\\alpha) - g(0) &lt; g'(0)\\alpha + |g'(0)|\\alpha \\\\\n\\Rightarrow & g(\\alpha) - g(0) &lt; \\alpha (g'(0) + |g'(0)|).\n\\end{aligned} \\tag{1}\\]\nClaim: \\(g'(0)\\geq 0\\).\n\nProof. Suppose otherwise (ie. \\(g'(0)&lt;0\\)). By choosing \\(\\alpha&gt;0\\) small enough and using Equation 1 we have\n\\[\\begin{aligned}\n& g(\\alpha) &lt; g(0) \\\\\n\\Rightarrow & g'(0) \\geq 0\n\\end{aligned}\\]■\n\nRecall $g’(0) = f(x^*)d $. (from first order optimality).\nOne can also conclude a weaker version of the second order condition of optimality:\n\\[d^T \\nabla^2 f(x^*) d \\geq 0\\]\nwhen \\(\\nabla f(x^*)\\cdot d = 0\\), which we call the second order condition of optimality.\nfigure\nWe cannot use \\(x^* + \\alpha d\\)! Doing so would take us out of the surface.\n\n\n\n\n\n\nObjective\n\n\n\nMinimise \\(f(x)\\) subject to \\(D\\).\n\n\nThis optmisation problem can be written as\n\\[\\left\\{ \\begin{array}{ll}\n\\min f(x) & \\\\\nh_i(x) = 0 & i=1,...,m\n\\end{array} \\right. ,\\]\nwhere \\(D\\) is described usin functions \\(h_i\\).\nWhat if instead we use some curves, \\(\\gamma(\\cdot)\\), passing through \\(x^*\\), ie. we take \\(\\gamma(0) = x^*\\). Let us take these curves to be at least \\(C^1\\).\nfigure\nLet us define \\(g(\\alpha) = f(\\gamma(\\alpha))\\). We can now carry the same argument as before, using \\(g(\\alpha) = g(0) + \\alpha g'(0) + \\mathcal{O}(\\alpha)\\), and argue that\n\\[\\nabla f(\\gamma(0))\\cdot \\gamma'(0) = 0.\\]\nWe need to study this \\(\\gamma'(0)\\) more carefully.\nNote that \\(\\gamma'(0)\\) lives in the so called tangent space at \\(x^*\\) which we denote by \\(T_{x^*}D\\) (this is a subspace of \\(\\mathbb{R}^n\\)). We have not used \\(h_i(\\gamma(\\alpha)) = 0 \\ \\forall i\\), differentiating to get \\(\\nabla h_i(\\gamma(\\alpha)) \\cdot \\gamma'(\\alpha) = 0\\). Choosing \\(\\alpha = 0\\), we have\n\\[\\nabla h_i(x^*) \\cdot \\gamma'(0) = 0\\]\n\n\n\n\n\n\nAssumption\n\n\n\nWe assume that \\(x^*\\) is regular. This means that the gradients \\(\\nabla h_i(x^*)\\) for \\(i\\in\\{1,...,m\\}\\) are linearly independent.\n\n\nNote that since \\(x^*\\) is regular, it’s also true that any vector \\(d\\in\\mathbb{R}^n\\) for which \\(\\nabla h_i(x^*) \\cdot d = 0\\) for all \\(i\\) has to lie in the tangent space \\(T_{x^*}D\\). (check this on your own)\nIn summary, we showed that\n\\[\\nabla f(x^*)\\cdot d = 0 \\tag{2}\\]\nfor all \\(d\\) such that \\(\\nabla h_i(x^*)\\cdot d = 0\\) for all \\(i\\).\n\nProposition 2 If Equation 2 holds, and \\(x^*\\) is regular, then we have that\n\\[\\nabla f(x^*) \\in \\texttt{span}\\{\\nabla h_i(x^*), \\ i\\in\\{1,...,m\\}\\}.\\]\nie. there exists a \\(\\lambda_i^*\\in\\mathbb{R}\\) such that\n\\[\\nabla f(x^*) + \\lambda_1^* \\nabla h_1(x^*) + ... + \\lambda_m^* \\nabla h_m(x^*) = 0.\\]\nThese \\(\\lambda_i^*\\) are called Langrange multipliers.\n\n\nProof. Use contradiction, do on your own."
  },
  {
    "objectID": "posts/opt-theory/index.html#convex-sets",
    "href": "posts/opt-theory/index.html#convex-sets",
    "title": "Optimisation Theory and Applications",
    "section": "3.1 Convex Sets",
    "text": "3.1 Convex Sets\n\nDefinition 6 (Convex set - informal) A set \\(S\\subseteq\\mathbb{R}^n\\) is convex is for all \\(x,y\\in S\\), the line segment connecting \\(x\\) and \\(y\\) is in \\(S\\).\nfigure\n\n\nDefinition 7 (Convex set - formal) \\(S\\subseteq\\mathbb{R}^n\\) is convex if for all \\(x,y\\in S\\) and \\(\\lambda\\in[0,1]\\),\n\\[(1-\\lambda) x + \\lambda y\\in S.\\]\n\n\n\nExamples\n\n\n\n\\(S_1 = \\{(x,y)\\in\\mathbb{R}^2 \\mid x^2 + y^2 \\leq 1\\}\\) unit disk is convex. figure\n\\(S_2 = \\{(x,0)\\in\\mathbb{R}^2 \\mid x\\in \\mathbb{R}\\}\\) (x-axis) is affine \\(\\Rightarrow\\) convex.\n\\(S_3 = \\{(x,y)\\in\\mathbb{R}^2 \\mid y\\geq e^x\\}\\) is convex. figure\n\\(S_4 = \\{(x,y)\\in\\mathbb{R}^2 \\mid y \\geq\\sin(x)\\}\\) is not convex. figure\n\n\n\nIf we are given \\(m\\) real numbers \\(\\lambda_1,...,\\lambda_m\\) such that \\(\\lambda_i\\geq 0\\) and \\(\\sum_{i=1}^m \\lambda_i = 1\\), then these are called convex coefficients, and a sum\n\\[\\sum_{i=1}^m \\lambda_i x_i\\]\nis called a convex combination of vectors \\(x_i\\in\\mathbb{R}^n\\), \\(i=1,...,m\\). When:\n\n\\(m=2\\): line figure\n\\(m=3\\): surface figure\n\n\nTheorem 5 The set \\(S\\subseteq \\mathbb{R}^n\\) is convex if and only if \\(S\\) contains all convex combinations of its elements.\n\n\nProof. \\(\\Leftarrow\\) Set \\(m=2\\), then it follows from the definition.\n\\(\\Rightarrow\\) By induction on \\(m\\).\nBase case: \\(m=2\\). By definition of convexity, \\(\\lambda x + (1-\\lambda)y \\in S\\). Let \\(\\lambda_1 = \\lambda\\), \\(\\lambda_2 = 1-\\lambda\\).\nInductive step: Assume true for \\(2,...,m\\) and prove for \\(m+1\\). Show that a convex combination of \\(m+1\\) elements of \\(S\\) lies in \\(S\\). Let \\(x_1,...,x_{m+1}\\in S\\), \\(\\lambda_1,...,\\lambda_{m+1}\\in\\mathbb{R}\\), \\(\\lambda_i\\geq 0\\) and \\(\\sum_{i=1}^{m+1}\\lambda_i =1\\).\nAssume \\(\\lambda_{m+1}\\neq 0\\), then\n\\[ \\begin{aligned}\n\\sum_{k=1}^{m+1} \\lambda_k x_k &= \\lambda_{m+1} x_{m+1} + \\sum_{k=1}^{m+1} \\lambda_k x_k \\\\\n&= \\lambda_{m+1} x_{m+1} + (1-\\lambda) \\sum_{k=1}^{m} \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k.\n\\end{aligned}\\]\nNeed to show \\(\\sum_{k=1}^{m} \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k\\) is in \\(S\\).\nClaim: \\(\\frac{\\lambda_k}{1-\\lambda_{m+1}}\\) are convex coefficients for all \\(k=1,...,m\\).\n\\[\\sum_{k=1}^m \\frac{\\lambda_k}{1-\\lambda_{m+1}} = \\frac{1}{1-\\lambda_{m+1}} \\sum_{k=1}^m \\lambda_k = \\frac{1-\\lambda_{m+1}}{1-\\lambda_{m+1}} = 1\\]\nThus, \\(\\sum_{k=1}^m \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k\\) is a convex combination of \\(m\\) points in \\(S\\), and is therefore in \\(S\\).\nThus, \\(\\lambda_{m+1} x_{m+1} + (1-\\lambda) \\sum_{k=1}^{m} \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k \\in S\\). ■\n\nWe now introduce an object called the convex hull. Given a set \\(X\\subseteq\\mathbb{R}^n\\), we want to construct a convex set which contains \\(X\\) and is as small as possible. Define\n\\[ \\texttt{co} \\ (X) = \\left\\lbrace \\sum_{k=1}^m \\lambda_k x_k \\mid x_k\\in X, \\lambda_k \\geq 0, \\sum_{k=1}^m \\lambda_k = 1, \\textnormal{ for all } m \\right\\rbrace .\\]\nThe convex hull is the set of all convex combinations of elements of \\(X\\). Imagine fitting an elastic band over the set.\nfigure\nClaims:\n\n\\(\\texttt{co} \\ (X)\\) is a convex set.\n\n\nProof. Let \\(x,y\\in\\texttt{co} \\ (X)\\). Then we can write\n\\[ x = \\sum_{k+1}^m \\lambda_k x_k, \\ \\ y = \\sum_{\\ell=1}^p \\mu_\\ell y_\\ell, \\]\nfor some \\(m,p, \\{x_k\\}, \\{y_k\\}\\in X\\), convex coefficients \\(\\lambda_1,...,\\lambda_m\\), \\(\\mu_1,...,\\mu_p\\). Now let \\(\\gamma\\in[0,1]\\):\n\\[\\begin{aligned}\n(1-\\gamma)x + \\gamma y &= \\sum_{k=1}^m (1-\\gamma)\\lambda_k x_k  + \\sum_{\\ell=1}^p \\gamma\\mu_ell y_\\ell \\\\\n&= \\sum_{k=1}^{m+p} \\alpha_k z_k\n\\end{aligned}\\]\nwhere\n\\[ \\begin{aligned}\n&\\alpha_k = \\left\\lbrace\\begin{array}{ll}\n(1-\\gamma)\\lambda_k, & 1\\leq k\\leq m\\\\\n\\gamma \\mu_{k-m}, & m+1 \\leq k\\leq m+p\n\\end{array} \\right.  \\\\\n&z_k = \\left\\lbrace\\begin{array}{ll}\nx_k, & 1\\leq k\\leq m \\\\\ny_{k-m}, & m+1\\leq k \\leq m+p\n\\end{array} \\right.\n\\end{aligned}\\]\nNote that \\(\\sum_{k=1}^{m+p} \\alpha_k = 1\\) (prove this on your own). Thus, we have shown \\((1-\\gamma)x + \\gamma y \\in \\texttt{co} \\ (X)\\) \\(\\Rightarrow\\) \\(\\texttt{co} \\ (X)\\) is a convex set. ■\n\n\n\\(\\texttt{co} \\ (X)\\) is minimal in the sense that if \\(S\\subseteq\\mathbb{R}^n\\) is convex and \\(X\\subseteq S\\), then \\(\\texttt{co} \\ (X) \\subseteq S\\).\n\n\nProof. If \\(x\\in\\texttt{co} \\ (X)\\) then \\(x = \\sum_{k=1}^m \\lambda_k x_k\\), \\(x_k\\in X\\) for all \\(k=1,...,m\\).\n\\(X\\subseteq S \\Rightarrow\\) if \\(x_k\\in X\\) then \\(x_k\\in S\\). Then, \\(\\texttt{co} \\ (X)\\subseteq S\\). ■\n\n\nTheorem 6 If \\(X\\subseteq\\mathbb{R}^n\\) and \\(x\\in\\texttt{co} \\ (X)\\), then\n\\[ x = \\sum_{k=1}^{n+1} \\lambda_k x_k \\]\nfor the convex coefficients \\(\\lambda_k\\) and points \\(x_k\\in X\\).\n\n\nProof. In text.\n\n\n3.1.1 Operations that Preserve Convexity\n\nIf \\(F:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\) is affine and \\(S\\subseteq\\mathbb{R}^n\\) is convex, \\(F(S)\\subseteq\\mathbb{R}^m\\) is convex.\n\n\nProof. Let \\(y_1, y_2\\in F(S)\\). Then, \\(y_1 = F(x_1)\\) and \\(y_2 = F(x_2)\\) for \\(x_1, x_2\\in S\\). Then, for any \\(\\lambda\\in[0,1]\\)\n\\[\\begin{aligned}\n(1-\\lambda) y_1 + \\lambda y_2 &= (1-\\lambda)F(x_1) + \\lambda F(x_2) \\\\\n&= F((1-\\lambda)x_1 + \\lambda x_2) \\in F(S)\n\\end{aligned}\\] ■\n\n\n\\(\\cap_{i\\in I} S_i\\) is convex if \\(S_i\\) are convex for all \\(i\\in I\\). This is not true for unions.\nMinkowski sum. \\(S_1 + S_2 + ... + S_m = \\{x_1 + x_2 + ... + x_m \\mid x_i\\in S_i\\}\\), the Minkowski sum preserves convexity.\n\n\n\nExample\n\n\n\\(S_1 = D\\) (unit disk) in \\(\\mathbb{R}^2\\), \\(S_2\\) is the \\(x\\)-axis.\nfigure\n\n\n\nProof. Of Minkowski sum preserving convexity.\nLet \\(x, y\\in S_1 + ... + S_m\\) and \\(S_i\\) convex for all \\(i=1,...,m\\). Then, \\(x = x_1 + ... + x_m\\), \\(y = y_1 + ... + y_m\\), \\(x_i\\in s_i\\), \\(y_i\\in S_i\\). For any $\n\\[(1-\\lambda)x + \\lambda y = \\underbrace{(1-\\lambda) x_1 + \\lambda y_1}_{\\in S_1} + ... \\underbrace{(1-\\lambda) x_m + \\lambda y_m}_{\\in S_m}\\]\nBy convexity, \\((1-\\lambda)x_i + \\lambda y_i \\in S_i\\). Thus, \\((1-\\lambda)x + \\lambda y \\in S_1 + ... S_m\\). ■\n\n\n\nExample\n\n\nMinkowski sums don’t necessarily preserve set properties.\n\\[\\begin{aligned}\n& S_1 = \\{(x,y)\\in\\mathbb{R}^ \\mid y\\geq e^x\\} \\textnormal{ closed} \\\\\n& S_2 = x-\\textnormal{axis}\n\\end{aligned}\\]\n\\(S_1+S_2 = \\{(x,y)\\in\\mathbb{R}^2 \\mid y&gt;0 \\}\\) is open, but still convex.\n\n\n\nDefinition 8 Let \\(C\\) be a set in a vector space \\(V\\). Then \\(C\\) is called a cone if \\(t\\cdot x\\in C\\) for all \\(x\\in C\\), \\(t\\geq 0\\). We call \\(C\\) a convex cone is \\(C\\) is additionally convex.\n\n\nLemma 3 A set \\(C\\) is a convex cone if and only if for all \\(x, y\\in C\\) and \\(t&gt;0\\) we have that \\(tx\\in C\\) and \\(x+y\\in C\\).\n\n\nProof. If \\(C\\) is a convex cone, then\n\\[ x + y = 2\\underbrace{\\left(\\frac{x}{2} + \\frac{y}{2}\\right)}_{\\in C \\textnormal{ by convexity}}  \\in C\\]\nConversely, if \\(tx\\in C\\) and \\(x+y\\in C\\) for all \\(x,y\\in C\\), then for any \\(\\lambda\\in(0,1)\\)\n\\[\\lambda x + (1-\\lambda)y = \\lambda \\left(x + \\frac{1-\\lambda}{\\lambda} y \\right) \\in C.\\] ■\n\n\n\nExample\n\n\nThe cone of symmetric positive definite matrices, denoted \\(\\mathbf{S}\\).\nNote that for \\(A,B\\in \\mathbf{S}\\):\n\n\\(A+B\\in \\mathbf{S}\\),\n\\(t A\\in \\mathbf{S}\\), \\(t&gt;0.\\)\n\nThus, \\(\\mathbf{S}\\) is a convex cone."
  },
  {
    "objectID": "posts/opt-theory/index.html#convex-functions",
    "href": "posts/opt-theory/index.html#convex-functions",
    "title": "Optimisation Theory and Applications",
    "section": "3.2 Convex Functions",
    "text": "3.2 Convex Functions\n\nDefinition 9 For \\(f:V\\rightarrow\\mathbb{R}\\), \\(V\\) a vector space, we say that \\(f\\) is a convex function if\n\\[f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y),\\]\nwhere \\(0\\leq\\lambda \\leq 1\\).\n\nWhat convexity tells us is that the values of the function are below the line between \\(x\\) and \\(y\\).\nfigure\n\nDefinition 10 For a function \\(f:V\\rightarrow\\mathbb{R}\\) the epigraph of \\(f\\) is defined to be\n\\[ \\texttt{epi}(f) = \\{(x,\\alpha) \\mid x\\in V, \\alpha \\in \\mathbb{R}, f(x) \\leq \\alpha\\}.\\]\n\n\nProposition 3 The function \\(f\\) is convex if and only if \\(\\texttt{epi}(f)\\) is convex.\n\n\nProof. Suppose that \\(f\\) is convex. Let \\((x_1,\\alpha_1), (x_2,\\alpha_2)\\in\\texttt{epi}(f)\\). For \\(\\lambda\\in[0,1]\\) we have:\n\\[\\begin{aligned}\nf(\\lambda x_1 + (1-\\lambda)x_2) &\\leq \\lambda f(x_1) + (1-\\lambda) f(x_2) \\\\\n&\\leq \\underbrace{\\lambda \\alpha_1 + (1-\\lambda)\\alpha_2}_{\\alpha} \\\\\n&\\leq \\alpha\n\\end{aligned}\\]\nThen, \\(\\lambda(x_1,\\alpha_1) + (1-\\lambda)(x_2,\\alpha_2) = (\\lambda x_1 + (1-\\lambda)x_2, \\alpha) \\in \\texttt{epi}(f)\\) \\(\\Rightarrow\\) \\(\\texttt{epi}(f)\\) is convex.\nFor the other direction, suppose that \\(\\texttt{epi}(f)\\) is convex. Then, for all \\(x_1, x_2\\in V\\), \\(\\lambda\\in[0,1]\\) we have\n\\[\\lambda (x_1, f(x_1)) + (1-\\lambda)(x_2, f(x_2)) \\in \\texttt{epi}(f). \\]\nHence, by definition of epigraph,\n\\[f(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2),\\]\nwhich means that \\(f\\) is convex. ■\n\n\n\nExamples\n\n\n\n\\(f: x\\mapsto e^x\\)\n\\(f: x\\mapsto -\\log(x)\\), \\(x\\in\\mathbb{R}\\) positive\n\\(f: x\\mapsto \\langle a, Ax\\rangle + \\langle b,x\\rangle\\) where \\(A\\in\\mathcal{M}^n\\) is symmetric positive definite and \\(b\\) is a vector in \\(\\mathbb{R}^n\\).\n\nare convex functions.\n\n\n\nTheorem 7 (Jensen’s inequality) Let \\(f:C\\rightarrow\\mathbb{R}\\) be a convex function and \\(\\lambda_i \\in[0,1]\\), \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then,\n\\[ f(\\sum_{i=1}^n \\lambda_i x_i) \\leq \\sum_{i=1}^n \\lambda_i f(x_i), \\ x_i\\in C.\\]\n\n\nProof. Note that \\((x_i,f(x_i))\\in\\texttt{epi}(f)\\). Since \\(f\\) is convex, \\(\\texttt{epi}(f)\\) is a convex set. Thus\n\\[\\begin{aligned}\n& \\sum_{i=1}^n \\lambda_i (x_i,f(x_i)) \\in\\texttt{epi}(f) \\\\\n\\Rightarrow & \\left(\\sum_{i=1}^n \\lambda_i x_i, \\sum_{i=1}^n \\lambda_i f(x_i) \\right) \\in\\texttt{epi}(f)\\\\\n\\Rightarrow & f\\left(\\sum_{i=1}^n \\lambda_i x_i\\right) \\leq \\sum_{i=1}^n \\lambda_i f(x_i)\n\\end{aligned}\\] ■\n\nNote that we can easily conclude the arithmetic geometric mean inequality using this: take \\(\\ln(\\cdot)\\), which is a concave function (ie. the negative of a convex function),\n\\[\\begin{aligned}\n& \\ln\\left(\\frac{1}{2} x + \\frac{1}{2}y\\right) \\geq \\frac{1}{2}\\ln(x) + \\frac{1}{2}\\ln(y), \\ x,y&gt;0 \\\\\n\\Rightarrow & \\ln\\left(\\frac{1}{2} x + \\frac{1}{2}y\\right) \\geq \\ln((xy)^{1/2}) \\\\\n\\Rightarrow & \\frac{1}{2} (x + y) \\geq \\sqrt(xy)\n\\end{aligned}\\]\n\n3.2.1 Conditions of Convexity\n\nTheorem 8 (First order condition of convexity) Given \\(C\\subseteq\\mathbb{R}^n\\) convex, \\(f:C\\rightarrow\\mathbb{R}\\) continuously differentiable, we have that \\(f\\) is convex if and only if\n\\[ \\langle \\nabla f(x), y-x\\rangle \\leq f(y) - f(x), \\ \\ \\forall x,y\\in C.\\]\n\nfigure\n\nProof. \\(\\Rightarrow\\). Assume that \\(f\\) is convex. Then for \\(\\alpha\\in(0,1)\\)\n\\[\\begin{aligned}\nf(x+ \\alpha(y-x)) &= f((1-\\alpha)x + \\alpha y) \\\\\n&\\leq (1-\\alpha) f(x) + \\alpha f(x).\n\\end{aligned}\\]\nHence,\n\\[ \\frac{f(x + \\alpha(y-x)) - f(x)}{\\alpha} \\leq f(y) - f(x).\\]\nTaking the limit \\(\\alpha\\rightarrow 0\\) results in\n\\[\\langle \\nabla f(x), y-x\\rangle \\leq f(y) - f(x), \\]\nwhich is the expression given in the theorem. Note, \\(\\langle \\nabla f(x), y-x\\rangle\\) is the directional derivative of \\(f\\) in the direction \\(y-x\\), as a result of the limit.\n\\(\\Leftarrow\\). Now suppose \\(\\langle \\nabla f(x), y-x\\rangle \\leq f(y) - f(x)\\), for all \\(x,y\\in C\\). Let \\(x_\\alpha = (1-\\alpha)x + \\alpha y\\) where \\(\\alpha\\in(0,1)\\). Consider\n\\[\\begin{aligned}\n& (1-\\alpha) & f(x) \\geq f(x_\\alpha) + \\langle \\nabla f(x_\\alpha), x- x_\\alpha\\rangle \\\\\n+ & & \\\\\n& \\alpha & f(y)\\geq f(x_\\alpha) + \\langle \\nabla f(x_\\alpha),y-x_\\alpha\\rangle .\n\\end{aligned}\\]\nExpanding and simplifying results in\n\\[ \\begin{aligned}\n&(1-\\alpha) f(x) + \\alpha f(y) \\geq f(x_\\alpha) + \\langle \\nabla f(x_\\alpha), \\underbrace{(1-\\alpha)x + \\alpha y - x_\\alpha}_{=0}\\rangle \\\\\n\\Rightarrow &(1-\\alpha) f(x) + \\alpha f(y) \\geq f(x_\\alpha) \\\\\n\\Rightarrow & (1-\\alpha) f(x) + \\alpha f(y) \\geq f((1-\\alpha)x + \\alpha y),\n\\end{aligned}\\]\nwhich holds for all \\(x,y\\in C\\), \\(\\alpha\\in\\mathbb{R}\\). Thus, \\(f\\) is convex. ■\n\nNote, a function can also be strictly convex. Strict convexity is when Definition 9 holds for \\(x\\neq y\\), ie.\n\\[ f((1-\\alpha)x + \\alpha y) &lt; (1-\\alpha) f(x) + \\alpha f(y).\\]\n\nTheorem 9 (Second order condition of convexity) Let \\(C\\subseteq\\mathbb{R}^n\\), and suppose \\(f:C\\rightarrow\\mathbb{R}\\) is twice continuously differentiable. Then, \\(f\\) is convex if and only if the Hessian of \\(f\\) at \\(x\\) is positive semidefinite for all \\(x\\in C\\), ie.\n\\[\\nabla^2 f(x) \\succeq 0 \\ \\forall x\\in C.\\]\nMoreover, if \\(\\nabla^2 f(x)\\) is strictly positive definite at every \\(x\\in C\\), then \\(f\\) is strictly convex. For concavity, the opposite holds.\n\n\nProof. \\(\\Rightarrow\\). Suppose \\(f\\) is convex, by Theorem 8\n\\[ f(x) + \\langle \\nabla f(x) ,\\alpha d\\rangle \\leq f(x + \\alpha d), \\]\nfor all \\(d\\in\\mathbb{R}^n\\), \\(\\alpha &gt;0\\). Fix \\(x\\) and consider the Taylor expansion of \\(f(x+ \\alpha d)\\) where \\(d\\) is the variable\n\\[\\begin{aligned}\n& f(x) + \\langle \\nabla f(x) ,\\alpha d\\rangle \\leq f(x) \\langle \\nabla f(x) + \\alpha d\\rangle + \\frac{\\alpha^2}{2} d^T \\nabla^2 f(x) d + \\mathcal{O}(\\alpha^2) \\\\\n\\Rightarrow & \\frac{1}{2} d^T \\nabla^2 f(x) d + \\frac{\\mathcal{O}(\\alpha^2)}{\\alpha^2}  \\geq 0.\n\\end{aligned}\\]\nTaking \\(\\alpha\\rightarrow 0\\), we conclude that\n\\[\\nabla^2 f(x) \\succeq 0.\\]\n\\(\\Leftarrow\\). Now, suppose \\(\\nabla^2 f(x) \\succeq 0\\). Then, by the mean value theorem, we know that there exists \\(z = x+\\lambda y\\) where \\(0\\leq \\lambda\\leq 1\\) such that\n\\[ f(y) = f(x) + \\langle \\nabla f(x), y-x\\rangle + \\frac{1}{2}\\langle \\nabla^2 f(z)(y-x), y-x\\rangle .\\]\nSince \\(\\nabla^2 f(z) \\succeq 0\\), we have\n\\[ f(y) \\geq f(x) + \\langle \\nabla f(x), y-x\\rangle, \\]\nwhich is the first order condition of convexity. Therefore, \\(f\\) is convex. ■\n\n\n\nExample\n\n\nA function that is strictly convex, but has \\(\\nabla^2 f(x) \\nsucc 0\\), is \\(f(x) = x^4\\). \\(\\nabla^2 f(x) = 12x^2 = 0\\) at \\(x=0\\), it is not strictly positive definite. Thus, the converse does not hold in the second part of Theorem 9.\n\n\n\n\nExample\n\n\nLet \\(f(x) =  \\frac{1}{2} x^T A x + \\langle b,x\\rangle\\), \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) symmetric and \\(b\\in\\mathbb{R}^n\\). Then, \\(f\\) is convex if and only if \\(\\nabla^2 f(x) \\succeq 0\\), that is\n\\[\\nabla^2 f(x) = A \\succeq 0,\\]\nie. \\(f\\) is convex if and only if \\(A\\) is positive semidefinite.\n\n\n\nProposition 4 A function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if and only if \\(g:\\mathbb{R}\\rightarrow\\mathbb{R}\\) given by\n\\[ g(t) = f(x + yt)\\]\nis convex for all \\(x,y\\in\\mathbb{R}^n\\).\n\n\n\nExample\n\n\nLet \\(f:S^n_{&gt;0} \\rightarrow\\mathbb{R}\\), \\(f(X) = \\log\\det X\\).\nClaim: \\(f\\) is concave.\nLet \\(g(t) = \\log\\det(Z + tY)\\), \\(Z,Y\\in S^n_{&gt;0}\\).\n\\[\\begin{aligned}\ng(t) &= \\log\\det(Z + tY) \\\\\n&= \\log\\det(Z^{1/2}(I + tZ^{-1/2}YZ^{-1/2})Z^{1/2}) \\\\\n&= \\log\\det(Z) + \\log\\det(I + tZ^{-1/2}YZ^{-1/2}).\n\\end{aligned}\\]\nLet \\(\\lambda_1,...,\\lambda_n\\) be the eigenvalues of \\(Z^{-1/2}Y Z^{-1/2}\\). Then\n\\[\\begin{aligned}\n& g(t) = \\log\\det(Z) + \\sum_{i=1}^n \\log(1 + t\\lambda_i)\\\\\n& g'(t) = \\sum_{i=1}^n \\frac{\\lambda_i}{1 + t\\lambda_i} \\\\\n& g''(t) = \\sum_{i=1}^n \\frac{-\\lambda_{i}^2}{(1+t\\lambda_i)^2} \\leq 0\n\\end{aligned}\\]\nThus, \\(g\\) is concave.\n\n\nWe now note that convex functions have convex sublevel sets. This is, given \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\) convex,\n\\[S_\\alpha = \\{x \\mid f(x) \\leq \\alpha \\}\\]\nis convex.\nBut what about the converse?\nfigure\nWe can see that the sublevel sets \\(S_\\alpha\\) are convex, but clearly \\(f\\) is not.\n\nDefinition 11 Any function that has convex sublevel sets is called quasi-convex. Similarly, quasi-concave functions have convex superlevel sets.\n\nNote that all convex functions are also quasi-convex."
  },
  {
    "objectID": "posts/opt-theory/index.html#separation-of-convex-sets",
    "href": "posts/opt-theory/index.html#separation-of-convex-sets",
    "title": "Optimisation Theory and Applications",
    "section": "3.3 Separation of Convex Sets",
    "text": "3.3 Separation of Convex Sets\nends at separating hyperplane proposition"
  },
  {
    "objectID": "posts/opt-theory/index.html#conjugate-functions",
    "href": "posts/opt-theory/index.html#conjugate-functions",
    "title": "Optimisation Theory and Applications",
    "section": "4.1 Conjugate Functions",
    "text": "4.1 Conjugate Functions"
  },
  {
    "objectID": "posts/opt-theory/index.html#duality-theory",
    "href": "posts/opt-theory/index.html#duality-theory",
    "title": "Optimisation Theory and Applications",
    "section": "4.2 Duality Theory",
    "text": "4.2 Duality Theory"
  },
  {
    "objectID": "posts/opt-theory/index.html#nonlinear-programming",
    "href": "posts/opt-theory/index.html#nonlinear-programming",
    "title": "Optimisation Theory and Applications",
    "section": "4.3 Nonlinear Programming",
    "text": "4.3 Nonlinear Programming\n\n4.3.1 Conditions for Optimality\n\n\n4.3.2 NLP and Duality"
  },
  {
    "objectID": "posts/opt-theory/index.html#gradient-methods---first-order",
    "href": "posts/opt-theory/index.html#gradient-methods---first-order",
    "title": "Optimisation Theory and Applications",
    "section": "5.1 Gradient Methods - First Order",
    "text": "5.1 Gradient Methods - First Order"
  },
  {
    "objectID": "posts/opt-theory/index.html#gradient-methods---second-order",
    "href": "posts/opt-theory/index.html#gradient-methods---second-order",
    "title": "Optimisation Theory and Applications",
    "section": "5.2 Gradient Methods - Second Order",
    "text": "5.2 Gradient Methods - Second Order"
  },
  {
    "objectID": "posts/opt-theory/index.html#minimising-polynomials",
    "href": "posts/opt-theory/index.html#minimising-polynomials",
    "title": "Optimisation Theory and Applications",
    "section": "6.1 Minimising Polynomials",
    "text": "6.1 Minimising Polynomials"
  }
]
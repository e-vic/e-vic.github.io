[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "Optimisation Theory and Applications\n\n\n\n\n\n\nnotes\n\n\noptimisation\n\n\n\nThis is a write up of notes from the Optimisation Theory and Applicaitons course taught by Bahman Gharesifard in 2016 at Queen’s University. \n\n\n\n\n\nJul 1, 2024\n\n\nBahman Gharesifard, Emma Hansen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/opt-theory/index.html#examples-of-optimisation-problems",
    "href": "posts/opt-theory/index.html#examples-of-optimisation-problems",
    "title": "Optimisation Theory and Applications",
    "section": "1.1 Examples of Optimisation Problems",
    "text": "1.1 Examples of Optimisation Problems\n\n\nExample: 150BC – Alexandria’s Problem\n\n\n\n\n\n\n\nFind a point \\(D\\in\\mathbb{R}^2\\) such that \\(||AD||+||DB||\\) is minimised.\nConsider the figure below.\n\n\n\n\n\nClaim: this \\(D\\) is the solution.\n\nProof. Suppose otherwise, i.e. let some point \\(D'\\) be the minimiser:\n\\[\\begin{aligned}\n||AD'|| + ||D'B|| &\\geq ||AB'|| \\textrm{ (from the claim)} \\\\\n&= ||AD|| + ||DB'|| \\\\\n&= ||AD|| + ||DB||\n\\end{aligned}\\]\nWhy is this true? Triangle inequality! Thus, \\(D\\) is the minimiser. ■\n\n\n\n\n\nExample: 850BC – Dido’s Problem\n\n\n\n\n\n\n\nConsider a curve of fixed length \\(\\ell\\). What is the maximum area of land you can enclose using this curve?\nTo start, think about what kind of shape you should use.\nFormally, the problem is: \\(\\max_y \\ \\ J(y) = \\int_a^b y(x) dx, \\ y\\in\\mathcal{C}([a,b])\\).\nIt turns out the optimal shape is a semicircle, but we will not be able to prove this even at the end of the course.\n\n\n\n\nExample: 1556 – Tartaglia’s Problem\n\n\nThis problem is also known as the Cardano-Tartaglia problem.\nPick two numbers \\(x\\) and \\(y\\) such that:\n\nthey sum to 8: \\(x+y = 8 \\Rightarrow y = (8-x)\\)\ntheir product, \\(xy\\), and difference, \\(x-y\\), is maximised\n\nToday this problem is easy to solve (because of calculus): we need to maximise \\(f(x) = (xy)(x-y) = x(8-x)(x-(8-x)) = -2x^3 + 24x^2 - 64x\\).\nSolution: \\(x = 4(1-\\frac{1}{\\sqrt{3}})\\).\n\n\n\n\nExample: 1954 – Max-flow\n\n\nConsider a directed graph \\(\\mathcal{G}(V,E)\\).\n\n\n\n\n\nLet each edge \\(ij\\) have a capacity \\(c_{ij}\\).\nWe are looking for a map \\(f:E\\rightarrow \\mathbb{R}_{\\geq 0}\\) such that\n\n\\(f(ij)\\leq c_{ij}\\)\nFlow is conserved at each vertex: incoming = outgoing\n\nWith these conditions, we want to maximise flow across the graph: \\(\\max \\ |f|\\) over all routes through the graph."
  },
  {
    "objectID": "posts/opt-theory/index.html#notations",
    "href": "posts/opt-theory/index.html#notations",
    "title": "Optimisation Theory and Applications",
    "section": "1.2 Notations",
    "text": "1.2 Notations\n\nFor parts of this course, we work with functionals on normed vector spaces. We usually consider functions on subsets of \\(\\mathbb{R}^n\\).\n\n\\[\\begin{aligned}\n&f: D\\subset\\mathbb{R}^n \\rightarrow \\mathbb{R},\\\\\n&f: E \\rightarrow \\mathbb{R}, \\ E \\textnormal{ a normed vector space}.\n\\end{aligned}\\]\n\nWe usually assume \\(f\\) is differentiable, specifically Fréchet or Gâteaux differentiable. Nevertheless, we denote gradient (or derivative) of \\(f\\) by \\(\\nabla f\\):\n\n\\[\\nabla f = (\\frac{\\partial f}{\\partial x_1},...,\\frac{\\partial f}{\\partial x_n})^T\\]"
  },
  {
    "objectID": "posts/opt-theory/index.html#references",
    "href": "posts/opt-theory/index.html#references",
    "title": "Optimisation Theory and Applications",
    "section": "1.3 References",
    "text": "1.3 References\n\nConvex Optimisation - Boyd and Vandenberghe (2004)\nFoundations of Optimisation - Güler (2010)\nNumerical Optimisation - Nocedal and Wright (2006)"
  },
  {
    "objectID": "posts/opt-theory/index.html#unconstrained-optimisation",
    "href": "posts/opt-theory/index.html#unconstrained-optimisation",
    "title": "Optimisation Theory and Applications",
    "section": "2.1 Unconstrained Optimisation",
    "text": "2.1 Unconstrained Optimisation\nLet \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\),\n\\[ \\min_{x\\in\\mathbb{R}^n} \\ \\ f(x) \\]\nis an unconstrained optimisation problem.\n\n2.1.1 Necessary Conditions of Optimality\nConsider a \\(C^1\\)-function \\(f\\) on \\(\\mathbb{R}^n\\) and suppose tht \\(x^*\\) is a local minimum of \\(f\\) in some open set \\(D\\subset \\mathbb{R}^n\\). Note that \\(x^*\\) is in the interior of \\(D\\).\n\n\n\n\n\nLet \\(d\\) be some vector in \\(\\mathbb{R}^n\\), and \\(\\alpha\\in\\mathbb{R}\\). Consider:\n\\[g(\\alpha) = f(x^* + \\alpha d).\\]\nNote that: \\(\\alpha = 0\\) is the local minimum of \\(g\\). Using a first order approximation:\n\\[ g(\\alpha) = g(0) + g'(0)(\\alpha - 0) + \\mathcal{O}(\\alpha),\\]\nwhere \\(\\lim_{\\alpha\\rightarrow } \\frac{\\mathcal{O}(\\alpha)}{\\alpha} = 0\\).\n\nLemma 1 We have that \\(g'(0) = 0\\).\n\n\nProof. Suppose otherwise. Then for \\(\\varepsilon &gt;0\\) small enough and \\(\\alpha \\neq 0\\) with \\(|\\alpha|&lt;\\varepsilon\\), we have\n\\[\\begin{aligned}\n|g'(0) \\alpha| &gt; |\\mathcal{O}(\\alpha)|\\\\\n\\Rightarrow g(\\alpha) - g(0) &lt; g'(0) \\alpha + |g'(0)\\alpha|\n\\end{aligned}\\]\nIf we let \\(\\alpha\\) have the opposite sign (since \\(\\alpha\\) is any number in \\(\\mathbb{R}\\)) of \\(g'(0)\\), then \\(g(\\alpha) &lt; g(0)\\). Which is a contradiction! Thus \\(g'(0)=0\\). ■\n\nNow, with \\(g'(0) = 0\\), we consider what this means for \\(f\\).\n\\[\\begin{aligned}\ng'(\\alpha) &= \\nabla f (x^* + \\alpha d) \\cdot \\vec{d} \\\\\n&= \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}(x^* + \\alpha d) \\cdot d_i\n\\end{aligned}\\]\nThen, \\(g'(0) = \\nabla f(x^*) \\cdot d = 0\\) for all \\(d\\in\\mathbb{R}^n\\). Thus, \\(\\nabla f(x^*) = 0\\). This is the first necessary condition of optimality.\n\n\n2.1.2 Sufficient Conditions of Optimality\nConsider now the function \\(f\\), this time in \\(C^2\\).\n\\[g(\\alpha) = g(0) + g'(0) \\alpha + \\frac{1}{2}g''(0)\\alpha^2 + \\mathcal{O}(\\alpha^2),\\]\nwith \\(\\lim_{\\alpha\\rightarrow 0} \\frac{\\mathcal{O}(\\alpha^2)}{\\alpha^2} = 0\\).\n\nLemma 2 If \\(x^*\\) is a local minimum, then \\(g''(0) \\geq 0\\).\n\n\nProof. Suppose otherwise. I.e. \\(g''(0) &lt; 0\\). Then, or \\(\\alpha\\) small enough, we have\n\\[\\frac{1}{2} |g''(0)|\\alpha^2 &gt; \\mathcal{O}(\\alpha^2). \\]\nSince we know that \\(g'(0) = 0 \\Rightarrow g(\\alpha) - g(0) &lt; \\frac{1}{2} g''(0)\\alpha^2 + \\frac{1}{2}|g''(0)|\\alpha^2\\) and \\(\\frac{1}{2}g''(0)\\alpha^2 &lt;0\\), then $g()- g(0) &lt;0 $, which is contradicts \\(g(0)\\) being the minimiser! ■\n\nLet us compute \\(g''(0)\\):\n\\[\\begin{aligned}\ng''(\\alpha) &= \\frac{d}{dx} g'(\\alpha) \\\\\n&= \\frac{d}{d\\alpha} \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}(x^* + \\alpha d) d_i \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (x^* + \\alpha d) d_i d_j\n\\end{aligned}\\]\nThen, \\(g''(0) = \\sum_{i,j = 1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (x^*)d_i d_j = d^T \\nabla^2 f(x^*) d\\). And since \\(g''(0)\\geq 0\\), then \\(d^T \\nabla^2 f(x^*) d \\geq 0\\) for all \\(d\\), which implies \\(\\nabla^2 f(x^*)\\) is symmetric and positive semidefinite.\n\n\n\n\n\n\nSymmetric matrices\n\n\n\nConsider an \\(n\\times n\\) real symmetric matrix \\(A\\in\\mathbb{R}^{n\\times n}\\). It is well known and easy to show that \\(A\\) has real eigenvalues, denote these by \\(\\lambda_1,...,\\lambda_n\\in\\mathbb{R}\\), with corresponding eigenvectors \\(\\vec{u}_1,...,\\vec{u}_n\\) such that \\(\\langle \\vec{u}_i, \\vec{u}_j\\rangle = 0\\) when \\(i\\neq j\\), and \\(||\\vec{u}_i|| = 1\\). Let now\n\\[ \\Lambda = \\left[\\begin{array}{ccc}\n\\lambda_1 & ... & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & ... & \\lambda_n\n\\end{array}\\right] = \\texttt{diag}(\\lambda_1,...,\\lambda_n), \\ \\ U = \\left[\\vec{u}_1,...,\\vec{u}_n \\right].\\]\nNote that \\(AU = \\left[A\\vec{u}_1,...,A\\vec{u}_n \\right] = \\left[\\lambda_1\\vec{u}_1,...,\\lambda_n\\vec{u}_n \\right] = U\\Lambda\\).\nSince \\(U^T U = I_n\\) (identity), then \\(A = A(U U^T) = (AU)U^T = U\\Lambda U^T\\). (note: \\(U\\) is non-singular).\n\nTheorem 3 (Spectral Decomposition of Symmetric Matrices) Let \\(A\\) be a real symmetric matrix. Then, \\(A\\) can be written as\n\\[ A = U \\Lambda U^T.\\]\n\n\nCorollary 1 \\(A\\) is positive semidefinite if and only if all eigenvalues of \\(A\\) are non-negative. Similarly, positive definite if and only if all eigenvalues are strictly positive.\n\n\nProof. By the previous result,\n\\[\\begin{aligned}\nd^T A d &= d^T U\\Lambda U^T d \\\\\n&= (U^Td)^T \\Lambda (U^T d)\n\\end{aligned}.\\]\nSince \\(U\\) is non-singular, we have that \\(d^T A d\\geq 0\\) for all \\(d\\) if and only if \\(d^T \\Lambda d\\geq 0\\) for all \\(d\\).\n\\[ \\begin{aligned}\nd^T \\Lambda d &= \\sum_{i=1}^n \\lambda_a d_i^2 \\geq 0 \\ \\ \\forall d_i \\\\\n&\\Rightarrow \\lambda_i \\geq 0 \\ \\forall i.\n\\end{aligned}\\]\nThe proof of the second part is similar. ■\n\n\n\nIt is easy to see that the necessary conditions we have are not sufficient. For example, take \\(f: x\\mapsto x^3\\). However, if \\(f\\) is in \\(C^2\\) and \\(\\nabla f (X^*) = 0\\), \\(\\nabla^2f(x^*) \\prec 0\\), then \\(x^*\\) is a minimiser.\n\nProof. To show this, note that\n\\[f(x^* + \\alpha d) = f(x^*) + \\frac{1}{2}\\alpha^2 d^T\\nabla^2 f(x^*) d + ... \\ .\\]\nAgain, we can choose $small enough to have\n\\[ \\frac{\\alpha^2}{2} d^T \\nabla^2 f(x^*) d &gt; |\\mathcal{O}(\\alpha^2)|.\\]\nSince \\(\\nabla^2 f(x^*)\\) is positive definite, then \\(f(x^* + \\alpha d) &gt; f(x^*)\\) for all \\(d\\). Thus, \\(x^*\\) is a local minimum! ■\n\nNote that: \\(\\alpha\\) depends on \\(d\\). If \\(|\\alpha| &lt; \\varepsilon^*(d)\\), different \\(d\\) will result in different \\(\\varepsilon\\), thus, choose the smallest \\(\\varepsilon\\). Without loss of generality assume \\(||d|| = 1\\), \\(\\varepsilon^* = \\min(\\varepsilon(d))\\) which results from Weierstrass theorem.\n\n\n\n\n\n\n\nExample: Arithmetic-Geometric mean inequality\n\n\nWe show that: for \\(x_i&gt;0 \\in\\mathbb{R} \\ \\forall i\\)\n\\[ (x_1 \\cdot ... \\cdot x_n)^{1/n} \\leq \\frac{1}{n} \\sum_{i=1}^n x_i .\\]\n\nProof. Let \\(y_i = \\ln(x_i)\\) and let\n\\[ f(y_1,...,y_n) \\triangleq \\frac{1}{n} \\sum_{i=1}^n e^{y_i} - e^{\\frac{y_1 + ... + y_n}{n}} \\]\n(this came from replacing \\(x_i\\) with \\(e^{y_i}\\) in the statement.)\nNote that we want tot show \\(f(y_1,..,y_n) \\geq 0\\). This is the same problem as above. It is enough to show that \\(f\\) acheives its minimum value at zero.\nTo do this, use the first order condition of optimality:\n\\[ \\frac{\\partial f}{\\partial y_i} = \\frac{1}{n} e^{y_i} - \\frac{1}{n} e^{\\frac{y_1 + ... + y_n}{n}} \\ \\ \\forall i \\]\nwhich are zero if and only if \\(e^{y_i} = e^{\\frac{y_1 + ... + y_n}{n}} \\ \\Rightarrow y_i = \\frac{y_1 + ... + y_n}{n}\\) for all \\(i\\). (Check this for yourselves).\nThis gives a system of linear equations with solution \\(y_i = \\frac{Y}{n}\\) for all \\(i\\). i.e. picked all \\(y_i\\) to be the same for any value of \\(Y\\in\\mathbb{R}\\).\nNote that \\(f\\left(\\frac{Y}{n},...,\\frac{Y}{n}\\right) = 0\\).\nWe now need to justify that we have a global minimiser. (And, ideally a unique one.)\nWe reduce the problem over the set\n\\[\\{(y_1,...,y_n) | \\sum_{i=1}^n y_i = Y\\}\\]\nfor some \\(Y\\in\\mathbb{R}\\). Using this, and by eliminating \\(y_n\\) (by saying \\(y_n = Y - y_1 - ... - y_{n-1}\\)), define\n\\[g(y_1,...,y_{n-1}) = \\frac{1}{n}\\left( e^{y_1} + ... + e^{y_{n-1}} + e^{Y - y_1 - ... - y_{n-1}}\\right) - e^\\frac{Y}{n}.\\]\nIt is easy to check now that \\(g\\) has a unique critical point\n\\[y_i = \\frac{Y}{n}, \\ \\ i\\in\\{1,...,n-1\\}.\\]\nThe function \\(g\\) is coercive and hence has a global minimum, which has to be\n\\[y_i = \\frac{Y}{n}, \\ \\ i\\in\\{1,...,n-1\\}.\\] ■"
  },
  {
    "objectID": "posts/opt-theory/index.html#constrained-optimisation",
    "href": "posts/opt-theory/index.html#constrained-optimisation",
    "title": "Optimisation Theory and Applications",
    "section": "2.2 Constrained Optimisation",
    "text": "2.2 Constrained Optimisation\n\n\n\n\n\n\nwe assume for now that \\(D\\) is not of lower dimension, \\(D\\subseteq \\mathbb{R}^n\\)\n\\(D\\) is assumed to be closed and bounded\n\nWe wish to study the minimisers of\n\\[f: D\\rightarrow \\mathbb{R}\\]\nWe can even assume that \\(f\\) has a minimiser in \\(D\\). This minimiser could be on the boundary.\nNote that we cannot conclude that if\n\\[ g(\\alpha) = f(x^* + \\alpha d),\\]\nwhere \\(d\\) is a feasible direction, we have \\(g'(0) = 0\\) (since \\(\\alpha\\) is dependent on \\(d\\)). We still have that, for \\(\\alpha&gt;0\\) small enough,\n\\[\\begin{aligned}\n& g(\\alpha) - g(0) &lt; g'(0)\\alpha + |g'(0)|\\alpha \\\\\n\\Rightarrow & g(\\alpha) - g(0) &lt; \\alpha (g'(0) + |g'(0)|).\n\\end{aligned} \\tag{1}\\]\n\n\n\n\n\n\nClaim\n\n\n\n\\(g'(0)\\geq 0\\).\n\n\n\nProof. Suppose otherwise (ie. \\(g'(0)&lt;0\\)). By choosing \\(\\alpha&gt;0\\) small enough and using Equation 1 we have\n\\[\\begin{aligned}\n& g(\\alpha) &lt; g(0) \\\\\n\\Rightarrow & g'(0) \\geq 0\n\\end{aligned}\\]■\n\nRecall $g’(0) = f(x^*)d $. (from first order optimality).\nOne can also conclude a weaker version of the second order condition of optimality:\n\\[d^T \\nabla^2 f(x^*) d \\geq 0\\]\nwhen \\(\\nabla f(x^*)\\cdot d = 0\\), which we call the second order condition of optimality.\n\nWe cannot use \\(x^* + \\alpha d\\)! Doing so would take us out of the surface.\n\n\n\n\n\n\nObjective\n\n\n\nMinimise \\(f(x)\\) subject to \\(D\\).\n\n\nThis optmisation problem can be written as\n\\[\\left\\{ \\begin{array}{ll}\n\\min f(x) & \\\\\nh_i(x) = 0 & i=1,...,m\n\\end{array} \\right. ,\\]\nwhere \\(D\\) is described usin functions \\(h_i\\).\nWhat if instead we use some curves, \\(\\gamma(\\cdot)\\), passing through \\(x^*\\), ie. we take \\(\\gamma(0) = x^*\\). Let us take these curves to be at least \\(C^1\\).\n\n\n\n\n\nLet us define \\(g(\\alpha) = f(\\gamma(\\alpha))\\). We can now carry the same argument as before, using \\(g(\\alpha) = g(0) + \\alpha g'(0) + \\mathcal{O}(\\alpha)\\), and argue that\n\\[\\nabla f(\\gamma(0))\\cdot \\gamma'(0) = 0.\\]\nWe need to study this \\(\\gamma'(0)\\) more carefully.\nNote that \\(\\gamma'(0)\\) lives in the so called tangent space at \\(x^*\\) which we denote by \\(T_{x^*}D\\) (this is a subspace of \\(\\mathbb{R}^n\\)). We have not used \\(h_i(\\gamma(\\alpha)) = 0 \\ \\forall i\\), differentiating to get \\(\\nabla h_i(\\gamma(\\alpha)) \\cdot \\gamma'(\\alpha) = 0\\). Choosing \\(\\alpha = 0\\), we have\n\\[\\nabla h_i(x^*) \\cdot \\gamma'(0) = 0\\]\n\n\n\n\n\n\nAssumption\n\n\n\nWe assume that \\(x^*\\) is regular. This means that the gradients \\(\\nabla h_i(x^*)\\) for \\(i\\in\\{1,...,m\\}\\) are linearly independent.\n\n\nNote that since \\(x^*\\) is regular, it’s also true that any vector \\(d\\in\\mathbb{R}^n\\) for which \\(\\nabla h_i(x^*) \\cdot d = 0\\) for all \\(i\\) has to lie in the tangent space \\(T_{x^*}D\\). (check this on your own)\nIn summary, we showed that\n\\[\\nabla f(x^*)\\cdot d = 0 \\tag{2}\\]\nfor all \\(d\\) such that \\(\\nabla h_i(x^*)\\cdot d = 0\\) for all \\(i\\).\n\nProposition 2 If Equation 2 holds, and \\(x^*\\) is regular, then we have that\n\\[\\nabla f(x^*) \\in \\texttt{span}\\{\\nabla h_i(x^*), \\ i\\in\\{1,...,m\\}\\}.\\]\nie. there exists a \\(\\lambda_i^*\\in\\mathbb{R}\\) such that\n\\[\\nabla f(x^*) + \\lambda_1^* \\nabla h_1(x^*) + ... + \\lambda_m^* \\nabla h_m(x^*) = 0.\\]\nThese \\(\\lambda_i^*\\) are called Langrange multipliers.\n\n\nProof. Use contradiction, do on your own."
  },
  {
    "objectID": "posts/opt-theory/index.html#convex-sets",
    "href": "posts/opt-theory/index.html#convex-sets",
    "title": "Optimisation Theory and Applications",
    "section": "3.1 Convex Sets",
    "text": "3.1 Convex Sets\n\nDefinition 6 (Convex set - informal) A set \\(S\\subseteq\\mathbb{R}^n\\) is convex is for all \\(x,y\\in S\\), the line segment connecting \\(x\\) and \\(y\\) is in \\(S\\).\n\n\n\n\n\n\n\n\n\nConvex.\n\n\n\n\n\n\n\nNot convex.\n\n\n\n\n\n\n\nDefinition 7 (Convex set - formal) \\(S\\subseteq\\mathbb{R}^n\\) is convex if for all \\(x,y\\in S\\) and \\(\\lambda\\in[0,1]\\),\n\\[(1-\\lambda) x + \\lambda y\\in S.\\]\n\n\n\nExamples\n\n\n\n\\(S_1 = \\{(x,y)\\in\\mathbb{R}^2 \\mid x^2 + y^2 \\leq 1\\}\\) unit disk is convex. \n\\(S_2 = \\{(x,0)\\in\\mathbb{R}^2 \\mid x\\in \\mathbb{R}\\}\\) (x-axis) is affine \\(\\Rightarrow\\) convex.\n\\(S_3 = \\{(x,y)\\in\\mathbb{R}^2 \\mid y\\geq e^x\\}\\) is convex. \n\\(S_4 = \\{(x,y)\\in\\mathbb{R}^2 \\mid y \\geq\\sin(x)\\}\\) is not convex. \n\n\n\nIf we are given \\(m\\) real numbers \\(\\lambda_1,...,\\lambda_m\\) such that \\(\\lambda_i\\geq 0\\) and \\(\\sum_{i=1}^m \\lambda_i = 1\\), then these are called convex coefficients, and a sum\n\\[\\sum_{i=1}^m \\lambda_i x_i\\]\nis called a convex combination of vectors \\(x_i\\in\\mathbb{R}^n\\), \\(i=1,...,m\\). When:\n\n\\(m=2\\): line \n\\(m=3\\): surface \n\n\nTheorem 5 The set \\(S\\subseteq \\mathbb{R}^n\\) is convex if and only if \\(S\\) contains all convex combinations of its elements.\n\n\nProof. \\(\\Leftarrow\\) Set \\(m=2\\), then it follows from the definition.\n\\(\\Rightarrow\\) By induction on \\(m\\).\nBase case: \\(m=2\\). By definition of convexity, \\(\\lambda x + (1-\\lambda)y \\in S\\). Let \\(\\lambda_1 = \\lambda\\), \\(\\lambda_2 = 1-\\lambda\\).\nInductive step: Assume true for \\(2,...,m\\) and prove for \\(m+1\\). Show that a convex combination of \\(m+1\\) elements of \\(S\\) lies in \\(S\\). Let \\(x_1,...,x_{m+1}\\in S\\), \\(\\lambda_1,...,\\lambda_{m+1}\\in\\mathbb{R}\\), \\(\\lambda_i\\geq 0\\) and \\(\\sum_{i=1}^{m+1}\\lambda_i =1\\).\nAssume \\(\\lambda_{m+1}\\neq 0\\), then\n\\[ \\begin{aligned}\n\\sum_{k=1}^{m+1} \\lambda_k x_k &= \\lambda_{m+1} x_{m+1} + \\sum_{k=1}^{m+1} \\lambda_k x_k \\\\\n&= \\lambda_{m+1} x_{m+1} + (1-\\lambda) \\sum_{k=1}^{m} \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k.\n\\end{aligned}\\]\nNeed to show \\(\\sum_{k=1}^{m} \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k\\) is in \\(S\\).\nClaim: \\(\\frac{\\lambda_k}{1-\\lambda_{m+1}}\\) are convex coefficients for all \\(k=1,...,m\\).\n\\[\\sum_{k=1}^m \\frac{\\lambda_k}{1-\\lambda_{m+1}} = \\frac{1}{1-\\lambda_{m+1}} \\sum_{k=1}^m \\lambda_k = \\frac{1-\\lambda_{m+1}}{1-\\lambda_{m+1}} = 1\\]\nThus, \\(\\sum_{k=1}^m \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k\\) is a convex combination of \\(m\\) points in \\(S\\), and is therefore in \\(S\\).\nThus, \\(\\lambda_{m+1} x_{m+1} + (1-\\lambda) \\sum_{k=1}^{m} \\frac{\\lambda_k}{1-\\lambda_{m+1}} x_k \\in S\\). ■\n\nWe now introduce an object called the convex hull. Given a set \\(X\\subseteq\\mathbb{R}^n\\), we want to construct a convex set which contains \\(X\\) and is as small as possible. Define\n\\[ \\texttt{co} \\ (X) = \\left\\lbrace \\sum_{k=1}^m \\lambda_k x_k \\mid x_k\\in X, \\lambda_k \\geq 0, \\sum_{k=1}^m \\lambda_k = 1, \\textnormal{ for all } m \\right\\rbrace .\\]\nThe convex hull is the set of all convex combinations of elements of \\(X\\). Imagine fitting an elastic band over the set.\n\n\n\n\n\n\n\n\n\n\n\nClaims\n\n\n\n\n\\(\\texttt{co} \\ (X)\\) is a convex set.\n\\(\\texttt{co} \\ (X)\\) is minimal in the sense that if \\(S\\subseteq\\mathbb{R}^n\\) is convex and \\(X\\subseteq S\\), then \\(\\texttt{co} \\ (X) \\subseteq S\\).\n\n\n\n\nProof (of 1). Let \\(x,y\\in\\texttt{co} \\ (X)\\). Then we can write\n\\[ x = \\sum_{k+1}^m \\lambda_k x_k, \\ \\ y = \\sum_{\\ell=1}^p \\mu_\\ell y_\\ell, \\]\nfor some \\(m,p, \\{x_k\\}, \\{y_k\\}\\in X\\), convex coefficients \\(\\lambda_1,...,\\lambda_m\\), \\(\\mu_1,...,\\mu_p\\). Now let \\(\\gamma\\in[0,1]\\):\n\\[\\begin{aligned}\n(1-\\gamma)x + \\gamma y &= \\sum_{k=1}^m (1-\\gamma)\\lambda_k x_k  + \\sum_{\\ell=1}^p \\gamma\\mu_ell y_\\ell \\\\\n&= \\sum_{k=1}^{m+p} \\alpha_k z_k\n\\end{aligned}\\]\nwhere\n\\[ \\begin{aligned}\n&\\alpha_k = \\left\\lbrace\\begin{array}{ll}\n(1-\\gamma)\\lambda_k, & 1\\leq k\\leq m\\\\\n\\gamma \\mu_{k-m}, & m+1 \\leq k\\leq m+p\n\\end{array} \\right.  \\\\\n&z_k = \\left\\lbrace\\begin{array}{ll}\nx_k, & 1\\leq k\\leq m \\\\\ny_{k-m}, & m+1\\leq k \\leq m+p\n\\end{array} \\right.\n\\end{aligned}\\]\nNote that \\(\\sum_{k=1}^{m+p} \\alpha_k = 1\\) (prove this on your own). Thus, we have shown \\((1-\\gamma)x + \\gamma y \\in \\texttt{co} \\ (X)\\) \\(\\Rightarrow\\) \\(\\texttt{co} \\ (X)\\) is a convex set. ■\n\n\nProof (of 2). If \\(x\\in\\texttt{co} \\ (X)\\) then \\(x = \\sum_{k=1}^m \\lambda_k x_k\\), \\(x_k\\in X\\) for all \\(k=1,...,m\\).\n\\(X\\subseteq S \\Rightarrow\\) if \\(x_k\\in X\\) then \\(x_k\\in S\\). Then, \\(\\texttt{co} \\ (X)\\subseteq S\\). ■\n\n\nTheorem 6 If \\(X\\subseteq\\mathbb{R}^n\\) and \\(x\\in\\texttt{co} \\ (X)\\), then\n\\[ x = \\sum_{k=1}^{n+1} \\lambda_k x_k \\]\nfor the convex coefficients \\(\\lambda_k\\) and points \\(x_k\\in X\\).\n\n\nProof. In text (Güler 2010).\n\n\n3.1.1 Operations that Preserve Convexity\n\nIf \\(F:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\) is affine and \\(S\\subseteq\\mathbb{R}^n\\) is convex, \\(F(S)\\subseteq\\mathbb{R}^m\\) is convex.\n\n\nProof. Let \\(y_1, y_2\\in F(S)\\). Then, \\(y_1 = F(x_1)\\) and \\(y_2 = F(x_2)\\) for \\(x_1, x_2\\in S\\). Then, for any \\(\\lambda\\in[0,1]\\)\n\\[\\begin{aligned}\n(1-\\lambda) y_1 + \\lambda y_2 &= (1-\\lambda)F(x_1) + \\lambda F(x_2) \\\\\n&= F((1-\\lambda)x_1 + \\lambda x_2) \\in F(S)\n\\end{aligned}\\] ■\n\n\n\\(\\cap_{i\\in I} S_i\\) is convex if \\(S_i\\) are convex for all \\(i\\in I\\). This is not true for unions.\nMinkowski sum. \\(S_1 + S_2 + ... + S_m = \\{x_1 + x_2 + ... + x_m \\mid x_i\\in S_i\\}\\), the Minkowski sum preserves convexity.\n\n\n\nExample\n\n\n\\(S_1 = D\\) (unit disk) in \\(\\mathbb{R}^2\\), \\(S_2\\) is the \\(x\\)-axis.\n\n\n\n\n\n\n\n\nProof. Of Minkowski sum preserving convexity.\nLet \\(x, y\\in S_1 + ... + S_m\\) and \\(S_i\\) convex for all \\(i=1,...,m\\). Then, \\(x = x_1 + ... + x_m\\), \\(y = y_1 + ... + y_m\\), \\(x_i\\in s_i\\), \\(y_i\\in S_i\\). For any $\n\\[(1-\\lambda)x + \\lambda y = \\underbrace{(1-\\lambda) x_1 + \\lambda y_1}_{\\in S_1} + ... \\underbrace{(1-\\lambda) x_m + \\lambda y_m}_{\\in S_m}\\]\nBy convexity, \\((1-\\lambda)x_i + \\lambda y_i \\in S_i\\). Thus, \\((1-\\lambda)x + \\lambda y \\in S_1 + ... S_m\\). ■\n\n\n\nExample\n\n\nMinkowski sums don’t necessarily preserve set properties.\n\\[\\begin{aligned}\n& S_1 = \\{(x,y)\\in\\mathbb{R}^ \\mid y\\geq e^x\\} \\textnormal{ closed} \\\\\n& S_2 = x-\\textnormal{axis}\n\\end{aligned}\\]\n\\(S_1+S_2 = \\{(x,y)\\in\\mathbb{R}^2 \\mid y&gt;0 \\}\\) is open, but still convex.\n\n\n\nDefinition 8 Let \\(C\\) be a set in a vector space \\(V\\). Then \\(C\\) is called a cone if \\(t\\cdot x\\in C\\) for all \\(x\\in C\\), \\(t\\geq 0\\). We call \\(C\\) a convex cone is \\(C\\) is additionally convex.\n\n\nLemma 3 A set \\(C\\) is a convex cone if and only if for all \\(x, y\\in C\\) and \\(t&gt;0\\) we have that \\(tx\\in C\\) and \\(x+y\\in C\\).\n\n\nProof. If \\(C\\) is a convex cone, then\n\\[ x + y = 2\\underbrace{\\left(\\frac{x}{2} + \\frac{y}{2}\\right)}_{\\in C \\textnormal{ by convexity}}  \\in C\\]\nConversely, if \\(tx\\in C\\) and \\(x+y\\in C\\) for all \\(x,y\\in C\\), then for any \\(\\lambda\\in(0,1)\\)\n\\[\\lambda x + (1-\\lambda)y = \\lambda \\left(x + \\frac{1-\\lambda}{\\lambda} y \\right) \\in C.\\] ■\n\n\n\nExample\n\n\nThe cone of symmetric positive definite matrices, denoted \\(\\mathbf{S}\\).\nNote that for \\(A,B\\in \\mathbf{S}\\):\n\n\\(A+B\\in \\mathbf{S}\\),\n\\(t A\\in \\mathbf{S}\\), \\(t&gt;0.\\)\n\nThus, \\(\\mathbf{S}\\) is a convex cone."
  },
  {
    "objectID": "posts/opt-theory/index.html#convex-functions",
    "href": "posts/opt-theory/index.html#convex-functions",
    "title": "Optimisation Theory and Applications",
    "section": "3.2 Convex Functions",
    "text": "3.2 Convex Functions\n\nDefinition 9 For \\(f:V\\rightarrow\\mathbb{R}\\), \\(V\\) a vector space, we say that \\(f\\) is a convex function if\n\\[f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y),\\]\nwhere \\(0\\leq\\lambda \\leq 1\\).\n\n\n\n\nWhat convexity tells us is that the values of the function are below the line between \\(x\\) and \\(y\\).\n\nDefinition 10 For a function \\(f:V\\rightarrow\\mathbb{R}\\) the epigraph of \\(f\\) is defined to be\n\\[ \\texttt{epi}(f) = \\{(x,\\alpha) \\mid x\\in V, \\alpha \\in \\mathbb{R}, f(x) \\leq \\alpha\\}.\\]\n\n\nProposition 3 The function \\(f\\) is convex if and only if \\(\\texttt{epi}(f)\\) is convex.\n\n\nProof. Suppose that \\(f\\) is convex. Let \\((x_1,\\alpha_1), (x_2,\\alpha_2)\\in\\texttt{epi}(f)\\). For \\(\\lambda\\in[0,1]\\) we have:\n\\[\\begin{aligned}\nf(\\lambda x_1 + (1-\\lambda)x_2) &\\leq \\lambda f(x_1) + (1-\\lambda) f(x_2) \\\\\n&\\leq \\underbrace{\\lambda \\alpha_1 + (1-\\lambda)\\alpha_2}_{\\alpha} \\\\\n&\\leq \\alpha\n\\end{aligned}\\]\nThen, \\(\\lambda(x_1,\\alpha_1) + (1-\\lambda)(x_2,\\alpha_2) = (\\lambda x_1 + (1-\\lambda)x_2, \\alpha) \\in \\texttt{epi}(f)\\) \\(\\Rightarrow\\) \\(\\texttt{epi}(f)\\) is convex.\nFor the other direction, suppose that \\(\\texttt{epi}(f)\\) is convex. Then, for all \\(x_1, x_2\\in V\\), \\(\\lambda\\in[0,1]\\) we have\n\\[\\lambda (x_1, f(x_1)) + (1-\\lambda)(x_2, f(x_2)) \\in \\texttt{epi}(f). \\]\nHence, by definition of epigraph,\n\\[f(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2),\\]\nwhich means that \\(f\\) is convex. ■\n\n\n\nExamples\n\n\n\n\\(f: x\\mapsto e^x\\)\n\\(f: x\\mapsto -\\log(x)\\), \\(x\\in\\mathbb{R}\\) positive\n\\(f: x\\mapsto \\langle a, Ax\\rangle + \\langle b,x\\rangle\\) where \\(A\\in\\mathcal{M}^n\\) is symmetric positive definite and \\(b\\) is a vector in \\(\\mathbb{R}^n\\).\n\nare convex functions.\n\n\n\nTheorem 7 (Jensen’s inequality) Let \\(f:C\\rightarrow\\mathbb{R}\\) be a convex function and \\(\\lambda_i \\in[0,1]\\), \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then,\n\\[ f(\\sum_{i=1}^n \\lambda_i x_i) \\leq \\sum_{i=1}^n \\lambda_i f(x_i), \\ x_i\\in C.\\]\n\n\nProof. Note that \\((x_i,f(x_i))\\in\\texttt{epi}(f)\\). Since \\(f\\) is convex, \\(\\texttt{epi}(f)\\) is a convex set. Thus\n\\[\\begin{aligned}\n& \\sum_{i=1}^n \\lambda_i (x_i,f(x_i)) \\in\\texttt{epi}(f) \\\\\n\\Rightarrow & \\left(\\sum_{i=1}^n \\lambda_i x_i, \\sum_{i=1}^n \\lambda_i f(x_i) \\right) \\in\\texttt{epi}(f)\\\\\n\\Rightarrow & f\\left(\\sum_{i=1}^n \\lambda_i x_i\\right) \\leq \\sum_{i=1}^n \\lambda_i f(x_i)\n\\end{aligned}\\] ■\n\nNote that we can easily conclude the arithmetic geometric mean inequality using this: take \\(\\ln(\\cdot)\\), which is a concave function (ie. the negative of a convex function),\n\\[\\begin{aligned}\n& \\ln\\left(\\frac{1}{2} x + \\frac{1}{2}y\\right) \\geq \\frac{1}{2}\\ln(x) + \\frac{1}{2}\\ln(y), \\ x,y&gt;0 \\\\\n\\Rightarrow & \\ln\\left(\\frac{1}{2} x + \\frac{1}{2}y\\right) \\geq \\ln((xy)^{1/2}) \\\\\n\\Rightarrow & \\frac{1}{2} (x + y) \\geq \\sqrt(xy)\n\\end{aligned}\\]\n\n3.2.1 Conditions of Convexity\n\nTheorem 8 (First order condition of convexity) Given \\(C\\subseteq\\mathbb{R}^n\\) convex, \\(f:C\\rightarrow\\mathbb{R}\\) continuously differentiable, we have that \\(f\\) is convex if and only if\n\\[ \\langle \\nabla f(x), y-x\\rangle \\leq f(y) - f(x), \\ \\ \\forall x,y\\in C.\\]\n\n\n\n\n\nProof. \\(\\Rightarrow\\). Assume that \\(f\\) is convex. Then for \\(\\alpha\\in(0,1)\\)\n\\[\\begin{aligned}\nf(x+ \\alpha(y-x)) &= f((1-\\alpha)x + \\alpha y) \\\\\n&\\leq (1-\\alpha) f(x) + \\alpha f(x).\n\\end{aligned}\\]\nHence,\n\\[ \\frac{f(x + \\alpha(y-x)) - f(x)}{\\alpha} \\leq f(y) - f(x).\\]\nTaking the limit \\(\\alpha\\rightarrow 0\\) results in\n\\[\\langle \\nabla f(x), y-x\\rangle \\leq f(y) - f(x), \\]\nwhich is the expression given in the theorem. Note, \\(\\langle \\nabla f(x), y-x\\rangle\\) is the directional derivative of \\(f\\) in the direction \\(y-x\\), as a result of the limit.\n\\(\\Leftarrow\\). Now suppose \\(\\langle \\nabla f(x), y-x\\rangle \\leq f(y) - f(x)\\), for all \\(x,y\\in C\\). Let \\(x_\\alpha = (1-\\alpha)x + \\alpha y\\) where \\(\\alpha\\in(0,1)\\). Consider\n\\[\\begin{aligned}\n& (1-\\alpha) & f(x) \\geq f(x_\\alpha) + \\langle \\nabla f(x_\\alpha), x- x_\\alpha\\rangle \\\\\n+ & & \\\\\n& \\alpha & f(y)\\geq f(x_\\alpha) + \\langle \\nabla f(x_\\alpha),y-x_\\alpha\\rangle .\n\\end{aligned}\\]\nExpanding and simplifying results in\n\\[ \\begin{aligned}\n&(1-\\alpha) f(x) + \\alpha f(y) \\geq f(x_\\alpha) + \\langle \\nabla f(x_\\alpha), \\underbrace{(1-\\alpha)x + \\alpha y - x_\\alpha}_{=0}\\rangle \\\\\n\\Rightarrow &(1-\\alpha) f(x) + \\alpha f(y) \\geq f(x_\\alpha) \\\\\n\\Rightarrow & (1-\\alpha) f(x) + \\alpha f(y) \\geq f((1-\\alpha)x + \\alpha y),\n\\end{aligned}\\]\nwhich holds for all \\(x,y\\in C\\), \\(\\alpha\\in\\mathbb{R}\\). Thus, \\(f\\) is convex. ■\n\nNote, a function can also be strictly convex. Strict convexity is when Definition 9 holds for \\(x\\neq y\\), ie.\n\\[ f((1-\\alpha)x + \\alpha y) &lt; (1-\\alpha) f(x) + \\alpha f(y).\\]\n\n\n\n\n\n\nTheorem 9 (Second order condition of convexity) Let \\(C\\subseteq\\mathbb{R}^n\\), and suppose \\(f:C\\rightarrow\\mathbb{R}\\) is twice continuously differentiable. Then, \\(f\\) is convex if and only if the Hessian of \\(f\\) at \\(x\\) is positive semidefinite for all \\(x\\in C\\), ie.\n\\[\\nabla^2 f(x) \\succeq 0 \\ \\forall x\\in C.\\]\nMoreover, if \\(\\nabla^2 f(x)\\) is strictly positive definite at every \\(x\\in C\\), then \\(f\\) is strictly convex. For concavity, the opposite holds.\n\n\nProof. \\(\\Rightarrow\\). Suppose \\(f\\) is convex, by Theorem 8\n\\[ f(x) + \\langle \\nabla f(x) ,\\alpha d\\rangle \\leq f(x + \\alpha d), \\]\nfor all \\(d\\in\\mathbb{R}^n\\), \\(\\alpha &gt;0\\). Fix \\(x\\) and consider the Taylor expansion of \\(f(x+ \\alpha d)\\) where \\(d\\) is the variable\n\\[\\begin{aligned}\n& f(x) + \\langle \\nabla f(x) ,\\alpha d\\rangle \\leq f(x) \\langle \\nabla f(x) + \\alpha d\\rangle + \\frac{\\alpha^2}{2} d^T \\nabla^2 f(x) d + \\mathcal{O}(\\alpha^2) \\\\\n\\Rightarrow & \\frac{1}{2} d^T \\nabla^2 f(x) d + \\frac{\\mathcal{O}(\\alpha^2)}{\\alpha^2}  \\geq 0.\n\\end{aligned}\\]\nTaking \\(\\alpha\\rightarrow 0\\), we conclude that\n\\[\\nabla^2 f(x) \\succeq 0.\\]\n\\(\\Leftarrow\\). Now, suppose \\(\\nabla^2 f(x) \\succeq 0\\). Then, by the mean value theorem, we know that there exists \\(z = x+\\lambda y\\) where \\(0\\leq \\lambda\\leq 1\\) such that\n\\[ f(y) = f(x) + \\langle \\nabla f(x), y-x\\rangle + \\frac{1}{2}\\langle \\nabla^2 f(z)(y-x), y-x\\rangle .\\]\nSince \\(\\nabla^2 f(z) \\succeq 0\\), we have\n\\[ f(y) \\geq f(x) + \\langle \\nabla f(x), y-x\\rangle, \\]\nwhich is the first order condition of convexity. Therefore, \\(f\\) is convex. ■\n\n\n\nExample\n\n\nA function that is strictly convex, but has \\(\\nabla^2 f(x) \\nsucc 0\\), is \\(f(x) = x^4\\). \\(\\nabla^2 f(x) = 12x^2 = 0\\) at \\(x=0\\), it is not strictly positive definite. Thus, the converse does not hold in the second part of Theorem 9.\n\n\n\n\nExample\n\n\nLet \\(f(x) =  \\frac{1}{2} x^T A x + \\langle b,x\\rangle\\), \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\) symmetric and \\(b\\in\\mathbb{R}^n\\). Then, \\(f\\) is convex if and only if \\(\\nabla^2 f(x) \\succeq 0\\), that is\n\\[\\nabla^2 f(x) = A \\succeq 0,\\]\nie. \\(f\\) is convex if and only if \\(A\\) is positive semidefinite.\n\n\n\nProposition 4 A function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if and only if \\(g:\\mathbb{R}\\rightarrow\\mathbb{R}\\) given by\n\\[ g(t) = f(x + yt)\\]\nis convex for all \\(x,y\\in\\mathbb{R}^n\\).\n\n\n\nExample\n\n\nLet \\(f:S^n_{&gt;0} \\rightarrow\\mathbb{R}\\), \\(f(X) = \\log\\det X\\).\nClaim: \\(f\\) is concave.\nLet \\(g(t) = \\log\\det(Z + tY)\\), \\(Z,Y\\in S^n_{&gt;0}\\).\n\\[\\begin{aligned}\ng(t) &= \\log\\det(Z + tY) \\\\\n&= \\log\\det(Z^{1/2}(I + tZ^{-1/2}YZ^{-1/2})Z^{1/2}) \\\\\n&= \\log\\det(Z) + \\log\\det(I + tZ^{-1/2}YZ^{-1/2}).\n\\end{aligned}\\]\nLet \\(\\lambda_1,...,\\lambda_n\\) be the eigenvalues of \\(Z^{-1/2}Y Z^{-1/2}\\). Then\n\\[\\begin{aligned}\n& g(t) = \\log\\det(Z) + \\sum_{i=1}^n \\log(1 + t\\lambda_i)\\\\\n& g'(t) = \\sum_{i=1}^n \\frac{\\lambda_i}{1 + t\\lambda_i} \\\\\n& g''(t) = \\sum_{i=1}^n \\frac{-\\lambda_{i}^2}{(1+t\\lambda_i)^2} \\leq 0\n\\end{aligned}\\]\nThus, \\(g\\) is concave.\n\n\nWe now note that convex functions have convex sublevel sets. This is, given \\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\) convex,\n\\[S_\\alpha = \\{x \\mid f(x) \\leq \\alpha \\}\\]\nis convex.\nBut what about the converse? Consider\n\n\n\n\n\nWe can see that the sublevel sets \\(S_\\alpha\\) are convex, but clearly \\(f\\) is not.\n\nDefinition 11 Any function that has convex sublevel sets is called quasi-convex. Similarly, quasi-concave functions have convex superlevel sets.\n\nNote that all convex functions are also quasi-convex.\n\nProposition 5 Let \\(f:C\\rightarrow\\mathbb{R}\\) be a convex function, where \\(C\\subseteq\\mathbb{R}^n\\) is convex. Then any local minimum of \\(f\\) is a global minimum.\n\n\nProof. Let \\(x^*\\) be a local minimimiser of \\(f\\), and let\n\\[ x_\\lambda = (1-\\lambda)x^* + \\lambda x,\\]\nwhere \\(x\\in C\\). Then \\(f(x^*)\\leq f(x_\\lambda)\\) for \\(\\lambda\\) small enough, so\n\\[ f(x^*) \\leq f((1-\\lambda)x^* + \\lambda x) \\leq (1-\\lambda)f(x^*) + \\lambda f(x)\\]\nThus, \\(f(x^*)\\leq f(x)\\) independantly of \\(\\lambda\\) and for all \\(x\\). Which confirms \\(x^*\\) is a global minimiser.\nIf additionally we assume strict convexity of \\(f\\), then the global minimiser, if it exists, is unique.\nSuppose otherwise: suppose \\(x_1^*\\) and \\(x_2^*\\) are both global minimisers, \\(x_1^*\\neq x_2^*\\). Then\n\\[f(\\lambda x_1^* + (1-\\lambda)x_2^*) &lt; \\lambda f(x_1^*) + (1-\\lambda)f(x_2^*). \\]\nSince \\(x_1^*\\) and \\(x_2^*\\) are both global minimisers, \\(f(x_1^*) = f(x_2^*) \\triangleq F^*\\). Then\n\\[f(\\lambda x_1^* + (1-\\lambda)x_2^*) &lt; F^*, \\]\nwhich is a contradiction. ■\n\n\nTheorem 10 (Variational Inequality) Let \\(f:C\\rightarrow\\mathbb{R}\\), \\(C\\subseteq\\mathbb{R}^n\\) is convex, and \\(f\\) is continuously differentiable. Suppose \\(x^*\\in C\\) is a minimiser of \\(f\\). Then\n\\[ \\langle \\nabla f(x^*) , x-x^*\\rangle \\geq 0  \\tag{3}\\]\nholds for all \\(x\\in C\\). Moreover, if \\(f\\) is convex, and \\(x^*\\) satisfies Equation 3, then \\(x^*\\) is a global minimiser of \\(f\\).\n\n\nProof. We have already seen the first part. For the second statement, suppose the variational inequality . Using the first order conditions of convexity\n\\[ f(x) \\geq f(x^*)+ \\underbrace{\\langle \\nabla f(x^*),x-x^*\\rangle}_{\\geq 0}\\]\nfor all \\(x\\in C\\), then \\(f(x) \\geq f(x^*)\\), and \\(x^*\\) is a global minimiser. ■\n\n\n\nExample\n\n\nConsider\n\\[\\begin{aligned}\n\\min & f(x) \\\\\n\\texttt{st} & Ax = b,\n\\end{aligned}\\]\nwhere \\(f\\) is convex and differentiable, \\(A\\in\\mathcal{M}_n(\\mathbb{R})\\), \\(b\\in\\mathbb{R}^n\\)\nLet \\(C = \\{x\\in\\mathbb{R}^n \\mid Ax = b\\}\\). This is a convex set. By the variational inequality\n\\[ \\langle \\nabla f(x^*), x - x^* \\rangle \\geq 0, \\ \\ \\forall x\\in C\\]\nfor \\(x^*\\) a solution to the optimisation problem. Since \\(Ax^* = b\\), we can rewrite this as\n\\[ \\langle \\nabla f(x^*), z\\rangle \\geq 0\\]\nfor all \\(z\\in\\ker(A)\\). Hence,\n\\[\\langle \\nabla f(x^*),z \\rangle = 0, \\ \\ z\\in\\ker(A).\\]\nRecall that \\(\\ker(A)^\\perp = \\textnormal{Im}(A^T)\\), thus \\(\\nabla f(x^*)\\in \\textnormal{Im}(A^T)\\) OR \\(P_{\\ker(A)} \\nabla f(x^*) = 0\\) (project \\(f\\) into the kernel).\n\n\nRecall the projection mapping. Consider a set \\(C\\in\\mathbb{R}^n\\) non-empty, closed, and convex. The projection map \\(P_C\\) takes a point \\(x\\) and finds a point in \\(C\\) that is closest (in Euclidean distance) to \\(x\\).\n\n\n\n\n\n\nTheorem 11 Take \\(C\\) as above. We have the following, where \\(z\\in C\\) and \\(x\\in\\mathbb{R}^n\\),\n\\[ \\langle x- P_C(x), z - P_c(x)\\rangle \\leq 0\\]\n\n\n\n\n\nProof. Note that \\(x^* = P_C(x)\\) is a solution to\n\\[\\min_{z\\in C} \\frac{1}{2} ||z-x||^2 .\\]\nThis function is coercive \\(\\Rightarrow\\) \\(x^*\\) exists. We can use Theorem 10. For all \\(z\\in C\\)\n\\[\\begin{aligned}\n&\\langle x^*-x,z-x^*\\rangle\\geq 0 \\\\\n\\Rightarrow & \\langle x-x^*,z-x^*\\rangle \\leq 0 \\\\\n\\Rightarrow & \\langle x-P_C(x), z-P_C(x)\\rangle \\leq 0.\n\\end{aligned}\\] ■\n\nThis result holds true for Hilbert spaces. To show the existence of a minimiser we take a minimising sequence \\(\\{x_n\\}_{n=1}^\\infty\\),\n\\[||x-x_n|| \\rightarrow d\\in\\inf\\{||x-z||, z\\in C\\}.\\]\nNote that Hilbert spaces are complete, and hence if we show that this sequence is Cauchy then it is convergent.\n\\[\\begin{aligned}\n||x_n-x_m||^2 &= ||(x-x_n) - (x-x_m)||^2 \\\\\n&= 2(||x-x_n||^2 + ||x-x_m||^2) - ||(x-x_n) + (x-x_m)||^2  \\\\\n&= 2(||x-x_n||^2 + ||x-x_m||^2) - 4||x - \\frac{x_n+x_m}{2}||^2 \\\\\n&\\leq 2(||x-x_n|^2 + ||x-x_m||^2) - 4d^2.\n\\end{aligned}\\] By the parallelogram law.\nFor all \\(\\varepsilon &gt;0\\), there exists \\(N\\) large enough such that \\(n,m&gt;N\\) then\n\\[2(\\underbrace{||x-x_n||^2}_{\\rightarrow d^2} + \\underbrace{||x-x_M||^2}_{\\rightarrow d^2}) - 4d^2 \\rightarrow 0.\\]\nSo, \\(||x_n - x_m||^2 \\leq 0\\). Therefore this is a Cauchy sequence, and the rest follows.\n\nProposition 6 Let \\(C\\subset\\mathbb{R}\\) be non-empty, closed, and convex. Then the projection map \\(P_C:\\mathbb{R}^n\\rightarrow C\\) is non-expansive.\ni.e. \\(||P_C(y) - P_C(x)|| \\leq ||y-x||\\) for all \\(x,y\\in\\mathbb{R}^n\\). Consequently, \\(P_C\\) is continuous.\n\n\nProof. Recall that\n\\[\\begin{aligned}\n& \\langle x-P_C(x), P_C(y) - P_C(x)\\rangle \\leq 0\\\\\n-&\\underline{\\langle y-P_c(y),P_C(x) - P_C(y)\\rangle \\leq 0} \\\\\n& \\langle x-y + P_C(y)-P_C(x), P_C(y) - P_C(x)\\rangle \\leq 0 \\\\\n\\Rightarrow & \\langle x-y,P_C(y)-P_C(x)\\rangle + ||P_C(y) - P_C(x)||^2 \\leq 0 \\\\\n\\Rightarrow & ||P_C(y) - P_C(x)||^2 \\leq \\langle y-x,P_C(y)-P_C(x)\\rangle \\\\\n\\Rightarrow & ||P_C(y) - P_C(x)||^2 \\leq ||y-x|| \\ ||P_C(y) - P_C(x)|| .\n\\end{aligned}\\]\nWhere the last line follows from Cauchy-Schwarz. ■"
  },
  {
    "objectID": "posts/opt-theory/index.html#separation-of-convex-sets",
    "href": "posts/opt-theory/index.html#separation-of-convex-sets",
    "title": "Optimisation Theory and Applications",
    "section": "3.3 Separation of Convex Sets",
    "text": "3.3 Separation of Convex Sets\nRecall that we denote by \\(H_{(a,\\alpha)}\\) the hyperplane (\\(a\\in\\mathbb{R}n\\), $)\n\\[ H_{(a,\\alpha)} = \\{x\\in\\mathbb{R}^n \\mid \\langle a,x\\rangle = \\alpha \\} .\\]\nWe also use\n\\[ H_{(a,\\alpha)}^+ = \\{x\\in\\mathbb{R}^n \\mid \\langle a,x\\rangle &gt; \\alpha \\}, \\]\nand\n\\[H_{(a,\\alpha)}^- = \\{x\\in\\mathbb{R}^n \\mid \\langle a,x\\rangle &lt; \\alpha \\}.\\]\n\nDefinition 12 Let \\(C_1\\) and \\(C_2\\) be two non-empty sets in \\(\\mathbb{R}^n\\)$. Then a hyperplane \\(H_{(a,\\alpha)}\\) is called:\n\nSeparating hyperplane for \\(C_1\\) and \\(C_2\\) if \\(C_1\\) is contained in \\(\\bar{H}^+\\) (\\(H^+\\) closure) and \\(C_2\\) in \\(\\bar{H}^-\\).\n\n\n\n\n\n\n\nStrictly separating hyperplane for \\(C_1\\) and \\(C_2\\) is \\(C_1\\) is contained in \\(H^+\\) and \\(C_2\\) in \\(H^-\\).\n\n\n\n\n\n\n\nStrongly separating for \\(C_1\\) and \\(C_2\\) if there exists \\(\\alpha_1,\\alpha_2\\in\\mathbb{R}\\) with \\(\\alpha_1 &gt; \\alpha &gt; \\alpha_2\\) such that \\(C_1 \\subset \\bar{H}^+_{(a,\\alpha_1)}\\) and \\(C_2\\subset \\bar{H}^-_{(a,\\alpha_2)}\\).\n\n\n\n\n\n\n\nProperly separating for \\(C_1\\) and \\(C_2\\) if \\(H\\) is a separating hyperplane and not both \\(C_1\\) and \\(C_2\\) are contained in \\(H\\).\nSupporting hyperplane of \\(C_1\\) at point \\(\\bar{x}\\in\\bar{C}_1\\) if \\(\\bar{x} \\in H\\) and \\(H\\) separates \\(\\{\\bar{x}\\}\\) and \\(C_1\\).\n\n\n\n\n\n\n\nRecall that for a set \\(S\\subseteq\\mathbb{R}^n\\), the relative interior of \\(S\\) is\n\\[\\textnormal{ri}(S) = \\{x\\in S\\mid \\exists \\varepsilon &gt; 0, B_\\varepsilon(x) \\cap \\textnormal{aff}(S) \\subset S\\},\\]\nwhere \\(\\textnormal{aff}(S)\\) is the affine hull of \\(S\\).\n\nTheorem 12 Let \\(C\\in\\mathbb{R}^n\\) be a non-empty, convex set, and let \\(\\bar{x}\\in\\bar{C}\\setminus \\textnormal{ri}(C)\\). Then, there exists a supporting hyperplane to \\(C\\) at \\(\\bar{x}\\), i.e. there exists a vector \\(a\\in\\mathbb{R}^n\\) such that\n\n\n\n\n\nThe supporting hyperplanes are in green.\n\n\n\\[\\langle a,x\\rangle \\geq \\langle a,\\bar{x}\\rangle\\]\nfor all \\(x\\in C\\).\n\n\nProof. Consider a sequence \\(\\{x_k\\}_{k=1}^\\infty\\) where \\(x_k\\in C\\) approaching \\(\\bar{x}\\).\n\n\n\n\n\nThis sequence gives \\(\\{\\hat{x}_k\\}_{k=1}^\\infty\\) where \\(\\hat{x}_k = P_{\\bar{C}}(x_k)\\). Note that\n\\[ \\langle \\hat{x}_k - x_k, x - \\hat{x}_k\\rangle \\geq 0 \\ \\ \\forall x\\in\\bar{C}. \\tag{4}\\]\nWe have that\n\\[ \\begin{aligned}\n\\langle \\hat{x}_k - x_k, x\\rangle & \\geq \\langle \\hat{x}_k - x_k, \\hat{x}_k\\rangle \\\\\n& = \\langle \\hat{x}_k - x_k, \\hat{x}_k - x_k\\rangle + \\langle \\hat{x}_k - x_k, x_k\\rangle \\\\\n& \\geq \\langle \\hat{x}_k - x_k, x_k \\rangle .\n\\end{aligned}\\]\nLet \\(\\alpha_k = \\frac{\\hat{x}_k - x_k}{|| \\hat{x}_k - x_k||}\\), then \\(\\langle \\alpha_k,x\\rangle \\geq \\langle \\alpha_k, x_k\\rangle\\). The sequence \\(\\{\\alpha_k\\}_{k=1}^\\infty\\) is bounded and has a convergent subsequence. Then, taking the limit,\n\\[\\langle \\alpha, x\\rangle \\geq \\langle \\alpha, \\bar{x}\\rangle, \\]\nwhich proves the result. ■\n\nNote that the sequence \\(\\{P_{\\bar{C}(x_k)\\}_{k=1}^\\infty\\) is convergent to \\(P_{\\bar{C}(\\bar{x})} = \\bar{x}\\) since \\(P_{\\bar{C}}\\) is continuous. We could have taken the limit directly from Equation 4.\n\nTheorem 13 Let \\(C_1\\) and \\(C_2\\) be two disjoint, non-empty, convex subsets of \\(\\mathbb{R}^n\\). Then, there exists a hyperplane that separates \\(C_1\\) and \\(C_2\\). I.e. there exists a vector \\(a\\in\\mathbb{R}^n\\) such that\n\\[ \\langle a,x_2\\rangle \\geq \\langle a,x_1\\rangle \\ \\ \\forall x_1\\in C_1, \\ x_2\\in C_2.\\]\n\n\nProof. Consider the convex set\n\\[ C = C_2 - C_1 = \\{x = x_2 - x_1 \\mid x_1\\in C_1, \\ x_2\\in C_2\\}. \\]\nThis set is convex. Since \\(C_1 \\cap C_2 = \\emptyset\\), we conclude that \\(0\\notin C\\). Now, by an extension of Theorem 12, we conclude that there exists a vector \\(a\\neq 0\\) passing through 0 such that\n\\[\\langle a,x\\rangle \\geq \\langle a,0\\rangle = 0, \\ \\forall x\\in C\\]\ni.e. \\(\\langle a,x_2\\rangle \\geq \\langle a,x_1\\rangle\\) for all \\(x_1\\in C_1\\) and \\(x_2\\in C_2\\). ■\n\n\nTheorem 14 Let \\(C_1\\) and \\(C_2\\) be two non-empty, disjoint, convex sets in \\(\\mathbb{R}^n\\). Suppose that the set \\(C_2-C_1\\) is closed. Then, there exists a hyperplane that separates \\(C_1\\) and \\(C_2\\) strongly.\n\n\nProof. Since \\(C_1\\) and \\(C_2\\) are disjoint, 0 is not in \\(C_2-C_1\\).\n\n\n\n\n\nLet then \\(\\bar{x}_2-\\bar{x}_1\\) be the vector of minimum norm over \\(C\\) which is well defined.\nLet \\(a=\\frac{\\bar{x}_2-\\bar{x}_1}{2}\\) and \\(\\bar{x} = \\frac{\\bar{x}_2+\\bar{x}_1}{2}\\). Note that \\(a\\neq 0\\).\nWe claim that the hyperplane \\(H_{(a,\\alpha)}\\) given by\n\\[ H_{(a,\\alpha)} = \\{x\\in\\mathbb{R}^n \\mid \\langle a,x\\rangle = \\alpha \\triangleq \\langle a, \\bar{x}\\rangle \\}\\]\nstrongly separates \\(C_1\\) and \\(C_2\\). Finish the proof on your own, show \\(\\langle a,x_1\\rangle &lt; \\beta &lt; \\langle a, x_2\\rangle\\) (similar to the previous proof). ■\n\n\nTheorem 15 Two non-empty convex sets \\(C_1\\), \\(C_2\\) can be properly separated if \\(\\textnormal{ri}(C_1)\\) and \\(\\textnormal{ri}(C_2)\\) are disjoint.\n\n\nProposition 7 Let \\(C\\subset\\mathbb{R}^n\\) be non-empty and convex, and let \\(A\\) be an affine set such that \\(\\textnormal{ri}(C) \\cap A = \\emptyset\\). Then, there exists a hyperplane \\(H\\) separating \\(A\\) and \\(C\\) such that \\(A\\subseteq H\\).\n\n\nProof. Existence of a hyperplane \\(H\\) is clear. If \\(H\\) includes \\(A\\), we are done. Otherwise, there exists \\(x\\in A\\backslash H\\).\nThe affine set \\(A\\) and \\(H\\) must be disjoint, otherwise \\(y\\in A\\cap H\\) and the line between \\(x\\) and \\(y\\) intersects both \\(\\bar{H}^+\\) and \\(\\bar{H}^-\\), which is not possible since, since \\(H\\) is separating. Thus, \\(A\\) has to be “parallel” to \\(H\\). Hence, we can shift the hyperplane \\(H\\) to a new one \\(\\tilde{H}\\) that includes \\(A\\). ■"
  },
  {
    "objectID": "posts/opt-theory/index.html#conjugate-functions",
    "href": "posts/opt-theory/index.html#conjugate-functions",
    "title": "Optimisation Theory and Applications",
    "section": "4.1 Conjugate Functions",
    "text": "4.1 Conjugate Functions\n\nDefinition 13 Let \\(C\\) be a convex non-empty set in \\(\\mathbb{R}^n\\) and let \\(f\\) be a function on \\(C\\). The conjugate set \\(C^*\\)of \\(C\\) on \\(\\mathbb{R}^n\\) is defined as\n\\[ C^* = \\{x^*\\in\\mathbb{R}^n \\mid \\sup_{x\\in C} (\\langle x, x^*\\rangle - f(x)) &lt; \\infty\\} \\]\nThe conjugate function \\(f^*\\) of \\(f\\) on \\(C^*\\) is defined as\n\\[ f^*(x^*) = \\sup_{x\\in C} (\\langle x, x^*\\rangle - f(x)).\\]\n\nNote that when \\(f\\) is differentiable the critical point of \\(\\langle x, x^*\\rangle - f(x)\\) happens at \\(x^* = f'(x)\\).\n\n\n\n\n\nFor example, in \\(\\mathbb{R}^2\\) this is: \\(x^*x - f(x)\\), now fix \\(x^*\\)\n\\[\\sup_{x\\in\\mathbb{R}^2} (x^*x - f(x)) \\Rightarrow x^* = f'(x).\\]\nThe slope of the upper purple line is parallel to the slope of \\(x^*x\\). I.e. \\(x\\) is such that the slope of the tangent to the curve is exactly \\(x^*\\).\nWe also note that the set conjugate set \\(C^*\\) and the conjugate function \\(f^*\\) are both convex. Moreover, \\(\\texttt{epi}(f^*)\\) is a closed convex set.\n\n\nExamples\n\n\n\nConsider \\(f:\\mathbb{R}\\rightarrow\\mathbb{R}\\) such that \\(x\\mapsto f(x) = ax + b\\), \\(a,b\\in\\mathbb{R}\\).\n\n\\[ f^*(x^*) = \\sup_{x\\in\\mathbb{R}} (x^*x - (ax + b)) = \\sup_{x\\in\\mathbb{R}} (x(x^*-a) - b) \\]\nIf \\(x^*\\neq a\\), then the supremum is unbounded, and \\(x^*\\notin C^*\\). When \\(x^* = a\\), then \\(f^*(x) = -b\\).\n\nConsider \\(f:\\mathbb{R}\\rightarrow\\mathbb{R}\\) such that \\(x\\mapsto (e^x)\\). Then\n\n\\[ f^*(x^*) = \\sup_{x\\in\\mathbb{R}} (x^*x-e^x)\\]\nWhen \\(x^* &lt;0\\), this is unbounded. When \\(x^*&gt;0\\), the maximum is reached at \\(x = \\log(x^*)\\) and hence\n\\[f^*(x^*) = x^*\\log(x^*) - x^*.\\]\nWhen \\(x^*=0\\), \\(f^*(x^*) = 0\\).\n\n\nThe notion of conjugate functions in the context of optimsation is due to Fenchal (1953), however, this is a generalisation of Legendre transforms (1787)\n\\[f^*(p) = \\sup_{x\\in S} (p^Tx - f(x)).\\]\n\n\nNote that \\(f(x) + f^*(y) \\geq x^T y\\), this is called the Fenchel-Young inequality.\n\n\nExample\n\n\nLet \\(f:\\mathbb{R}\\rightarrow\\mathbb{R}\\) such that \\(a\\mapsto \\frac{1}{2} a^2\\), then \\(f^*(b) = \\sup (ab - \\frac{a^2}{2}) = \\frac{b^2}{2}\\). We know from Young’s inequality\n\\[ \\frac{1}{2}(a^2 + b^2) \\geq ab,\\]\nbut this is the Fenchel-Young inequality using the funciton \\(f\\) defined above!"
  },
  {
    "objectID": "posts/opt-theory/index.html#duality-theory",
    "href": "posts/opt-theory/index.html#duality-theory",
    "title": "Optimisation Theory and Applications",
    "section": "4.2 Duality Theory",
    "text": "4.2 Duality Theory\n\nTheorem 17 (Fenchel Duality) Let \\(f\\) be a convex function over \\(C\\), \\(g\\) be a concave function of \\(D\\). Both \\(C\\) and \\(D\\) are convex sets in \\(X = \\mathbb{R}^n\\). Suppose \\(C\\cap D\\) has a non-empty relative interior and \\(\\texttt{epi}_f(C)\\) and \\(\\texttt{epi}_g(D)\\) have non-empty interiors. Consider\n\\[ \\inf_{x\\in C\\cap D} \\ (f(x) - g(x))\\]\nand suppose that this problem has a finite solution \\(\\mu\\in\\mathbb{R}\\), its value achieved at some \\(x_0\\in C\\cap D\\). Then,\n\\[ \\mu = \\inf_{x\\in C\\cap D}  \\ (f(x) - g(x)) = \\sup_{x^*\\in C^*\\cap D^*} \\ (g^*(x^*) - f^*(y^*))\\]\nand the supremum on the right hand side is achieved at some \\(x_0^*\\in C^*\\cap D^*\\). Moreover\n\\[\\max_{x\\in C} (\\langle x, x_0^*\\rangle - f(x)) = \\langle x_0, x_0^*\\rangle - f(x_0)\\]\nand\n\\[\\min_{x\\in D} (\\langle x, x_0^*\\rangle - g(x)) = \\langle x_0,x_0^*\\rangle - g(x_0).\\]\n\n\nProof. We first show that\n\\[\\mu = \\inf_{x\\in C\\cap D} (f(x) - g(x)) \\geq \\sup_{x^*\\in C^*\\cap D^*}(g^*(x^*)-f^*(x^*)). \\tag{6}\\]\nBy the Fenchel-Young inequality\n\\[f^*(x^*) \\geq \\langle x,x^*\\rangle - f(x),\\]\nand\n\\[f^*(x^*) \\geq \\langle x,x^*\\rangle g(x),\\]\nfor all \\(x,x^*\\). Thus, \\(f(x) - g(x) \\geq g^*(x^*) - f^*(x^*)\\). This proves the claim.\nIt is enough to show that there exists some point \\(x_0^* \\in C^*\\cap D^*\\) for with the inequality in Equation 6 is achieved.\n\n\n\n\n\nWe want to separate the epigrpahs. The natural candidates for separation are\n\\[\\begin{aligned}\n\\texttt{epi}_{f-\\mu}(C) = \\{(x,\\alpha) \\mid x\\in C, f(x) - \\mu\\leq \\alpha\\}, \\\\\n\\texttt{epi}_{g}(D) = \\{(x,\\alpha) \\mid x\\in D, g(x) \\geq \\alpha\\},\n\\end{aligned}\\]\nnoting that for concave functions, the epigraph is written as the infimum. After verifying tha tthe separation theorem can be applied, we get that there exists a hyperplane that separates \\(\\texttt{epi}_g(D)\\) and \\(\\texttt{epi}_{f-\\mu}(C)\\), denoted by\n\\[ H= \\{(r,x) \\in\\mathbb{R}\\times X \\mid \\langle x, x_0^*\\rangle -r = c\\},\\]\nwhere \\(x_0^*\\in\\mathbb{R}^n\\) and \\(c\\in\\mathbb{R}\\), without loss of generality. Note that we are saying that this hyperplane is not “vertical”, since\n\\[\\langle x, x_0^*\\rangle - r = (r,x)\\cdot(-1,x_0^*).\\]\nThe \\(-1\\) can’t be zero, since if it was it would separate \\(C\\) and \\(D\\) and we assumed they can’t be separated (since their intesection has non-empty relative interior).\nNow that we have this hyperplane, we note that \\(H\\) is above \\(\\texttt{epi}_g(D)\\) and arbitrarily close to it. I.e.\n\\[c = \\inf_{x\\in D} (\\langle x,x_0^*\\rangle - g(x)) = g^*(x^*), \\tag{7}\\]\nsimilarly, \\(H\\) is below \\(\\texttt{epi}_{f-\\mu}(C)\\) and arbitrarily close to it\n\\[c = \\sup_{x\\in C} (\\langle x,x_0^*\\rangle - f(x) + \\mu) = f^*(x^*) + \\mu. \\tag{8}\\]\nThus, with Equation 7 and Equation 8 we have \\(g^*(x_0^*) = f^*(x_0^*) + \\mu\\), i.e. the inequality Equation 6 is achieved! ■\n\n\n\nExample: Resource allocation\n\n\nSuppose a total of \\(R\\in\\mathbb{R}_{&gt;0}\\) units of resources are supposed to be allocated to \\(n\\) locations.\n\n\n\n\n\nThe payoff of allocating \\(x_i\\) units of resources to location \\(i\\) is \\(g_i(x_i)\\), \\(g_i:\\mathbb{R}\\rightarrow\\mathbb{R}\\), and suppose that \\(g_i\\) is increasing and concave.\nWe want to maximise \\(\\sum_{i=1}^n g_i(x)\\) subject to \\(\\sum_{i=1}^n x_i = R\\) (max number of resources) and \\(x_i\\geq 0\\).\nLet us take: - \\(D\\) to be the positive orthant, \\(D = \\{x \\mid x_i\\geq 0, i\\in\\{1,...,n\\}\\}\\) - \\(C = \\{x \\mid \\sum_{i=1}^n x_i = R\\}\\) - \\(f\\) to be zero (since we are going to be using Fenchel Duality and need a convex and a concave function)\nNote that \\(f^*(y) = \\sup_{x\\in C} y^T x\\). When is \\(f^*\\) well defined? Suppose \\(y = \\alpha\\mathbb{1}_n\\) (where \\(\\mathbb{1}_n\\) is the vector of ones), then\n\\[ f^*(y) = \\sup_{x\\in C} \\ \\alpha \\mathbb{1}_n^T x = \\alpha R.\\]\nIf \\(y\\neq \\alpha\\mathbb{1}_n\\) for any \\(\\alpha\\in\\mathbb{R}\\), then \\(f^*\\) is not well defined (i.e. infinity, check this on your own). Then\n\\[C^* = \\{y \\mid y = \\alpha\\mathbb{1}_n, \\alpha\\in\\mathbb{R}\\}.\\]\nLet \\(g_i^*(y_i) = \\inf_{x_i\\geq 0} (x_i y_i - g_i(x_i))\\). Then the dual problem is given by\n\\[\\min_{\\alpha\\in\\mathbb{R}} (\\alpha R - \\sum_{i=1}^n g_i^*(\\alpha))\\]\n\n\nConsider two convex sets \\(X\\) and \\(Y\\) in \\(\\mathbb{R}^n\\). Suppose that there are two players \\(A\\) and \\(B\\), \\(A\\) choosing \\(x\\in X\\) and \\(B\\) choosing \\(y\\in Y\\), with the goal of, respectively, minimising \\(\\langle x,y\\rangle\\) and maximising \\(\\langle x,y\\rangle\\) over \\(x\\) and \\(y\\).\nLet now the strategy of one player be \\(\\min_{x\\in X} \\max_{y\\in Y} \\langle x,y\\rangle\\), and the strategy of the other player to be \\(\\max_{y\\in Y}\\min_{x\\in X} \\langle x,y\\rangle\\). Do the players end up with the same result?\n\nTheorem 18 (Minimax) Let \\(X\\) and \\(Y\\) be two compact convex sets in \\(\\mathbb{R}^n\\). Then,\n\\[\\min_{x\\in X} \\max_{y\\in Y} \\langle x,y\\rangle = \\max_{y\\in Y}\\min_{x\\in X} \\langle x,y\\rangle.\\]\n\n\nProof. Using Fenchel Duality (Theorem 17) let \\(C = \\mathbb{R}^n\\). Define \\(f(x) = \\max_{y\\in Y} \\langle x,y\\rangle\\), \\(x\\in C\\). By compactness of \\(Y\\), \\(f\\) is continuous and clearly convex on \\(C\\). Consider now the problem\n\\[\\min_{x\\in X} \\ f(x)\\]\nwhich has a solution by the Weierstrass theorem (Theorem 1). Let us now take \\(g\\) to be the zero function on \\(D=X\\). Note that\n\\[g^*(x^*) = \\min_{x\\in X} \\langle x,x^*\\rangle.\\]\nIt is well-defined and hence \\(D^* = \\mathbb{R}^n\\) (since \\(X\\) is compact). It is left to compute \\(f^*\\) on \\(C^*\\)\n\\[\\begin{aligned}\nf^*(x^*) &= \\sup_{x\\in C} (\\langle x,x^*\\rangle - f(x)) \\\\\n&= \\sup_{x\\in C} (\\langle x,x^*\\rangle - \\max_{y\\in Y}\\langle x,y\\rangle), \\ \\ x^*\\in Y\n\\end{aligned}\\]\nNote that \\(X\\subset C\\) if \\(x^*\\in Y\\), then \\(f^*(x^*) = 0\\) (by picking \\(x\\) such that \\(\\max_{y\\in Y} \\langle x,y\\rangle\\) is at \\(x^*\\)). Also, we can show that, if \\(x^*\\notin Y\\), then\n\\[\\sup_{x\\in C} (\\langle x,x^*\\rangle - \\max_{y\\in Y}\\langle x,y\\rangle) = +\\infty.\\]\nAs a result, \\(C^* = Y\\).\nNow, using Fenchel duality\n\\[\\begin{aligned}\n& \\min_{x\\in C\\cap D} (f(x) - 0) = \\max_{y\\in C^*\\cap D^*} (g^*(y) - 0) \\\\\n\\Rightarrow & \\min_{x\\in X} f(x) = \\max_{y\\in Y} g^*(y) \\\\\n\\Rightarrow & \\min_{x\\in X} \\max_{y\\in Y} \\langle x,y\\rangle = \\max_{y\\in Y} \\min_{x\\in X} \\langle x,y\\rangle\n\\end{aligned}\\] ■\n\n\nDefinition 14 (Saddle points) A point \\(x^*,z^*\\in X\\times Z\\), where \\(X\\subseteq \\mathbb{R}^n\\) and \\(Z\\subseteq\\mathbb{R}^m\\), is called a local min-max saddle point of a continuously differentiable function \\(F\\) if there exists open neighbourhoods \\(U_x^* \\subseteq\\mathbb{R}^n\\) and \\(U_z^* \\subseteq \\mathbb{R}^m\\) such that\n\\[F^*(x^*,z) \\leq F(x^*,z^*) \\leq F(x,z^*).\\]\nThis point is called a global min-max saddle point if \\(U_x^* = X\\) and \\(U_z^* = Z\\).\n\n\nTheorem 19 (Weak duality) Let \\(F:X\\times Z\\rightarrow\\mathbb{R}\\) be as before. Then,\n\\[\\sup_{z\\in Z}\\inf_{x\\in X} F(x,z) \\leq \\inf_{x\\in X}\\sup_{z\\in Z} F(x,z).\\]\n\n\nProof. We know \\(\\inf_{x\\in X} F(x,z) \\leq F(\\tilde{x},z)\\) for all \\(z\\in Z\\), \\(\\tilde{x}\\in X\\). In particular, we have\n\\[\\sup_{z\\in Z} \\inf_{x\\in X} F(x,z) \\leq \\sup_{z\\in Z} F(\\tilde{x},z) \\ \\forall \\tilde{x}\\in X.\\]\nAs a result,\n\\[\\sup_{z\\in Z}\\inf_{x\\in X} F(x,z) \\leq \\inf_{x\\in X}\\sup_{z\\in Z} F(x,z).\\] ■\n\nNow we ask the question, when is this inequality an equality? We now study this question.\nDefine\n\\[P_v \\triangleq \\inf_{x\\in X} \\sup_{z\\in Z} F(x,z)\\]\nto be the primal problem and \\(P_v\\) the primal value, and\n\\[D_v\\triangleq \\sup_{z\\in Z}\\inf_{x\\in X} F(x,z)\\]\nto be the dual problem with \\(D_v\\) the dual value. So far we know that \\(P_v \\geq D_v\\). Let us define the duality gap to be the difference between the primal and dual, \\(P_v - D_v\\).\n\nTheorem 20 Let \\(F:X\\times Z\\rightarrow\\mathbb{R}\\) be as before. Then the following two conditions are equivalent.\n\n\\((x^*,z^*)\\in X\\times Z\\) is a saddle point of \\(F\\). (Note: if it isnot specified as local, then saddle points are assumed to be global.)\n\\(x^*\\) solves the primal problem, \\(z^*\\) solves the dual problem, and the duality gap is zero, i.e.\n\n\\[\\min_{x\\in X}\\max_{z\\in Z} F(x,z) = \\max_{z\\in Z}\\min_{x\\in X} F(x,z).\\]\nAnd, whenever any of these two conditions are satisfied, \\(P_v = D_v = F(x^*,z^*)\\).\n\n\nProof. Suppose that 1. holds. Then, by definition we have\n\\[F(x^*,z) \\leq F(x^*,z^*) \\leq F(x,z^*).\\]\nHence, \\(\\sup_{z\\in Z} F(x^*,z) = \\max_{z\\in Z} F(x^*,z) = F(x^*,z^*)\\), and \\(\\inf_{x\\in X} F(x,z^*) = \\min_{x,z^*} F(x,z^*) = F(x^*,z^*)\\). And as a result,\n\\[\\begin{aligned}\n\\inf_{x\\in X}\\sup_{z\\in Z} F(x,z) \\leq \\sup_{z\\in Z} F(x^*,z) &= F(x^*,z^*) \\\\\n&= \\inf_{x\\in X} F(x,z^*) \\\\\n&\\leq \\sup_{z\\in Z} \\inf_{x\\in X} F(x,z).\n\\end{aligned}\\]\nThen, using Weak duality (Theorem 19), we have \\(\\min_{x\\in X}\\sup_{z\\in Z} F(x,z) = \\max_{z\\in Z}\\inf_{x\\in X} F(x,z) = F(x^*,z^*)\\) and the duality gap is 0.\nTo prove the other side, suppose that 2. holds. Then, we have that the primal and dual problems have the same solution\n\\[\\inf_{x\\in X} F(x,z^*) = \\sup_{z\\in Z} F(x^*,z).\\]\nThis implies that \\(F(x^*,z^*) = \\sup_{\\tilde{z}\\in Z} F(x^*,\\tilde{z}) \\geq F(x^*,z)\\) for all \\(z\\in Z\\). Similarly, \\(F(x^*,z^*) = \\inf_{\\tilde{x}\\in X} F(\\tilde{x},z^*) \\leq F(x,z^*)\\) for all \\(x\\in X\\). Using these, we have\n\\[F(x^*,z) \\leq F(x^*,z^*) \\leq F(x,z^*),\\]\nwhich implies that \\((x^*,z^*)\\) is a saddle point. ■"
  },
  {
    "objectID": "posts/opt-theory/index.html#nonlinear-programming",
    "href": "posts/opt-theory/index.html#nonlinear-programming",
    "title": "Optimisation Theory and Applications",
    "section": "4.3 Nonlinear Programming",
    "text": "4.3 Nonlinear Programming\nConsider the problem of minimising a function subject to both equality and inequality constraints\n\\[\\begin{aligned}\n\\min_x & \\ f(x) \\\\\n\\textnormal{such that} & \\ h_i(x) = 0, i\\in\\{1,...,m\\},\\\\\n& \\ g_j(x) \\leq 0, j\\in\\{1,...,r\\}.\n\\end{aligned} \\tag{9}\\]\nNote, the functions \\(f\\), \\(h_i\\), and \\(g_j\\) are “nice”, map \\(\\mathbb{R^n}\\rightarrow\\mathbb{R}\\), and continuously differentiable.\n\nDefinition 15 (Regularity) A feasible point \\(x\\) is said to be regular if the equality constraint gradients, \\(\\nabla h_i(x)\\), \\(i\\in\\{1,...,m\\}\\) and the inequality constraint gradients, \\(\\nabla g_j(x)\\), for \\(j\\) in the active set\n\\[A(x) = \\{j \\mid g_j(x) = 0\\},\\]\nare linearly independent.\n\n\nTheorem 21 (Karush-Kuhn-Tucker (KKT) necessary conditions of optimality) Suppose that \\(x^*\\) is a solution to the nonlinear programming problem Equation 9. Suppose that \\(X^*\\) is regular. Then, there exists unique vectors \\(\\lambda^* = (\\lambda_1^*,...,\\lambda_m^*)\\) and \\(\\mu^* = (\\mu_1^*,...,\\mu_r^*)\\) such that for\n\\[L(x,\\lambda,\\mu) = f(x) + \\sum_{i=1}^m \\lambda_i h_i(x) + \\sum_{j=1}^r \\mu_j g_j(x),\\]\nwe have\n\\[\\begin{aligned}\n& \\nabla L(x^*,\\lambda^*,\\mu^*) = 0 \\\\\n& \\mu_j^* \\geq 0 \\textnormal{ for any } j\\in\\{1,...,r\\} \\\\\n& \\mu_j^* = 0, j\\notin A(x^*).\n\\end{aligned}\\]\n\n\nProof. We have shown this result for the case of equality constraints. Hence, all we need to show here is that \\(\\mu_j^*\\geq 0\\) for all \\(j\\in A(x^*)\\).\nLet \\(g_j^+(x) = \\max\\{0,g_j(x)\\}\\), \\(j\\in\\{1,...,r\\}\\). Note tht this function is continuously differentiable. Let us now define a sequence of functions \\(\\{\\tilde{F}^k\\}_{k=1}^\\infty\\)\n\\[ \\tilde{F}^k(x) = f(x) + \\frac{k}{2}||h(x)||^2 + \\frac{k}{2}\\sum_{j=1}^r (g_j^+(x))^{-2} +\\frac{\\alpha}{2}||x-x^*||^2,\\]\nwhere \\(x\\in\\mathbb{S} = \\{x\\mid ||x-x^*||\\leq \\varepsilon\\}\\), \\(\\alpha&gt;0\\), \\(\\varepsilon&gt;0\\) small enough such that \\(f(x^*)\\leq f(x) \\ \\forall x\\in\\mathbb{S}\\) that are feasible.\nFor simplicity, let us assume that the functions \\(\\tilde{F}^k\\) are represented by equality constraints only. The general proof is similar.\n\\[ F^k(x) = f(x) + \\frac{k}{2} ||h(x)||^2 + \\frac{\\alpha}{2}||x-x^*||^2.\\]\nSuppose now that \\(\\{x^k\\}\\) is a sequence of minimisers of \\(\\{F^k\\}\\) over \\(\\mathbb{S}\\) (we can say this because of Weierstrass Theorem 1).\n\\[ F^k(x^k) = f(x^k) + \\frac{k}{2}||h(x^k)||^2 + \\frac{\\alpha}{2}||x^k-x^*||^2 \\leq F^k(x^*) = f(x^*).\\]\nNow, since \\(f(x^k)\\) is bounded over \\(\\mathbb{S}\\), we must have that\n\\[\\lim_{k\\rightarrow 0} ||h(x^k)|| = 0.\\]\nTherefore, by continuity, the limit point \\(\\bar{x}\\) of \\(\\{x^k\\}\\) satisfies \\(h(\\bar{x}) = 0\\). Taking the limit\n\\[ f(\\bar{x}) + \\frac{\\alpha}{2}||\\bar{x}-x^*||^2 \\leq f(x^*).\\]\nBut since \\(\\bar{x}\\in\\mathbb{S}\\) and \\(\\bar{x}\\) is also feasible, we have that\n\\[f(x^*) \\leq f(\\bar{x}).\\]\nThus, combining the last two inequalities, we must have \\(\\bar{x} = x^*\\). We conclude that \\(\\{x^k\\}\\) converges to \\(x^*\\).\nNow, by the necessary condition of optimality for \\(F^k\\), we have\n\\[\\nabla F^k(x^k) = \\nabla f(x^k) + k\\nabla h(x^k) h(x^k) + \\alpha (x^k - x^*) = 0.\\]\nRecall that by regularity, for \\(k\\) large enough, \\(\\nabla h(x^k)\\) has rank \\(m\\). Hence, \\(H_h(x^k) \\triangleq \\nabla^T h(x^k) \\nabla h(x^k)\\) is invertible. Thus,\n\\[ k h(x^k) = - H_h^{-1} (x_k) \\nabla^T h(x^k) (\\nabla f(^k) + \\alpha (x^k - x^*)).\\]\nBy taking the limit\n\\[ \\begin{aligned}\n& \\lim_{k\\rightarrow\\infty} k h(x^k) = -(\\nabla^T h(x^*) \\nabla h(x^*))^{-1} \\nabla h(x^*) \\nabla f(x^*), \\\\\n\\Rightarrow & \\nabla^T h(x^*) \\left(\\lim_{k\\rightarrow\\infty} k h(x^k)\\nabla h(x^k) + \\nabla f(x^k)\\right) = 0.\n\\end{aligned}\\]\nUsing that \\(\\nabla h(x^*)\\) is full rank,\n\\[\\lim_{k\\rightarrow\\infty} (k h(x^k))\\nabla h(x^*) + \\nabla f(x^*) = 0.\\]\nBy the necessary condition of optimality,\n\\[ \\lim_{k\\rightarrow\\infty} k h(x^k) = \\lambda^*.\\]\nA similar argument for \\(\\tilde{F}(x^k)\\) shows that\n\\[\\lim_{k\\rightarrow\\infty} k g_j^+(x^k) = \\mu_j^*.\\]\nAnd, note that \\(g_j^+(x^*) \\geq 0 \\ \\Rightarrow \\mu_j^* \\geq 0\\). ■\n\n\n4.3.1 NLP and Duality\nWe wish to study the following optimisation problem\n\\[\\begin{aligned}\n\\min_{x\\in C} \\ & f(x) \\\\\n\\textnormal{such that} \\ & g_j(x) \\leq 0, j\\in\\{1,...,r\\}, \\\\\n& h_i(x) = 0, i\\in\\{1,...,m\\},\n\\end{aligned}\\]\nwhere \\(C\\) is non-empty and convex, \\(f, g_j\\) are continuously differentiable from \\(C\\rightarrow\\mathbb{R}\\), and \\(h_i(x) = \\langle a_i, x\\rangle + \\beta_i\\), \\(i = 1,...,m\\).\nWe are going to state the so-called Strong Duality Theorem for Convex Programming (which is the problem we just stated). Let\n\\[L(x,\\lambda,\\mu) = f(x) + \\sum_{i=1}^m \\lambda_i h_i(x) + \\sum_{j=1}^r \\mu_j g_j(x).\\]\nAs before, we can write the primal problem\n\\[ \\inf_{x\\in C} \\sup_{\\mu\\geq 0, \\lambda\\in\\mathbb{R}^m} L(x,\\lambda, \\mu),\\]\nand the dual problem\n\\[\\sup_{\\mu\\geq 0, \\lambda\\in\\mathbb{R}^n} \\inf_{x\\in C} L(x,\\lambda,\\mu).\\]"
  },
  {
    "objectID": "posts/opt-theory/index.html#gradient-methods---first-order",
    "href": "posts/opt-theory/index.html#gradient-methods---first-order",
    "title": "Optimisation Theory and Applications",
    "section": "5.1 Gradient Methods - First Order",
    "text": "5.1 Gradient Methods - First Order"
  },
  {
    "objectID": "posts/opt-theory/index.html#gradient-methods---second-order",
    "href": "posts/opt-theory/index.html#gradient-methods---second-order",
    "title": "Optimisation Theory and Applications",
    "section": "5.2 Gradient Methods - Second Order",
    "text": "5.2 Gradient Methods - Second Order"
  },
  {
    "objectID": "posts/opt-theory/index.html#minimising-polynomials",
    "href": "posts/opt-theory/index.html#minimising-polynomials",
    "title": "Optimisation Theory and Applications",
    "section": "6.1 Minimising Polynomials",
    "text": "6.1 Minimising Polynomials"
  },
  {
    "objectID": "posts/opt-theory/index.html",
    "href": "posts/opt-theory/index.html",
    "title": "Optimisation Theory and Applications",
    "section": "",
    "text": "\\[\n\\newcommand{\\co}{\\texttt{co} \\ }\n\\newcommand{\\epi}{\\texttt{epi}}\n\\]"
  },
  {
    "objectID": "posts/opt-theory/index.html#conjugate-functions-1",
    "href": "posts/opt-theory/index.html#conjugate-functions-1",
    "title": "Optimisation Theory and Applications",
    "section": "4.4 Conjugate Functions",
    "text": "4.4 Conjugate Functions"
  },
  {
    "objectID": "posts/opt-theory/index.html#duality-theory-1",
    "href": "posts/opt-theory/index.html#duality-theory-1",
    "title": "Optimisation Theory and Applications",
    "section": "4.5 Duality Theory",
    "text": "4.5 Duality Theory"
  },
  {
    "objectID": "posts/opt-theory/index.html#nonlinear-programming-1",
    "href": "posts/opt-theory/index.html#nonlinear-programming-1",
    "title": "Optimisation Theory and Applications",
    "section": "4.6 Nonlinear Programming",
    "text": "4.6 Nonlinear Programming\n\n4.6.1 Conditions for Optimality\n\n\n4.6.2 NLP and Duality"
  }
]